{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ACM Machine Learning Workshop","text":""},{"location":"#unlock-the-power-of-data-with-machine-learning","title":"Unlock the Power of Data with Machine Learning","text":"<p>Welcome to the ACM Machine Learning Workshop, where curiosity meets computation. This workshop is your gateway into the fascinating world of Machine Learning (ML) \u2014 a field that empowers computers to learn from data and make intelligent decisions without being explicitly programmed.</p>"},{"location":"#about-this-workshop","title":"About This Workshop","text":"<p>This 4-day workshop is designed to blend theory with hands-on practice. You'll explore essential ML concepts, clean and prepare real datasets, build models, and even experiment with deep learning and natural language processing.</p>"},{"location":"#workshop-outline","title":"Workshop Outline","text":"Day Topic Highlights Day 1 Data Cleaning &amp; Feature Engineering Handle missing data, create features, prepare datasets Day 2 Model Training &amp; Analysis Train, test, and evaluate key ML models"},{"location":"#1-what-is-machine-learning","title":"1. What is Machine Learning?","text":"<p>Machine Learning is a branch of Artificial Intelligence (AI) that enables systems to identify patterns, make predictions, and improve automatically through experience. Unlike traditional programming where we explicitly code rules, ML allows computers to discover rules and patterns from data.</p>"},{"location":"#11-how-does-machine-learning-work","title":"1.1 How Does Machine Learning Work?","text":"<p>Think of ML like teaching a child to recognize animals. Instead of giving them a rulebook with detailed descriptions, you show them many pictures of cats and dogs. Over time, they learn to identify the patterns that distinguish cats from dogs \u2014 pointy ears, whiskers, size, behavior, etc.</p> <p>Similarly, ML systems: 1. Learn from data (training phase) 2. Identify patterns automatically 3. Make predictions on new, unseen data 4. Improve over time with more experience</p> <p>Data --&gt; ML Algorithm --&gt; Model --&gt; Predictions</p>"},{"location":"#12-real-world-applications-use-cases","title":"1.2 Real-World Applications &amp; Use Cases","text":"<p>Machine Learning is transforming industries and solving complex problems across diverse domains:</p>"},{"location":"#121-entertainment-media","title":"1.2.1 Entertainment &amp; Media","text":"<ul> <li>Netflix &amp; Spotify: Personalized recommendations based on viewing/listening history</li> <li>YouTube: Content suggestions and automatic video categorization</li> <li>Gaming: AI opponents that adapt to player behavior</li> </ul>"},{"location":"#122-healthcare-life-sciences","title":"1.2.2 Healthcare &amp; Life Sciences","text":"<ul> <li>Disease Diagnosis: Early detection of cancer, diabetes, and heart conditions from medical images</li> <li>Drug Discovery: Predicting molecular behavior to accelerate pharmaceutical research</li> <li>Patient Monitoring: Predicting patient deterioration in ICUs</li> <li>Personalized Treatment: Recommending treatments based on genetic profiles</li> </ul>"},{"location":"#123-finance-banking","title":"1.2.3 Finance &amp; Banking","text":"<ul> <li>Fraud Detection: Identifying suspicious transactions in real-time</li> <li>Credit Scoring: Assessing loan eligibility and risk</li> <li>Algorithmic Trading: Predicting stock market trends</li> <li>Customer Segmentation: Personalizing banking services</li> </ul>"},{"location":"#124-transportation-autonomous-vehicles","title":"1.2.4 Transportation &amp; Autonomous Vehicles","text":"<ul> <li>Self-Driving Cars: Tesla, Waymo using computer vision and sensor fusion</li> <li>Route Optimization: Uber, Google Maps predicting traffic and suggesting routes</li> <li>Predictive Maintenance: Anticipating vehicle component failures</li> </ul>"},{"location":"#125-manufacturing-industry","title":"1.2.5 Manufacturing &amp; Industry","text":"<ul> <li>Quality Control: Detecting defects in production lines</li> <li>Predictive Maintenance: Preventing equipment failures</li> <li>Supply Chain Optimization: Forecasting demand and managing inventory</li> <li>Energy Optimization: Reducing power consumption</li> </ul>"},{"location":"#13-what-kind-of-problems-can-ml-solve","title":"1.3 What Kind of Problems Can ML Solve?","text":"<p>Machine Learning excels at solving problems that fall into these categories:</p>"},{"location":"#131-classification-problems","title":"1.3.1 Classification Problems","text":"<p>Assigning data into predefined categories</p> <ul> <li>Is this email spam or not spam?</li> <li>Is this tumor malignant or benign?</li> <li>Which animal is in this photo?</li> <li>Will this customer churn or stay?</li> </ul> <p>Suggested Image: Classification visualization (e.g., scatter plot with different colored regions)</p>"},{"location":"#132-regression-problems","title":"1.3.2 Regression Problems","text":"<p>Predicting continuous numerical values</p> <ul> <li>What will be the house price?</li> <li>How much will sales be next month?</li> <li>What temperature will it be tomorrow?</li> <li>What's the expected lifetime value of a customer?</li> </ul>"},{"location":"#133-clustering-problems","title":"1.3.3 Clustering Problems","text":"<p>Finding natural groupings in data</p> <ul> <li>Customer segmentation for targeted marketing</li> <li>Document organization and topic modeling</li> <li>Genetic sequence analysis</li> <li>Social network community detection</li> </ul> <p>Suggested Image: Cluster visualization showing data points grouped into different clusters</p>"},{"location":"#134-anomaly-detection","title":"1.3.4 Anomaly Detection","text":"<p>Identifying unusual patterns</p> <ul> <li>Credit card fraud detection</li> <li>Network intrusion detection</li> <li>Manufacturing defect identification</li> <li>System health monitoring</li> </ul>"},{"location":"#135-recommendation-systems","title":"1.3.5 Recommendation Systems","text":"<p>Suggesting relevant items</p> <ul> <li>Movie/music recommendations</li> <li>Product suggestions</li> <li>Content personalization</li> <li>Friend suggestions on social media</li> </ul>"},{"location":"#136-natural-language-processing","title":"1.3.6 Natural Language Processing","text":"<p>Understanding and generating human language</p> <ul> <li>Sentiment analysis (is this review positive or negative?)</li> <li>Machine translation</li> <li>Text summarization</li> <li>Chatbots and virtual assistants</li> </ul> <p>Suggested Image: Word cloud or sentiment analysis visualization</p>"},{"location":"#137-computer-vision","title":"1.3.7 Computer Vision","text":"<p>Understanding visual information</p> <ul> <li>Face recognition</li> <li>Object detection and tracking</li> <li>Medical image analysis</li> <li>Autonomous navigation</li> </ul> <p>Suggested Image: Object detection example (image with bounding boxes and labels)</p>"},{"location":"#14-when-not-to-use-machine-learning","title":"1.4 When NOT to Use Machine Learning","text":"<p>While ML is powerful, it's not always the right solution:</p> <ul> <li>When simple rules suffice: If you can solve it with if-else statements, you probably don't need ML</li> <li>When data is scarce: ML needs substantial data to learn patterns</li> <li>When interpretability is critical: Some ML models are \"black boxes\" (though this is improving)</li> <li>When the problem is well-understood: Traditional algorithms may be more efficient</li> </ul>"},{"location":"#15-the-ml-workflow","title":"1.5 The ML Workflow","text":"<p>Every ML project follows a similar journey:</p> <ol> <li>Problem Definition: What are you trying to predict or understand?</li> <li>Data Collection: Gather relevant data</li> <li>Data Preparation: Clean, transform, and engineer features (Day 1)</li> <li>Model Selection: Choose appropriate algorithms</li> <li>Training: Let the model learn from data (will be taught on Day 2)</li> <li>Evaluation: Measure performance on test data (will be taught on Day 2)</li> <li>Deployment: Put the model into production (will be taught on Day 4)</li> <li>Monitoring: Track performance and retrain as needed</li> </ol>"},{"location":"#2-why-learn-machine-learning","title":"2. Why Learn Machine Learning?","text":"<p>Machine Learning has become one of the most in-demand skills across industries. By understanding ML, you can:</p> <ul> <li>Turn data into actionable insights.</li> <li>Build intelligent systems that adapt and improve.</li> <li>Contribute to innovations in AI, automation, and analytics.</li> </ul>"},{"location":"#3-resources-next-steps","title":"3. Resources &amp; Next Steps","text":"<ul> <li>Recommended reading and tutorials will be shared during the sessions.</li> <li>After the workshop, try applying what you've learned to a small end-to-end project.</li> <li>Questions? Feel free to ask during the sessions or reach out via the repository.</li> </ul> <p>We hope you enjoy the workshop and gain valuable insights into the world of machine learning!</p>"},{"location":"day1/day1/","title":"Day 1: Data Cleaning and Feature Engineering","text":""},{"location":"day1/day1/#workshop-resources","title":"\ud83d\udce5 Workshop Resources","text":"<p>For today's hands-on session, you'll need the following materials:</p> <p>\ud83d\udcd3 Colab Notebook: Open in Google Colab</p> <p>\ud83d\udcca Dataset: Download day1_materials.zip - Contains the Bangalore house prices dataset</p> <p>Important: Make a copy of the Colab notebook (File \u2192 Save a copy in Drive) before running it. We'll be using these resources throughout today's session.</p>"},{"location":"day1/day1/#1-introduction-to-real-estate-price-prediction","title":"1. Introduction to Real Estate Price Prediction","text":"<p>Welcome to Day 1 of our Machine Learning workshop! Today, we'll embark on an exciting journey to build a real estate price prediction model using data from Bangalore, India. Before we dive into coding, let's understand the fundamental concepts that make machine learning projects successful.</p>"},{"location":"day1/day1/#2-understanding-the-problem","title":"2. Understanding the Problem","text":"<p>Imagine you're a real estate agent or a home buyer trying to determine the fair price of a property. What factors would you consider? The location, size of the house, number of bedrooms, bathrooms, and many other features play crucial roles. Our goal is to teach a computer to understand these patterns and predict prices automatically.</p>"},{"location":"day1/day1/#3-the-machine-learning-pipeline","title":"3. The Machine Learning Pipeline","text":"<p>Every successful machine learning project follows a structured approach:</p> <ol> <li>Data Collection: Gathering relevant information</li> <li>Data Cleaning: Removing errors and inconsistencies</li> <li>Feature Engineering: Creating meaningful variables from raw data</li> <li>Exploratory Data Analysis: Understanding patterns in data</li> <li>Model Building: Training algorithms (we'll cover this in Day 2)</li> <li>Model Evaluation: Testing how well our model performs</li> </ol> <p>Today, we'll focus on the first four crucial steps - the foundation of any ML project.</p>"},{"location":"day1/day1/#4-data-cleaning-the-foundation-of-quality-models","title":"4. Data Cleaning: The Foundation of Quality Models","text":""},{"location":"day1/day1/#41-why-is-data-cleaning-important","title":"4.1 Why is Data Cleaning Important?","text":"<p>Think of data cleaning like preparing ingredients before cooking. You wouldn't use rotten vegetables or unwashed produce in a meal, right? Similarly, dirty data leads to poor predictions. Real-world data is messy - it has missing values, inconsistencies, duplicates, and errors that can mislead our model.</p>"},{"location":"day1/day1/#42-understanding-missing-values","title":"4.2 Understanding Missing Values","text":"<p>Example Scenario: Imagine a dataset of house listings where some entries don't have information about the number of bathrooms or the location. What should we do?</p> <p>Two Common Approaches:</p> <ol> <li>Deletion: Remove rows with missing data (when dataset is large)</li> <li>Imputation: Fill missing values with mean, median, or mode (when data is scarce)</li> </ol> <p>When to delete vs. impute? If you have 13,000 rows and only 1,000 have missing values, deletion is safe. But if 8,000 rows have missing values, you might want to impute to preserve information.</p> <p>Example DataFrame - Before:</p> Location BHK Bath Total_sqft Price Whitefield 3 2.0 1500 85 Marathahalli 2 NaN 1100 60 Electronic City 4 NaN 2000 120 Koramangala 3 3.0 1450 95 HSR Layout 2 NaN 950 55 Indiranagar 4 3.0 1800 110 <p>After Deletion:</p> Location BHK Bath Total_sqft Price Whitefield 3 2.0 1500 85 Koramangala 3 3.0 1450 95 Indiranagar 4 3.0 1800 110 <p>Result: Lost 3 rows of data</p> <p>After Imputation (with median = 2.5):</p> Location BHK Bath Total_sqft Price Whitefield 3 2.0 1500 85 Marathahalli 2 2.5 1100 60 Electronic City 4 2.5 2000 120 Koramangala 3 3.0 1450 95 HSR Layout 2 2.5 950 55 Indiranagar 4 3.0 1800 110 <p>Result: Preserved all 6 rows of data</p>"},{"location":"day1/day1/#43-dealing-with-irrelevant-features","title":"4.3 Dealing with Irrelevant Features","text":"<p>Not every piece of information is useful. Consider these columns in a house price dataset:</p> <ul> <li>Society Name: The specific housing society</li> </ul> <p>Question: Do these strongly influence price predictions? Often, the answer is no. Removing irrelevant features:</p> <ul> <li>Simplifies the model</li> <li>Reduces computational cost</li> <li>Prevents overfitting</li> <li>Improves model performance</li> </ul>"},{"location":"day1/day1/#44-standardizing-data-formats","title":"4.4 Standardizing Data Formats","text":"<p>The Problem: Your dataset has a \"size\" column with values like:</p> <ul> <li>\"2 BHK\"</li> <li>\"3 Bedroom\"</li> <li>\"4 BHK\"</li> </ul> <p>The Solution: Extract just the numeric part (2, 3, 4) to create a consistent \"bhk\" (Bedroom, Hall, Kitchen) column that machines can understand.</p> <p>Example - Before:</p> Location Size Bath Price Whitefield 2 BHK 2 60 Marathahalli 3 Bedroom 2 85 HSR Layout 4 BHK 3 120 Koramangala 2 Bedroom 1 55 Indiranagar 3 BHK 2 95 Electronic City 1 RK 1 35 <p>After Standardization:</p> Location Size BHK Bath Price Whitefield 2 BHK 2 2 60 Marathahalli 3 Bedroom 3 2 85 HSR Layout 4 BHK 4 3 120 Koramangala 2 Bedroom 2 1 55 Indiranagar 3 BHK 3 2 95 Electronic City 1 RK 1 1 35 <p>Now we have a clean numeric BHK column that machines can process!</p>"},{"location":"day1/day1/#45-handling-range-values","title":"4.5 Handling Range Values","text":"<p>Example: A property's size is listed as \"1133 - 1384 sq ft\" instead of a single number.</p> <p>Solution: Convert ranges to their average. For \"1133 - 1384\", we'd use (1133 + 1384) / 2 = 1258.5 sq ft.</p> <p>Example - Before:</p> Location BHK Total_sqft Price Whitefield 3 1200 85 Marathahalli 2 1133 - 1384 60 HSR Layout 2 950 - 1100 55 Koramangala 4 2200 - 2450 120 Indiranagar 3 1500 95 Electronic City 3 1350 - 1550 80 <p>After Range Conversion:</p> Location BHK Total_sqft Price Whitefield 3 1200.0 85 Marathahalli 2 1258.5 60 HSR Layout 2 1025.0 55 Koramangala 4 2325.0 120 Indiranagar 3 1500.0 95 Electronic City 3 1450.0 80 <p>All range values are now converted to single numeric values (averages)</p>"},{"location":"day1/day1/#46-cleaning-inconsistent-units","title":"4.6 Cleaning Inconsistent Units","text":"<p>Sometimes you'll find values like:</p> <ul> <li>\"2500 sq ft\"</li> <li>\"34.46 Sq. Meter\"</li> <li>\"4125 Perch\"</li> </ul> <p>These mixed units make comparison impossible. The best approach is to convert everything to a standard unit or exclude entries that can't be converted reliably.</p>"},{"location":"day1/day1/#5-feature-engineering-creating-meaningful-variables","title":"5. Feature Engineering: Creating Meaningful Variables","text":"<p>Feature engineering is the art of creating new, more informative variables from existing data. It's often the difference between a mediocre and an excellent model.</p>"},{"location":"day1/day1/#51-creating-price-per-square-foot","title":"5.1 Creating Price Per Square Foot","text":"<p>Why? Absolute price doesn't tell the whole story. A 3000 sq ft house costing \u20b960 lakhs might be a better deal than a 1000 sq ft house at \u20b930 lakhs.</p> <p>Calculation: Price per sq ft = (Price \u00d7 100,000) / Total Square Feet</p> <p>This normalized metric helps us compare properties of different sizes on equal footing.</p> <p>Example - Before:</p> Location BHK Total_sqft Price (Lakhs) Whitefield 2 1000 30 Marathahalli 3 3000 60 Koramangala 3 1500 50 HSR Layout 2 900 35 Indiranagar 4 2000 80 Electronic City 3 1200 45 <p>After Feature Engineering:</p> Location BHK Total_sqft Price (Lakhs) Price_per_sqft Whitefield 2 1000 30 3,000 Marathahalli 3 3000 60 2,000 Koramangala 3 1500 50 3,333 HSR Layout 2 900 35 3,889 Indiranagar 4 2000 80 4,000 Electronic City 3 1200 45 3,750 <p>Insights from Price per sqft: - Despite Marathahalli being expensive (\u20b960L), it has the lowest price per sqft (\u20b92,000) - HSR Layout offers better value at \u20b93,889 per sqft - Indiranagar is the most expensive at \u20b94,000 per sqft</p>"},{"location":"day1/day1/#52-grouping-rare-categories","title":"5.2 Grouping Rare Categories","text":"<p>The Problem: Your dataset has 1,293 unique locations, but 1,052 of them appear fewer than 10 times.</p> <p>The Solution: Group infrequent categories into an \"other\" category. Why?</p> <ol> <li>Statistical Significance: Locations with only 1-2 properties don't provide enough data for reliable patterns</li> <li>Model Simplicity: Fewer categories mean fewer variables to process</li> <li>Generalization: Helps the model focus on common patterns rather than rare exceptions</li> </ol> <p>Real-world analogy: If you're learning to recognize cars, you'd focus on common brands like Toyota, Honda, and Ford before worrying about rare vintage models.</p> <p>Example - Before:</p> Location Property_Count Avg_Price Whitefield 250 75 Marathahalli 180 65 Koramangala 220 85 HSR Layout 150 80 Indiranagar 200 95 Electronic City 120 55 Yelahanka 8 60 Devanahalli 5 45 Bagalur 3 40 Attibele 2 35 Hoskote 1 30 <p>After Grouping (threshold &lt; 10):</p> Location Property_Count Avg_Price Whitefield 250 75 Marathahalli 180 65 Koramangala 220 85 HSR Layout 150 80 Indiranagar 200 95 Electronic City 120 55 Other 19 42 <p>Result: Reduced from 11 unique locations to 7, grouping 5 rare locations with insufficient data</p>"},{"location":"day1/day1/#6-outlier-detection-and-removal","title":"6. Outlier Detection and Removal","text":"<p>Outliers are extreme values that don't fit the general pattern. They can severely distort your model's understanding of the data.</p>"},{"location":"day1/day1/#61-what-are-outliers","title":"6.1 What are Outliers?","text":"<p>Example 1: A 6-bedroom house with only 1,020 square feet total. That's roughly 170 sq ft per room - smaller than most bathrooms! This is clearly an error or exceptional case.</p> <p>Example 2: A property listed at \u20b912,000,000 per square foot when most properties in that area are \u20b95,000-10,000 per sq ft.</p> <p>Example DataFrame:</p> Location BHK Bath Total_sqft Price Price_per_sqft Sqft_per_room Whitefield 2 2 1200 65 5,417 600 Marathahalli 3 2 1500 90 6,000 500 Koramangala 6 2 1020 80 7,843 170 \u2190 Outlier! HSR Layout 3 2 1400 16,800 12,000,000 467 \u2190 Extreme! Indiranagar 4 3 2000 115 5,750 500 Electronic City 2 1 1100 62 5,636 550 Whitefield 8 3 1200 95 7,917 150 \u2190 Outlier! <p>Problems Identified: - Row 3: 6 BHK in only 1020 sqft = 170 sqft per room (impossible!) - Row 4: \u20b912 million per sqft (data entry error, probably meant \u20b912,000) - Row 7: 8 BHK in 1200 sqft = 150 sqft per room (unrealistic)</p>"},{"location":"day1/day1/#62-why-remove-outliers","title":"6.2 Why Remove Outliers?","text":"<p>Imagine teaching someone about typical house prices by showing them:</p> <ul> <li>99 normal houses (\u20b930-80 lakhs)</li> <li>1 ultra-luxury mansion (\u20b9500 lakhs)</li> </ul> <p>They might develop a skewed understanding. Similarly, outliers can mislead machine learning models.</p>"},{"location":"day1/day1/#63-domain-based-outlier-removal","title":"6.3 Domain-Based Outlier Removal","text":"<p>Rule of Thumb: In urban Indian housing, a reasonable minimum is about 300 square feet per bedroom.</p> <p>Logic: </p> <ul> <li>1 BHK should have at least 300 sq ft</li> <li>2 BHK should have at least 600 sq ft</li> <li>3 BHK should have at least 900 sq ft</li> </ul> <p>Properties below these thresholds are likely data entry errors or exceptional cases we should exclude.</p> <p>Example - Before:</p> Location BHK Total_sqft Price Sqft_per_room Status Whitefield 2 1200 65 600 \u2713 Valid Marathahalli 3 1500 90 500 \u2713 Valid Koramangala 6 1020 80 170 \u2717 Remove HSR Layout 4 1800 110 450 \u2713 Valid Indiranagar 4 800 70 200 \u2717 Remove Electronic City 2 950 55 475 \u2713 Valid Yelahanka 5 1200 75 240 \u2717 Remove Hebbal 3 1350 85 450 \u2713 Valid <p>After Domain-Based Removal (minimum 300 sqft/room):</p> Location BHK Total_sqft Price Sqft_per_room Status Whitefield 2 1200 65 600 \u2713 Valid Marathahalli 3 1500 90 500 \u2713 Valid HSR Layout 4 1800 110 450 \u2713 Valid Electronic City 2 950 55 475 \u2713 Valid Hebbal 3 1350 85 450 \u2713 Valid <p>Result: Removed 3 properties with unrealistic sqft per room ratios</p>"},{"location":"day1/day1/#64-outlier-removal-using-box-plots-and-iqr","title":"6.4 Outlier Removal using Box Plots and IQR","text":""},{"location":"day1/day1/#box-plot-visualization","title":"Box Plot Visualization","text":"<p>A box plot (or whisker plot) is a graphical representation that helps visualize the spread and skewness of numerical data. It displays:</p> <ul> <li>Median (Q2) \u2014 The midpoint of the dataset, dividing it into two equal halves.</li> <li>First Quartile (Q1) \u2014 The 25th percentile \u2014 25% of the data lies below this value.</li> <li>Third Quartile (Q3) \u2014 The 75th percentile \u2014 75% of the data lies below this value.</li> <li>Interquartile Range (IQR) \u2014 The range between Q3 and Q1, calculated as IQR = Q3 - Q1.</li> <li>Whiskers: Extend from Q1 and Q3 to show variability outside the upper and lower quartiles.</li> <li>Outliers: Points plotted beyond the whiskers that indicate unusually high or low values.</li> </ul> <p>A box plot makes it easy to spot outliers visually, as they appear as isolated points away from the main data cluster.</p> <p>Steps:</p> <ol> <li> <p>Compute Q1 and Q3 \u2014 Find the 25th and 75th percentiles of the data.</p> <p>Suppose you have house prices (in lakhs):</p> <p>[10, 48, 55, 120, 125, 185, 600]</p> <p>Q1 (First Quartile / 25th Percentile)</p> <p>The value below which 25% of the data falls.</p> <p>Here, Q1 \u2248 48 \u2192 one-fourth of the data is below \u20b948 L.</p> <p>Q3 (Third Quartile / 75th Percentile)</p> <p>The value below which 75% of the data falls.</p> <p>Here, Q3 \u2248 185 \u2192 most data (three-fourths) is below \u20b9185 L.</p> </li> <li> <p>Calculate IQR</p> <p><code>\ud835\udc3c\ud835\udc44\ud835\udc45=\ud835\udc443\u2212\ud835\udc441</code></p> </li> <li> <p>Determine cutoff limits</p> <p><code>Lower bound = Q1 - 1.5 \u00d7 IQR</code></p> <p><code>Upper bound = Q3 + 1.5 \u00d7 IQR</code></p> </li> <li> <p>Identify and remove outliers \u2014 Any value less than the lower bound or greater than the upper bound is considered an outlier.</p> </li> </ol> <p></p> <p></p>"},{"location":"day1/day1/#7-data-normalization-and-standardization","title":"7. Data Normalization and Standardization","text":"<p>Before feeding data to machine learning models, we often need to scale our features to ensure they're on similar ranges.</p>"},{"location":"day1/day1/#71-why-scale-features","title":"7.1 Why Scale Features?","text":"<p>The Problem: Features with larger ranges can dominate the learning process.</p> <p>Example - Unscaled Data:</p> Location BHK Bath Total_sqft Price Whitefield 2 2 1200 60 Marathahalli 3 3 2500 120 Koramangala 2 1 800 40 HSR Layout 4 3 3000 150 Indiranagar 3 2 1800 95 Electronic City 2 2 1000 50 <p>Feature Ranges:</p> <ul> <li><code>Total_sqft</code>: 800 to 3000 (range = 2200)</li> <li><code>Bath</code>: 1 to 3 (range = 2)</li> <li><code>BHK</code>: 2 to 4 (range = 2)</li> </ul> <p>Notice how <code>Total_sqft</code> has a much larger range! Without scaling, models might give it disproportionate importance simply because of its larger numeric values.</p>"},{"location":"day1/day1/#72-standardization-z-score-normalization","title":"7.2 Standardization (Z-score Normalization)","text":"<p>Transforms data to have mean = 0 and standard deviation = 1.</p> <p>Formula: z = (x - \u03bc) / \u03c3</p> <p>When to use: When your data follows a normal distribution or when using algorithms like SVM, Linear Regression, or Logistic Regression.</p> <p>Example - After Standardization:</p> Location Total_sqft Standardized_sqft Bath Standardized_bath Whitefield 1200 -0.27 2 0.00 Marathahalli 2500 1.46 3 1.22 Koramangala 800 -1.18 1 -1.22 HSR Layout 3000 2.19 3 1.22 Indiranagar 1800 0.64 2 0.00 Electronic City 1000 -0.64 2 0.00 <p>Result:</p> <ul> <li>Mean \u2248 0 for both features</li> <li>Standard deviation = 1</li> <li>Values can be negative or positive</li> <li>Preserves outliers' relationships</li> </ul>"},{"location":"day1/day1/#73-normalization-min-max-scaling","title":"7.3 Normalization (Min-Max Scaling)","text":"<p>Scales data to a fixed range, typically [0, 1].</p> <p>Formula: x_scaled = (x - x_min) / (x_max - x_min)</p> <p>When to use: When you need a bounded range, especially for neural networks or when data doesn't follow normal distribution.</p> <p>Example - After Normalization:</p> Location Total_sqft Normalized_sqft Bath Normalized_bath Whitefield 1200 0.18 2 0.50 Marathahalli 2500 0.77 3 1.00 Koramangala 800 0.00 1 0.00 HSR Layout 3000 1.00 3 1.00 Indiranagar 1800 0.45 2 0.50 Electronic City 1000 0.09 2 0.50 <p>Result:</p> <ul> <li>All values between 0 and 1</li> <li>Minimum value becomes 0</li> <li>Maximum value becomes 1</li> <li>Preserves the relative distances between values</li> </ul> <p>Key Difference: </p> <ul> <li>Standardization: Values can be negative or &gt; 1; preserves outliers better</li> <li>Normalization: Always between [0,1]; more affected by outliers</li> </ul>"},{"location":"day1/day1/#8-preparing-data-for-machine-learning","title":"8. Preparing Data for Machine Learning","text":""},{"location":"day1/day1/#81-encoding-categorical-variables","title":"8.1 Encoding Categorical Variables","text":"<p>Machine learning algorithms work with numbers, not text. We need to convert categorical variables like \"location\" into numerical format.</p>"},{"location":"day1/day1/#811-one-hot-encoding","title":"8.1.1 One-Hot Encoding","text":"<p>Creates separate binary (0 or 1) columns for each category.</p> <p>Example - Before One-Hot Encoding:</p> Location BHK Bath Total_sqft Price Rajaji Nagar 2 2 1200 50 Hebbal 3 2 1500 75 Koramangala 2 1 1000 60 Rajaji Nagar 3 3 1400 70 Whitefield 4 3 2000 95 Hebbal 2 2 1100 55 <p>After One-Hot Encoding:</p> BHK Bath Total_sqft Price Rajaji_Nagar Hebbal Koramangala Whitefield 2 2 1200 50 1 0 0 0 3 2 1500 75 0 1 0 0 2 1 1000 60 0 0 1 0 3 3 1400 70 1 0 0 0 4 3 2000 95 0 0 0 1 2 2 1100 55 0 1 0 0 <p>Each location now has its own binary column. A \"1\" indicates the property is in that location.</p> <p>Advantages:</p> <ul> <li>Works well with algorithms that assume linear relationships</li> <li>No ordinal relationship assumed between categories</li> <li>Widely supported</li> </ul> <p>Disadvantages:</p> <ul> <li>Creates many columns for high-cardinality features</li> <li>Increases memory usage and computation time</li> <li>Can lead to sparse matrices</li> </ul> <p>Note: We don't create a column for \"other\" because if all location columns are 0, the model knows it's \"other.\"</p>"},{"location":"day1/day1/#812-label-encoding","title":"8.1.2 Label Encoding","text":"<p>Assigns a unique integer to each category.</p> <p>Example:</p> Location BHK Price Label_Encoded Rajaji Nagar 2 50 0 Hebbal 3 75 1 Koramangala 2 60 2 Rajaji Nagar 3 70 0 Whitefield 4 95 3 Hebbal 2 55 1 <p>Each unique location gets a single integer. Memory efficient but implies order.</p> <p>Advantages:</p> <ul> <li>Memory efficient (single column)</li> <li>Simple and fast</li> </ul> <p>Disadvantages:</p> <ul> <li>Implies ordinal relationship (Koramangala &gt; Hebbal &gt; Rajaji Nagar)</li> <li>Can mislead algorithms like Linear Regression</li> <li>Best for: Ordinal data (e.g., Low, Medium, High) or tree-based models</li> </ul>"},{"location":"day1/day1/#813-targetmean-encoding","title":"8.1.3 Target/Mean Encoding","text":"<p>Replaces categories with the mean of the target variable for that category.</p> <p>Example - Original Data:</p> Location BHK Price Rajaji Nagar 2 50 Hebbal 3 75 Koramangala 2 60 Rajaji Nagar 3 70 Hebbal 2 80 Koramangala 4 65 <p>After Target Encoding (using mean price per location):</p> Location BHK Price Location_Encoded Rajaji Nagar 2 50 60.0 Hebbal 3 75 77.5 Koramangala 2 60 62.5 Rajaji Nagar 3 70 60.0 Hebbal 2 80 77.5 Koramangala 4 65 62.5 <p>Each location is replaced by its average price (Rajaji Nagar = 60, Hebbal = 77.5, Koramangala = 62.5)</p> <p>Advantages:</p> <ul> <li>Captures relationship between category and target</li> <li>Single column (memory efficient)</li> </ul> <p>Disadvantages:</p> <ul> <li>Risk of data leakage</li> <li>Can overfit</li> <li>Best for: High-cardinality features in tree-based models</li> </ul>"},{"location":"day1/day1/#814-frequency-encoding","title":"8.1.4 Frequency Encoding","text":"<p>Replaces categories with their frequency count or percentage.</p> <p>Example:</p> Location BHK Price Frequency_Count Frequency_Pct Rajaji Nagar 2 50 250 0.25 Hebbal 3 75 180 0.18 Koramangala 2 60 220 0.22 Rajaji Nagar 3 70 250 0.25 Whitefield 4 95 200 0.20 Hebbal 2 55 180 0.18 <p>Popular locations get higher frequency values (Rajaji Nagar appears 250 times = 25% of dataset)</p> <p>Best for: When frequency itself is predictive (e.g., popular locations might be more expensive)</p> <p>Comparison Summary:</p> Method Columns Created Memory Preserves Info Risk of Overfitting Best Use Case One-Hot Many (n-1) High Yes Low Linear models, nominal data Label 1 Low Partial Low Tree models, ordinal data Target 1 Low Yes High Tree models, high cardinality Frequency 1 Low Partial Medium When frequency matters <p>For our project, we'll use One-Hot Encoding as it works well with linear models and doesn't assume any ordinal relationship between locations.</p>"},{"location":"day1/day1/#82-separating-features-and-target","title":"8.2 Separating Features and Target","text":"<p>Features (X): The input variables we use to make predictions - Total square feet - Number of bathrooms - Number of bedrooms (BHK) - Location (one-hot encoded)</p> <p>Target (y): What we're trying to predict - Price</p> <p>This separation is crucial because we train the model to find patterns between X and y.</p>"},{"location":"day1/day1/#9-data-visualization-seeing-patterns","title":"9. Data Visualization: Seeing Patterns","text":"<p>Visualization helps us understand our data intuitively.</p> <p></p>"},{"location":"day1/day1/#91-histogram-of-price-per-square-foot","title":"9.1 Histogram of Price Per Square Foot","text":"<p>A histogram shows the distribution of values: - X-axis: Price ranges (e.g., \u20b93,000-4,000, \u20b94,000-5,000) - Y-axis: How many properties fall in each range</p> <p>What to look for: - Where most properties are concentrated - Whether the distribution is normal (bell-shaped) - Presence of extreme values</p> <p></p>"},{"location":"day1/day1/#92-scatter-plots-for-outlier-detection","title":"9.2 Scatter Plots for Outlier Detection","text":"<p>Purpose: Compare 2 BHK vs. 3 BHK properties in the same location.</p> <p>Axes: - X-axis: Total square feet - Y-axis: Price</p> <p>What we expect:  - 3 BHK properties (green) should generally be above 2 BHK properties (blue) for the same square footage - Both should show an upward trend (more sq ft = higher price)</p> <p>Red Flags: - Blue dots (2 BHK) above green crosses (3 BHK) at the same square footage - Properties that don't follow the general upward trend</p> <p></p>"},{"location":"day1/day1/#10-summary-of-day-1-concepts","title":"10. Summary of Day 1 Concepts","text":"<p>Today we've learned that successful machine learning requires careful preparation:</p> <ol> <li>Clean your data: Remove inconsistencies, handle missing values, standardize formats</li> <li>Engineer features: Create meaningful variables like price per sq ft</li> <li>Remove outliers: Eliminate extreme values using domain knowledge and statistical methods</li> <li>Prepare for algorithms: Convert categories to numbers, separate features from targets</li> </ol>"},{"location":"day1/day1/#11-now-lets-begin-to-code","title":"11. Now let's begin to Code!","text":"<p>Open the Colab Link, Make a Copy and Upload the dataset on Colab</p> <p>\ud83d\udcd3 Colab Notebook: Open in Google Colab</p> <p>\ud83d\udcca Dataset: day1.zip - Contains the Bangalore house prices dataset</p> <p>See you next week! \ud83d\ude80</p>"},{"location":"day2/day2/","title":"Day 2: Model Training and Analysis","text":""},{"location":"day2/day2/#complete-documentation-guide","title":"Complete Documentation Guide","text":""},{"location":"day2/day2/#1-overview","title":"1. Overview","text":"<p>This workshop introduces fundamental machine learning concepts with practical implementation. You'll learn how to: - Split data for training and testing - Implement multiple regression and classification models - Evaluate model performance - Compare and select the best model</p> <p>Dataset: Real estate pricing data </p>"},{"location":"day2/day2/#workshop-resources-make-a-copy-and-run-the-cells","title":"Workshop Resources (Make a copy and run the cells) :","text":"<ol> <li> <p>Model Training(Regression) : ML_Workshop_Day2_Regression</p> </li> <li> <p>Preprocessing of Classification dataset : Drugclassification_Preprocessing</p> </li> <li> <p>Model Training(Classification) : ML_Worshop_Day2_Classification</p> </li> <li> <p>Dataset Files : Datasets</p> </li> </ol>"},{"location":"day2/day2/#2-dataset-information","title":"2. Dataset Information","text":""},{"location":"day2/day2/#preprocessed-dataset","title":"Preprocessed Dataset","text":"<ul> <li>Total Records: 10,835 properties</li> <li>Features: 246 (after preprocessing)</li> <li>Target Variable: <code>price</code> (continuous numerical value) - that we want to predict</li> </ul>"},{"location":"day2/day2/#feature-preparation","title":"Feature Preparation","text":"<ul> <li>The first step we do after pre processing our dataset, is split the features and target</li> <li>Target : what we are predicting</li> <li>Features : the properties that we use to predict certain value</li> </ul> <pre><code># Separating features and target\n# Features: drop 'price' (target), keep all others\nfeature_cols = [col for col in df1.columns if col != 'price']\nX = df1[feature_cols]\ny = df1['price']\n\n\n# Dataset shape\nFeatures: (10835, 244)\nTarget: (10835,)\n</code></pre>"},{"location":"day2/day2/#3-overfitting-and-underfitting","title":"3. Overfitting and Underfitting","text":"<p>When building machine learning models, the goal is to capture the true underlying patterns in data so the model can generalize to new, unseen examples.  </p> <p>However, models can sometimes go wrong in two common ways:</p> <ol> <li>Overfitting</li> <li>Underfitting </li> </ol> <p></p> <p>Striking the right balance between underfitting and overfitting is key to building robust machine learning models.</p>"},{"location":"day2/day2/#31-overfitting","title":"3.1. Overfitting","text":"<p>Overfitting happens when a model learns too much from the training data, including details that don\u2019t matter (like noise or outliers).</p> <p>Example : - Imagine fitting a very complicated curve to a set of points. The curve will go through every point, but it won\u2019t represent the actual pattern. - As a result, the model works great on training data but fails when tested on new data.</p> <p></p> <p>Reasons for Overfitting:</p> <ol> <li>High variance and low bias.</li> <li>The model is too complex.</li> <li>The size of the training data.</li> </ol>"},{"location":"day2/day2/#32-underfitting","title":"3.2. Underfitting","text":"<p>Underfitting is the opposite of overfitting. It happens when a model is too simple to capture what\u2019s going on in the data.</p> <p>Example: - Imagine drawing a straight line to fit points that actually follow a curve. The line misses most of the pattern. - In this case, the model doesn\u2019t work well on either the training or testing data.</p> <p></p> <p>Reasons for Underfitting:</p> <ol> <li>The model is too simple, So it may be not capable to represent the complexities in the data.</li> <li>The input features which is used to train the model is not the adequate representations of underlying factors influencing the target variable.</li> <li>The size of the training dataset used is not enough.</li> <li>Features are not scaled.</li> </ol>"},{"location":"day2/day2/#4-train-test-split","title":"4. Train-Test Split","text":""},{"location":"day2/day2/#what-is-train-test-split","title":"What is Train-Test Split?","text":"<p>Train-test split divides your dataset into two parts:</p> <p></p> <p>Training Set (80%): Used to teach the model - Model learns patterns from this data - Used for fitting/training algorithms</p> <p>Testing Set (20%): Used to evaluate the model - Model has never seen this data - Tests how well model generalizes to new data</p>"},{"location":"day2/day2/#implementation","title":"Implementation","text":"<pre><code>X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2,random_state=42)\n\n#test_size -&gt; indicates that 20% is for testing and 80% is for training\n\n#random_state -&gt; reproducibility - ensures same kind of split occurs everytime\n</code></pre>"},{"location":"day2/day2/#41-why-split-data","title":"4.1. Why Split Data?","text":"<ul> <li>Prevents Overfitting: Model doesn't memorize training data</li> <li>Tests Generalization: Evaluates performance on unseen data</li> <li>Realistic Performance: Simulates real-world predictions</li> </ul>"},{"location":"day2/day2/#5-types-of-machine-learning-problems","title":"5. Types of Machine Learning Problems","text":"<p>In supervised learning, problems are usually divided into two types :   </p> <ul> <li>Regression Problem</li> <li>Classification Problem</li> </ul>"},{"location":"day2/day2/#51-regression-problem","title":"5.1. Regression Problem","text":"<ul> <li>Goal : To predict a continuous numeric value.</li> <li> <p>Regression models try to find relationships between input variables (features) and a continuous output.</p> </li> <li> <p>Examples:</p> <ul> <li>Predicting house prices \ud83c\udfe0</li> <li>Estimating temperature \ud83c\udf21\ufe0f</li> <li>Forecasting stock prices \ud83d\udcc8  </li> </ul> </li> <li> <p>Common Algorithms:</p> <ol> <li>Linear Regression    </li> <li>Decision Tree Regressor</li> <li>Random Forest Regressor</li> <li>K - Nearest Neighbors Regressor</li> </ol> </li> <li> <p>Evaluation Metrics:</p> <ol> <li>Mean Squared Error (MSE)</li> <li>Root Mean Squared Error (RMSE)</li> <li>Mean Absolute Error (MAE)</li> <li>R\u00b2 Score</li> </ol> </li> </ul>"},{"location":"day2/day2/#52-classification-problem","title":"5.2. Classification Problem","text":"<ul> <li>Goal : To predict a discrete label or category.</li> <li> <p>Classification models learn to separate data into different classes.</p> </li> <li> <p>Examples:</p> <ul> <li>Email spam detection \u2709\ufe0f</li> <li>Disease diagnosis (positive/negative) \ud83e\uddec</li> <li>Image recognition (cat vs. dog) \ud83d\udc31\ud83d\udc36 </li> </ul> </li> <li> <p>Common Algorithms:</p> <ol> <li>Logistic Regression</li> <li>Decision Tree Classifier</li> <li>Random Forest Classifier</li> <li>k-Nearest Neighbors (KNN)</li> </ol> </li> <li> <p>Evaluation Metrics:</p> <ol> <li>Accuracy</li> <li>Precision &amp; Recall</li> <li>F1 Score</li> <li>Confusion Matrix</li> </ol> </li> </ul>"},{"location":"day2/day2/#6-regression-models","title":"6. Regression Models","text":"<p>Regression predicts continuous numerical values (e.g., house prices, temperature, sales).</p>"},{"location":"day2/day2/#61-linear-regression","title":"6.1. Linear Regression","text":"<p>How it works: Finds the best straight line through your data points.</p> <p>Mathematical Formula: </p> <pre><code>\u0177 = \u03b2\u2080 + \u03b2\u2081x\u2081 + \u03b2\u2082x\u2082 + ... + \u03b2\u2099x\u2099\n\nWhere:\n\u0177 = predicted value\n\u03b2\u2080 = intercept (bias)\n\u03b2\u2081, \u03b2\u2082, ..., \u03b2\u2099 = coefficients (weights)\nx\u2081, x\u2082, ..., x\u2099 = feature values\n</code></pre> <p>Simple form: <code>y = mx + b</code> </p> <ul> <li>Simple and interpretable</li> <li>Assumes linear relationship between features and target</li> </ul> <p></p> <p>Strengths:  </p> <ul> <li>Fast to train</li> <li>Easy to interpret</li> <li>Works well with linear relationships</li> </ul> <p>Weaknesses:  </p> <ul> <li>Cannot capture complex non-linear patterns</li> <li>Sensitive to outliers - basically those values that are much out of range when compared to normal values</li> </ul> <p>Use cases : </p> <ul> <li>Predicting house prices based on area, location, etc.  </li> <li>Estimating sales revenue from advertising spend.  </li> <li>Forecasting demand or performance metrics. </li> </ul>"},{"location":"day2/day2/#62-decision-tree-regressor","title":"6.2. Decision Tree Regressor","text":"<p>How it works: Creates a tree of yes/no questions to make predictions.</p> <p>Example:</p> <pre><code>Is size &gt; 2000 sq ft?\n  \u251c\u2500 Yes \u2192 Is location = downtown?\n  \u2502         \u251c\u2500 Yes \u2192 Predict $500k\n  \u2502         \u2514\u2500 No \u2192 Predict $350k\n  \u2514\u2500 No \u2192 Predict $250k\n</code></pre> <p></p> <p>Mathematical Formula :</p> <pre><code>Prediction at leaf node = (1/n) \u03a3\u1d62\u208c\u2081\u207f y\u1d62\n\nWhere:\nn = number of samples in the leaf\ny\u1d62 = actual values in the leaf\n(Takes the mean of training samples that reach that leaf)\n\nSplit criterion (MSE):\nMSE = (1/n) \u03a3\u1d62\u208c\u2081\u207f (y\u1d62 - \u0177)\u00b2\n</code></pre> <p>Strengths:  </p> <ul> <li>Handles non-linear relationships</li> <li>Easy to visualize and understand</li> <li>No feature scaling needed</li> </ul> <p>Weaknesses:  </p> <ul> <li>Can overfit easily</li> <li>Sensitive to small data changes</li> <li>May create overly complex trees</li> </ul> <p>Use cases : </p> <ul> <li>Predicting sales based on season, location, and marketing.  </li> <li>Modeling complex, non-linear data patterns.  </li> </ul>"},{"location":"day2/day2/#63-random-forest-regressor","title":"6.3. Random Forest Regressor","text":"<p>How it works: Creates many decision trees and averages their predictions.</p> <p>Think of it as: A committee of experts voting on the answer - Each tree sees slightly different data - Final prediction = average of all trees - Reduces overfitting compared to single tree</p> <p></p> <p>Mathematical Formula :</p> <pre><code>\u0177 = (1/T) \u03a3\u209c\u208c\u2081\u1d40 h\u209c(x)\n\nWhere:\nT = number of trees in the forest\nh\u209c(x) = prediction from tree t\n\u0177 = final prediction (average of all trees)\n</code></pre> <p>Strengths:  </p> <ul> <li>More accurate than single decision tree</li> <li>Handles complex relationships</li> <li>Reduces overfitting</li> <li>Shows feature importance</li> </ul> <p>Weaknesses:  </p> <ul> <li>Slower to train</li> <li>Less interpretable</li> <li>Requires more memory</li> </ul> <p>Use Cases : </p> <ul> <li>Predicting house prices, insurance claim amounts.  </li> <li>Forecasting demand or energy consumption. </li> </ul>"},{"location":"day2/day2/#64-k-nearest-neighbors-knn-regressor","title":"6.4. K-Nearest Neighbors (KNN) Regressor","text":"<p>How it works: Predicts based on the K closest training examples.</p> <p>Example (K=5): - Find 5 nearest houses to your property - Average their prices - That's your prediction</p> <p>Mathematical Formula :</p> <pre><code>\u0177 = (1/K) \u03a3\u1d62\u208c\u2081\u1d37 y\u1d62\n\nWhere:\nK = number of nearest neighbors\ny\u1d62 = value of i-th nearest neighbor\n\nDistance (Euclidean):\nd(x, x\u1d62) = \u221a(\u03a3\u2c7c\u208c\u2081\u207f (x\u2c7c - x\u1d62\u2c7c)\u00b2)\n</code></pre> <p></p> <p>Strengths:  </p> <ul> <li>Simple to understand</li> <li>No training phase (lazy learning)</li> <li>Naturally handles non-linear patterns</li> </ul> <p>Weaknesses:  </p> <ul> <li>Slow predictions on large datasets</li> <li>Needs feature scaling</li> <li>Sensitive to irrelevant features</li> </ul> <p>Use Cases : </p> <ul> <li>Estimating house rent based on nearby similar properties.  </li> <li>Predicting temperature using data from nearby weather stations.</li> </ul>"},{"location":"day2/day2/#7-classification-models","title":"7. Classification Models","text":"<p>Classification predicts categories/classes (e.g., spam/not spam, disease/healthy, high/medium/low price).</p>"},{"location":"day2/day2/#71-logistic-regression","title":"7.1. Logistic Regression","text":"<p>How it works: Despite the name, it's for classification! Predicts probability of belonging to a class.</p> <p>Example: Predicting if house is \"expensive\" or \"affordable\"</p> <pre><code>Probability = 1 / (1 + e^(-score))\nIf probability &gt; 0.5 \u2192 Expensive\nIf probability \u2264 0.5 \u2192 Affordable\n</code></pre> <p>Mathematical Formula :</p> <pre><code>P(y=1|x) = 1 / (1 + e^(-z))\n\nWhere:\nz = \u03b2\u2080 + \u03b2\u2081x\u2081 + \u03b2\u2082x\u2082 + ... + \u03b2\u2099x\u2099\nP(y=1|x) = probability of class 1\ne = Euler's number (\u22482.718)\n\nDecision: If P(y=1|x) &gt; 0.5 \u2192 Class 1\n          If P(y=1|x) \u2264 0.5 \u2192 Class 0\n</code></pre> <p></p> <p>Strengths:  </p> <ul> <li>Fast and efficient</li> <li>Provides probability scores</li> <li>Easy to interpret</li> </ul> <p>Weaknesses:  </p> <ul> <li>Assumes linear decision boundary</li> <li>Not effective for complex relationships</li> </ul> <p>When to use: Binary classification with linearly separable data</p>"},{"location":"day2/day2/#72-decision-tree-classifier","title":"7.2. Decision Tree Classifier","text":"<p>How it works: Same tree structure as regression, but predicts categories.</p> <p>Example:</p> <pre><code>Is size &gt; 2000 sq ft?\n  \u251c\u2500 Yes \u2192 Is location = downtown?\n  \u2502         \u251c\u2500 Yes \u2192 Class: Luxury\n  \u2502         \u2514\u2500 No \u2192 Class: Standard\n  \u2514\u2500 No \u2192 Class: Budget\n</code></pre> <p>Mathematical Formula :</p> <pre><code>Gini Impurity = 1 - \u03a3\u1d62\u208c\u2081\u1d9c p\u1d62\u00b2\n\nWhere:\nc = number of classes\np\u1d62 = proportion of class i in node\n\nEntropy (alternative):\nH = -\u03a3\u1d62\u208c\u2081\u1d9c p\u1d62 log\u2082(p\u1d62)\n\n(Tree splits to minimize impurity)\n</code></pre> <p></p> <p>Strengths:  </p> <ul> <li>Handles non-linear boundaries</li> <li>Interpretable</li> <li>Works with categorical data</li> </ul> <p>Weaknesses:  </p> <ul> <li>Overfits easily</li> <li>Unstable with small data changes</li> </ul> <p>When to use: When you need interpretability and have categorical data</p>"},{"location":"day2/day2/#73-random-forest-classifier","title":"7.3. Random Forest Classifier","text":"<p>How it works: Ensemble of decision trees voting on the class.</p> <p>Voting Example (5 trees):  </p> <ul> <li>Tree 1: Luxury</li> <li>Tree 2: Standard</li> <li>Tree 3: Luxury</li> <li>Tree 4: Luxury</li> <li>Tree 5: Standard</li> <li>Final Prediction: Luxury (majority vote: 3/5)</li> </ul> <p>Mathematical Formula :</p> <pre><code>\u0177 = mode{h\u2081(x), h\u2082(x), ..., h\u209c(x)}\n\nWhere:\nT = number of trees\nh\u209c(x) = prediction from tree t\nmode = most frequent class (majority vote)\n\nFor probabilities:\nP(class=c|x) = (1/T) \u03a3\u209c\u208c\u2081\u1d40 I(h\u209c(x) = c)\n</code></pre> <p></p> <p>Strengths:  </p> <ul> <li>High accuracy</li> <li>Reduces overfitting</li> <li>Shows feature importance</li> <li>Handles imbalanced data well</li> </ul> <p>Weaknesses:  </p> <ul> <li>Slower than single tree</li> <li>Less interpretable</li> <li>More memory intensive</li> </ul> <p>When to use: When accuracy is priority and you have sufficient data</p>"},{"location":"day2/day2/#74-k-nearest-neighbors-knn-classifier","title":"7.4. K-Nearest Neighbors (KNN) Classifier","text":"<p>How it works: Assigns class based on K nearest neighbors' majority vote.</p> <p>Example (K=5):  </p> <ul> <li>Find 5 nearest houses</li> <li>3 are \"Luxury\", 2 are \"Standard\"</li> <li>Predict: \"Luxury\" (majority)</li> </ul> <p>Mathematical Formula :</p> <pre><code>\u0177 = mode{y\u2081, y\u2082, ..., y\u2096}\n\nWhere:\nK = number of nearest neighbors\ny\u1d62 = class of i-th nearest neighbor\nmode = most frequent class\n\nDistance (Euclidean):\nd(x, x\u1d62) = \u221a(\u03a3\u2c7c\u208c\u2081\u207f (x\u2c7c - x\u1d62\u2c7c)\u00b2)\n</code></pre> <p></p> <p>Strengths:  </p> <ul> <li>Simple and intuitive</li> <li>No training needed</li> <li>Naturally handles multi-class</li> </ul> <p>Weaknesses:  </p> <ul> <li>Slow for large datasets</li> <li>Sensitive to feature scaling</li> <li>Curse of dimensionality</li> </ul> <p>When to use: Small to medium datasets with good feature engineering</p>"},{"location":"day2/day2/#8-evaluation-metrics","title":"8. Evaluation Metrics","text":""},{"location":"day2/day2/#section-81-regression-metrics","title":"Section 8.1 : Regression Metrics","text":""},{"location":"day2/day2/#811-mean-squared-error-mse","title":"8.1.1. Mean Squared Error (MSE)","text":"<p>Formula: Average of squared differences between predictions and actual values <code>MSE = (1/n) * \u03a3 (y\u1d62 - \u0177\u1d62)\u00b2</code></p> <p>Interpretation:  </p> <ul> <li>Lower is better</li> <li>Heavily penalizes large errors</li> <li>Units are squared (e.g., dollars\u00b2)</li> </ul> <p>Example:   </p> <ul> <li>Actual: $300k, Predicted: $310k \u2192 Error\u00b2: (10k)\u00b2 = 100M</li> <li>Actual: $300k, Predicted: $320k \u2192 Error\u00b2: (20k)\u00b2 = 400M</li> <li>MSE = (100M + 400M) / 2 = 250M</li> </ul> <p>Common Use Cases :</p> <ul> <li> <p>Training neural networks: Used as a loss function because it's differentiable and penalizes large errors  </p> </li> <li> <p>Quality control: When large deviations are particularly costly or dangerous  </p> </li> <li> <p>Financial forecasting: Where overestimating or underestimating by large amounts has severe consequences  </p> </li> </ul>"},{"location":"day2/day2/#812-root-mean-squared-error-rmse","title":"8.1.2. Root Mean Squared Error (RMSE)","text":"<p>Formula: Square root of MSE <code>RMSE = \u221a[ (1/n) * \u03a3 (y\u1d62 - \u0177\u1d62)\u00b2 ]</code></p> <p>Interpretation:  </p> <ul> <li>Lower is better</li> <li>Same units as target (dollars, not dollars\u00b2)</li> <li>More interpretable than MSE</li> </ul> <p>Example:  </p> <ul> <li>From above, MSE = 250M  </li> <li>RMSE = \u221a250M \u2248 15.8k</li> </ul> <p>Common Use Cases :</p> <ul> <li> <p>Real estate price prediction: Easy to interpret (\"model is off by $15.8k on average\")  </p> </li> <li> <p>Weather forecasting: Temperature predictions where errors need to be in degrees, not degrees\u00b2  </p> </li> <li> <p>Sales forecasting: When stakeholders need to understand prediction error in actual sales units</p> </li> </ul>"},{"location":"day2/day2/#813-mean-absolute-error-mae","title":"8.1.3. Mean Absolute Error (MAE)","text":"<p>Formula: Average of absolute differences <code>MAE = (1/n) * \u03a3 |y\u1d62 - \u0177\u1d62|</code></p> <p>Interpretation:  </p> <ul> <li>Lower is better</li> <li>Less sensitive to outliers than RMSE</li> <li>Direct average error</li> </ul> <p>Example: </p> <ul> <li>Actual: $300k, Predicted: $310k \u2192 |Error| = 10k  </li> <li>Actual: $300k, Predicted: $320k \u2192 |Error| = 20k  </li> <li>MAE = (10k + 20k) / 2 = 15k</li> </ul> <p>Common Use Cases : </p> <ul> <li> <p>Energy consumption forecasting: Where extreme values (holidays, events) shouldn't dominate the metric  </p> </li> <li> <p>Customer lifetime value prediction: When a few high-value customers shouldn't distort performance  </p> </li> <li> <p>Budget planning: Where you need realistic average deviations for resource allocation</p> </li> </ul>"},{"location":"day2/day2/#814-r2-score-r-squared","title":"8.1.4. R\u00b2 Score (R-Squared)","text":"<p>Formula: 1 - (Sum of Squared Residuals / Total Sum of Squares) <code>R\u00b2 = 1 - [ \u03a3 (y\u1d62 - \u0177\u1d62)\u00b2 / \u03a3 (y\u1d62 - \u0233)\u00b2 ]</code></p> <p>Interpretation:  </p> <ul> <li>Range: -\u221e to 1.0</li> <li>1.0 = Perfect predictions</li> <li>0.0 = Model no better than predicting mean</li> <li>&lt; 0 = Model worse than predicting mean</li> </ul> <p>Example: </p> <ul> <li>Total variance (\u03a3(y\u1d62 - \u0233)\u00b2) = 1000M  </li> <li>Residual variance (\u03a3(y\u1d62 - \u0177\u1d62)\u00b2) = 250M  </li> <li>R\u00b2 = 1 - (250 / 1000) = 0.75 \u2192 Model explains 75% of variance</li> </ul> <p>Common Use Cases : </p> <ul> <li> <p>Scientific research: Reporting how well your model explains the phenomenon  </p> </li> <li> <p>Marketing analytics: Understanding how much of sales variance is explained by campaigns vs. other factors  </p> </li> <li> <p>Academic/reporting contexts: When stakeholders need a single, intuitive performance metric</p> </li> </ul>"},{"location":"day2/day2/#82-classification-metrics","title":"8.2. Classification Metrics","text":"<p>Let us assume we have:</p> Actual Predicted Positive (1) Positive (1) Negative (0) Positive (1) Positive (1) Negative (0) Negative (0) Negative (0) <p>So: TP = 1, TN = 1, FP = 1, FN = 1</p> <p> </p>"},{"location":"day2/day2/#821-accuracy","title":"8.2.1. Accuracy","text":"<p>Formula: (Correct Predictions) / (Total Predictions) <code>Accuracy = (TP + TN) / (TP + TN + FP + FN)</code></p> <p>Example: Accuracy = (1 + 1) / (1 + 1 + 1 + 1) = 0.5 \u2192 50% accuracy</p> <p>Limitation: Misleading with imbalanced classes</p> <p>Common Use Cases : </p> <ul> <li> <p>Quality assessment: Product defect detection when defects and non-defects are roughly equal  </p> </li> <li> <p>Initial model evaluation: Quick baseline metric before diving into detailed analysis  </p> </li> <li> <p>Avoid for: Fraud detection, disease diagnosis, or any imbalanced dataset (accuracy paradox)</p> </li> </ul>"},{"location":"day2/day2/#822-confusion-matrix","title":"8.2.2. Confusion Matrix","text":"<p>Compares predictions vs actual:</p> <pre><code>                Predicted\n              No    Yes\nActual  No   [TN]  [FP]\n        Yes  [FN]  [TP]\n</code></pre> <ul> <li>TP: True Positives (correctly predicted Yes)</li> <li>TN: True Negatives (correctly predicted No)</li> <li>FP: False Positives (predicted Yes, actually No)</li> <li>FN: False Negatives (predicted No, actually Yes)</li> </ul> <p>Common Use Cases : </p> <ul> <li> <p>Medical diagnosis: Understanding both false alarms (FP) and missed cases (FN)  </p> </li> <li> <p>Spam filtering: Seeing how many legitimate emails are caught (FP) vs. spam that gets through (FN)</p> </li> </ul>"},{"location":"day2/day2/#823-precision","title":"8.2.3. Precision","text":"<p>Formula:  <code>TP / (TP + FP)</code></p> <p>Meaning: \"Of all positive predictions, how many were correct?\"</p> <p>Example:  = 1 / (1 + 1) = 0.5 \u2192 50% of predicted positives are correct</p> <p>Common Use Cases : </p> <ul> <li> <p>Product recommendations: Only recommend products you're confident users will like  </p> </li> <li> <p>Marketing campaign targeting: When contacting customers has a cost, ensure targets are relevant </p> </li> <li> <p>Legal document review: When reviewing flagged documents is expensive, minimize false flags</p> </li> </ul>"},{"location":"day2/day2/#824-recall-sensitivity","title":"8.2.4. Recall (Sensitivity)","text":"<p>Formula:  <code>TP / (TP + FN)</code></p> <p>Meaning: \"Of all actual positives, how many did we catch?\"</p> <p>Example : = 1 / (1 + 1) = 0.5 \u2192 50% of actual positives identified</p> <p>Common Use Cases : </p> <ul> <li> <p>Fraud detection: Better to flag suspicious transactions for review than miss actual fraud  </p> </li> <li> <p>Security systems: Airport security, intrusion detection where missing threats is catastrophic</p> </li> </ul>"},{"location":"day2/day2/#825-f1-score","title":"8.2.5. F1-Score","text":"<p>Formula:  <code>F1 = 2 * (Precision * Recall) / (Precision + Recall)</code></p> <p>Meaning: Harmonic mean of precision and recall</p> <p>Example : F1 = 2 * (0.5 * 0.5) / (0.5 + 0.5) = 0.5</p> <p>When to use: Balances precision and recall, especially with imbalanced data</p> <p>Common Use Cases : </p> <ul> <li> <p>Medical diagnosis with cost considerations: Balancing false alarms with missed diagnoses  </p> </li> <li> <p>Model comparison: Single metric for comparing models when both precision and recall matter</p> </li> </ul>"},{"location":"day2/day2/#9-model-comparison-regression-results","title":"9. Model Comparison (Regression Results)","text":""},{"location":"day2/day2/#91-regression-performance-ranking","title":"9.1. Regression Performance Ranking","text":"Rank Model R\u00b2 Score RMSE MAE \ud83e\udd47 1 Linear Regression 0.7904 30.79 9.95 \ud83e\udd48 2 Random Forest 0.7375 34.45 1.77 \ud83e\udd49 3 KNN 0.6585 39.29 1.91 4 Decision Tree 0.6268 41.08 3.14"},{"location":"day2/day2/#92-key-observations","title":"9.2. Key Observations","text":"<p>Linear Regression wins because:  </p> <ul> <li>\u2705 Highest R\u00b2 score (79.04%)  </li> <li>\u2705 Lowest RMSE (best average error)  </li> <li>\u2705 Fast training and prediction  </li> <li>\u2705 Easy to interpret  </li> </ul> <p>Interesting finding: Despite lower MAE, Random Forest has better overall performance metrics than simpler models.</p>"},{"location":"day2/day2/#10-model-comparison-classification-results","title":"10. Model Comparison (Classification Results)","text":""},{"location":"day2/day2/#101-classification-performance-ranking","title":"10.1. Classification Performance Ranking","text":"Model Accuracy Precision Recall F1 Score Logistic Regression 0.9231 0.9479 0.9231 0.9271 Decision Tree 1.0000 1.0000 1.0000 1.0000 Random Forest 1.0000 1.0000 1.0000 1.0000 KNN 0.6410 0.6282 0.6410 0.6197"},{"location":"day2/day2/#102-confusion-matrix-for-all-models","title":"10.2. Confusion Matrix for all models","text":""},{"location":"day2/day2/#103-key-observations","title":"10.3. Key Observations","text":"<p>\ud83c\udfc6 BEST CLASSIFICATION MODELS</p> Model Accuracy Precision Recall F1 Score Decision Tree 1.0 1.0 1.0 1.0 Random Forest 1.0 1.0 1.0 1.0 <p>Decision Tree &amp; Random Forest win because: </p> <ul> <li>\u2705 Perfect test accuracy on this dataset</li> <li>\u2705 Can capture complex, non-linear relationships</li> <li>\u2705 Handle both categorical and numerical features naturally</li> <li>\u2705 Robust and flexible for small datasets</li> </ul>"},{"location":"day2/day2/#11-general-ml-best-practices","title":"11. General ML Best Practices","text":"<ul> <li> <p>Always split your data</p> <ul> <li>Train-test split prevents overfitting</li> <li>Use cross-validation for robust evaluation</li> </ul> </li> <li> <p>Try multiple models</p> <ul> <li>Different models work better for different data</li> <li>No \"one size fits all\" solution</li> </ul> </li> <li> <p>Understand your metrics</p> <ul> <li>R\u00b2 for overall model fit</li> <li>RMSE for average prediction error</li> <li>MAE for median error magnitude</li> </ul> </li> <li> <p>Consider the business context</p> <ul> <li>Is $31k error acceptable for your use case?</li> <li>Sometimes a simple, interpretable model is better than a complex one</li> </ul> </li> </ul>"},{"location":"day2/day2/#12-summary","title":"12. Summary","text":"<p>You've learned:</p> <ul> <li>\u2705 Train-test split methodology</li> <li>\u2705 5 regression algorithms</li> <li>\u2705 6 classification algorithms </li> <li>\u2705 Multiple evaluation metrics</li> <li>\u2705 Model comparison techniques</li> </ul> <p>Remember:  The best model isn't always the most complex one.    Choose based on:  </p> <ul> <li>Performance on test data</li> <li>Interpretability needs</li> <li>Computational resources</li> <li>Business requirements</li> </ul> <p>Created for ML Workshop Day 2 | Happy Learning! \ud83c\udf93</p>"},{"location":"day3/day3/","title":"Coming Soon","text":""},{"location":"day4/day4/","title":"Coming Soon","text":""},{"location":"files/day4/code_which/","title":"Code which","text":"In\u00a0[\u00a0]: Copied! <pre>import torch\nimport torch.nn as nn\nfrom torchvision import datasets, transforms, models\n\n# \u2705 Device configuration (GPU if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# \u2705 Data transforms\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.Grayscale(num_output_channels=3),\n    transforms.ToTensor(),\n    transforms.Normalize((0.485, 0.456, 0.406),\n                         (0.229, 0.224, 0.225))\n])\n\n# \u2705 Datasets &amp; DataLoader\ntrain_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n\n# \u2705 Model setup\nmodel = models.resnet18(weights=None)\nmodel.fc = nn.Linear(model.fc.in_features, 10)\nmodel = model.to(device)\n\n# \u2705 Loss &amp; optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\n# \u2705 Training loop with progress display\nnum_epochs = \ntotal_steps = len(train_loader)\ntotal_images = len(train_dataset)\n\nfor epoch in range(num_epochs):\n    running_loss = 0.0\n    for batch_idx, (imgs, labels) in enumerate(train_loader):\n        imgs, labels = imgs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(imgs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n        # Print progress\n        processed = (batch_idx + 1) * imgs.size(0)\n        print(f\"Epoch [{epoch+1}/{num_epochs}] | \"\n              f\"Step [{batch_idx+1}/{total_steps}] | \"\n              f\"Images: {processed}/{total_images} | \"\n              f\"Loss: {loss.item():.4f}\", end=\"\\r\")\n\n    print(f\"\\n\u2705 Epoch [{epoch+1}/{num_epochs}] completed. \"\n          f\"Average Loss: {running_loss / total_steps:.4f}\\n\")\n\nprint(\"\ud83c\udf89 Training complete!\")\n</pre> import torch import torch.nn as nn from torchvision import datasets, transforms, models  # \u2705 Device configuration (GPU if available) device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") print(f\"Using device: {device}\")  # \u2705 Data transforms transform = transforms.Compose([     transforms.Resize((224, 224)),     transforms.Grayscale(num_output_channels=3),     transforms.ToTensor(),     transforms.Normalize((0.485, 0.456, 0.406),                          (0.229, 0.224, 0.225)) ])  # \u2705 Datasets &amp; DataLoader train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform) train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)  # \u2705 Model setup model = models.resnet18(weights=None) model.fc = nn.Linear(model.fc.in_features, 10) model = model.to(device)  # \u2705 Loss &amp; optimizer criterion = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)  # \u2705 Training loop with progress display num_epochs =  total_steps = len(train_loader) total_images = len(train_dataset)  for epoch in range(num_epochs):     running_loss = 0.0     for batch_idx, (imgs, labels) in enumerate(train_loader):         imgs, labels = imgs.to(device), labels.to(device)          optimizer.zero_grad()         outputs = model(imgs)         loss = criterion(outputs, labels)         loss.backward()         optimizer.step()          running_loss += loss.item()          # Print progress         processed = (batch_idx + 1) * imgs.size(0)         print(f\"Epoch [{epoch+1}/{num_epochs}] | \"               f\"Step [{batch_idx+1}/{total_steps}] | \"               f\"Images: {processed}/{total_images} | \"               f\"Loss: {loss.item():.4f}\", end=\"\\r\")      print(f\"\\n\u2705 Epoch [{epoch+1}/{num_epochs}] completed. \"           f\"Average Loss: {running_loss / total_steps:.4f}\\n\")  print(\"\ud83c\udf89 Training complete!\") In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport torch\n\n# Assuming 'imgs' and 'outputs' are from your last training batch\n# Get predicted class\n_, predicted = torch.max(outputs, 1)\n\n# \u2705 Denormalize images (since you used ImageNet normalization)\nmean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\nstd = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n\n# Move tensors to CPU for plotting\nimgs_cpu = imgs.detach().cpu()\npredicted_cpu = predicted.detach().cpu()\n\n# \u2705 Denormalize\nimages_denorm = imgs_cpu * std + mean\nimages_denorm = torch.clamp(images_denorm, 0, 1)  # keep pixel range valid\n\n# \u2705 Plot 15 images with predictions\nfig, axes = plt.subplots(1, 15, figsize=(15, 3))\nfor i in range(15):\n    ax = axes[i]\n    ax.imshow(images_denorm[i].permute(1, 2, 0))  # C\u00d7H\u00d7W \u2192 H\u00d7W\u00d7C\n    ax.set_title(f\"Pred: {predicted_cpu[i].item()}\", fontsize=8)\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()\n</pre> import matplotlib.pyplot as plt import torch  # Assuming 'imgs' and 'outputs' are from your last training batch # Get predicted class _, predicted = torch.max(outputs, 1)  # \u2705 Denormalize images (since you used ImageNet normalization) mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1) std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)  # Move tensors to CPU for plotting imgs_cpu = imgs.detach().cpu() predicted_cpu = predicted.detach().cpu()  # \u2705 Denormalize images_denorm = imgs_cpu * std + mean images_denorm = torch.clamp(images_denorm, 0, 1)  # keep pixel range valid  # \u2705 Plot 15 images with predictions fig, axes = plt.subplots(1, 15, figsize=(15, 3)) for i in range(15):     ax = axes[i]     ax.imshow(images_denorm[i].permute(1, 2, 0))  # C\u00d7H\u00d7W \u2192 H\u00d7W\u00d7C     ax.set_title(f\"Pred: {predicted_cpu[i].item()}\", fontsize=8)     ax.axis('off')  plt.tight_layout() plt.show()"},{"location":"files/day4/simple_cnn/","title":"Simple cnn","text":"<p>First, we start off with importing all the models required, for this particular workshop, we will be using pytorch</p> In\u00a0[\u00a0]: Copied! <pre>import torch\nimport torch.nn as nn\nfrom torchvision import datasets, transforms, models\nimport matplotlib.pyplot as plt\n</pre> import torch import torch.nn as nn from torchvision import datasets, transforms, models import matplotlib.pyplot as plt <p>The above cell is used to detect if there's CUDA functionality, basically if you have an Nvidia GPU, you can use that for model training, instead of using CPU power completely.</p> In\u00a0[7]: Copied! <pre># \u2705 Device configuration (GPU if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n</pre> # \u2705 Device configuration (GPU if available) device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") print(f\"Using device: {device}\") <pre>Using device: cuda\n</pre> <p>Now, the model we will be focusing on is called Resnet-50. For resnet-50, the input required is 224 x 224 x 3, but then the image size of the dataset is 28 x 28 x 1. So, for resnet-50, we will need to transform to 224 x 224, and now, its only 1 channel present in the image, but then the input taken by the model is 3 channels, so we use the greyscale function to convert to 3 channels, and then we convert images to tensor values, and we normalize it.</p> In\u00a0[10]: Copied! <pre>transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.Grayscale(num_output_channels=3),\n    transforms.ToTensor(),\n    transforms.Normalize((0.485, 0.456, 0.406),\n                         (0.229, 0.224, 0.225))\n])\n</pre> transform = transforms.Compose([     transforms.Resize((224, 224)),     transforms.Grayscale(num_output_channels=3),     transforms.ToTensor(),     transforms.Normalize((0.485, 0.456, 0.406),                          (0.229, 0.224, 0.225)) ])  <p>This is to load the MNIST dataset (which is a digit classification dataset, has multiple hand written images from 0-9).</p> In\u00a0[\u00a0]: Copied! <pre>train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n</pre> train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform) train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True) <p>This is to load the resnet-18 model, and the resnet-18 model initially outputs 1000 classes, but then we only need 10 (0-9), so we mention 10 as well to output only 10 classes.</p> In\u00a0[\u00a0]: Copied! <pre>model = models.resnet18(weights=None)\nmodel.fc = nn.Linear(model.fc.in_features, 10)\nmodel = model.to(device)\n</pre> model = models.resnet18(weights=None) model.fc = nn.Linear(model.fc.in_features, 10) model = model.to(device) <p>This initializes the loss and optimizer required.</p> In\u00a0[\u00a0]: Copied! <pre># \u2705 Loss &amp; optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n</pre> # \u2705 Loss &amp; optimizer criterion = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=1e-4) <p>Initializes the number of epochs ( the number of times the model trains ), and total images</p> In\u00a0[\u00a0]: Copied! <pre>num_epochs = 2\ntotal_steps = len(train_loader)\ntotal_images = len(train_dataset)\n</pre> num_epochs = 2 total_steps = len(train_loader) total_images = len(train_dataset)  <p>Training with log information as well</p> In\u00a0[\u00a0]: Copied! <pre>for epoch in range(num_epochs):\n    running_loss = 0.0\n    for batch_idx, (imgs, labels) in enumerate(train_loader):\n        imgs, labels = imgs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(imgs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n        # Print progress\n        processed = (batch_idx + 1) * imgs.size(0)\n        print(f\"Epoch [{epoch+1}/{num_epochs}] | \"\n              f\"Step [{batch_idx+1}/{total_steps}] | \"\n              f\"Images: {processed}/{total_images} | \"\n              f\"Loss: {loss.item():.4f}\", end=\"\\r\")\n\n    print(f\"\\n\u2705 Epoch [{epoch+1}/{num_epochs}] completed. \"\n          f\"Average Loss: {running_loss / total_steps:.4f}\\n\")\n\nprint(\"\ud83c\udf89 Training complete!\")\n</pre> for epoch in range(num_epochs):     running_loss = 0.0     for batch_idx, (imgs, labels) in enumerate(train_loader):         imgs, labels = imgs.to(device), labels.to(device)          optimizer.zero_grad()         outputs = model(imgs)         loss = criterion(outputs, labels)         loss.backward()         optimizer.step()          running_loss += loss.item()          # Print progress         processed = (batch_idx + 1) * imgs.size(0)         print(f\"Epoch [{epoch+1}/{num_epochs}] | \"               f\"Step [{batch_idx+1}/{total_steps}] | \"               f\"Images: {processed}/{total_images} | \"               f\"Loss: {loss.item():.4f}\", end=\"\\r\")      print(f\"\\n\u2705 Epoch [{epoch+1}/{num_epochs}] completed. \"           f\"Average Loss: {running_loss / total_steps:.4f}\\n\")  print(\"\ud83c\udf89 Training complete!\") <p>To visualize 15 predictions, using matplotlib</p> In\u00a0[\u00a0]: Copied! <pre># Assuming 'imgs' and 'outputs' are from your last training batch\n# Get predicted class\n_, predicted = torch.max(outputs, 1)\n\n# \u2705 Denormalize images (since you used ImageNet normalization)\nmean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\nstd = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n\n# Move tensors to CPU for plotting\nimgs_cpu = imgs.detach().cpu()\npredicted_cpu = predicted.detach().cpu()\n\n# \u2705 Denormalize\nimages_denorm = imgs_cpu * std + mean\nimages_denorm = torch.clamp(images_denorm, 0, 1)  # keep pixel range valid\n\n# \u2705 Plot 15 images with predictions\nfig, axes = plt.subplots(1, 15, figsize=(15, 3))\nfor i in range(15):\n    ax = axes[i]\n    ax.imshow(images_denorm[i].permute(1, 2, 0))  # C\u00d7H\u00d7W \u2192 H\u00d7W\u00d7C\n    ax.set_title(f\"Pred: {predicted_cpu[i].item()}\", fontsize=8)\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()\n</pre> # Assuming 'imgs' and 'outputs' are from your last training batch # Get predicted class _, predicted = torch.max(outputs, 1)  # \u2705 Denormalize images (since you used ImageNet normalization) mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1) std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)  # Move tensors to CPU for plotting imgs_cpu = imgs.detach().cpu() predicted_cpu = predicted.detach().cpu()  # \u2705 Denormalize images_denorm = imgs_cpu * std + mean images_denorm = torch.clamp(images_denorm, 0, 1)  # keep pixel range valid  # \u2705 Plot 15 images with predictions fig, axes = plt.subplots(1, 15, figsize=(15, 3)) for i in range(15):     ax = axes[i]     ax.imshow(images_denorm[i].permute(1, 2, 0))  # C\u00d7H\u00d7W \u2192 H\u00d7W\u00d7C     ax.set_title(f\"Pred: {predicted_cpu[i].item()}\", fontsize=8)     ax.axis('off')  plt.tight_layout() plt.show()"}]}