# DAY 3-DEEP LEARNING & NLP WORKSHOP

## ğŸ§  Introduction to Deep Learning

**Deep Learning (DL)** is a subset of Machine Learning that uses **neural networks** with many layers to automatically learn complex patterns from data.

### ğŸ§© Analogy

| Human Brain | Deep Learning |
| --- | --- |
| Neurons in the brain | Artificial Neurons in a network |
| Learn from experience | Learn from data |
| Recognizes faces/voices | Recognizes patterns in data |

### ğŸ” Example

- Traditional rule: â€œIf price < 10, buy.â€
- Deep Learning: *Learns* to buy/sell automatically from thousands of examples.

---

## âš–ï¸ Why Deep Learning over Traditional ML

| Aspect | Traditional Machine Learning | Deep Learning |
| --- | --- | --- |
| **Feature Engineering** | Manual â€” you select features | Automatic â€” model learns best features |
| **Data Requirement** | Works on small datasets | Requires large data, but performs better |
| **Performance** | Plateaus with complex data | Improves with more data & compute |
| **Examples** | Linear Regression, SVM, Decision Trees | CNN, LSTM, Transformers |
| **Applications** | Simple classification/regression | Image, speech, NLP, autonomous systems |

### ğŸ’¡ Example

- ML: You manually count words and predict sentiment.
- DL: The model *understands meaning* (e.g., â€œnot badâ€ = positive) automatically.

---

## ğŸ’¬ What is NLP (Natural Language Processing)?

**NLP** helps computers understand and generate human language.

### ğŸ’¡ Real-Life Examples

- ChatGPT (conversation)
- Google Translate (language conversion)
- Spam filters (email classification)
- Siri/Alexa (speech recognition)

---

## ğŸ§¹ Text Preprocessing Steps

Before giving text to a deep learning model, we clean and convert it into numbers.

| Step | What It Does | Example |
| --- | --- | --- |
| **Lowercasing** | Standardize text | â€œHarryâ€ â†’ â€œharryâ€ |
| **Tokenization** | Split into words | â€œharry potter wentâ€ â†’ [â€œharryâ€, â€œpotterâ€, â€œwentâ€] |
| **Stopword Removal** | Remove unimportant words | Remove â€œaâ€, â€œtheâ€, â€œtoâ€, etc. |
| **Stemming/Lemmatization** | Reduce to root form | â€œrunningâ€ â†’ â€œrunâ€ |
| **Vectorization** | Convert to numbers | â€œharryâ€ â†’ `[0.23, 0.89, -0.12, â€¦]` |

---

## ğŸ§© Understanding Word Embeddings

**Word Embeddings** represent words as numerical vectors such that similar words are close in space.

### âœ¨ Example:

- â€œkingâ€ â€“ â€œmanâ€ + â€œwomanâ€ â‰ˆ â€œqueenâ€
- â€œcatâ€ and â€œdogâ€ will be close in embedding space.

### ğŸ“Š Visualization

```markdown
          king
            \
             \    queen
              \  /
          man   woman

     cat -------- dog

```

> Embeddings capture semantic meaning, not just spelling.
> 

Popular embeddings:

- **Word2Vec**
- **GloVe**
- **FastText**

---

## âš™ï¸ Deep Learning Architectures for NLP

---

### ğŸŒ€ (A) LSTMs â€” Long Short-Term Memory Networks

LSTMs are good at learning **sequences** (like sentences or time series).

They solve a key problem â€” remembering **long-term context**.

### ğŸ§© Example

Sentence:

> â€œHarry looked at Ron and said he was __.â€
> 

To predict the blank word (â€œangryâ€), the model must **remember earlier words** â€” thatâ€™s what LSTMs do.

### ğŸ§  Concept Diagram

```markdown
Input â†’ [LSTM cell â†’ LSTM cell â†’ LSTM cell] â†’ Output
            â†‘ remembers previous words â†‘

```

### ğŸ§® Applications

- Next word prediction
- Sentiment analysis
- Chatbots

---

### âš¡ (B) Transformers

Transformers are the **modern standard** for NLP.

Instead of reading one word at a time, they read the **whole sentence at once** and use **attention** to find relationships between words.

### ğŸ§© Example

Sentence:

> â€œThe ball hit the boy because he was careless.â€
> 

The model learns that **â€œheâ€** refers to **â€œboyâ€**, not â€œballâ€.

### ğŸ” Attention Mechanism

The model gives â€œattention scoresâ€ â€” how much each word relates to another.

| Word | Attends to | Importance |
| --- | --- | --- |
| he | boy | â­â­â­â­ |
| ball | hit | â­â­ |
| because | careless | â­â­â­ |

---

## ğŸ¤– Modern Models: BERT & GPT

| Model | Direction | Main Purpose | Example Use |
| --- | --- | --- | --- |
| **BERT** | Bidirectional (reads both directions) | Understand text | Sentiment, Q&A |
| **GPT** | Unidirectional (leftâ†’right) | Generate text | Chatbots, writing |

---

## ğŸ§ª Hands-On Project: Predicting Next Word (Harry Potter Corpus)

### ğŸ§° Setup

```bash
pip install transformers torch

```

### ğŸ§© Python Code

```python
from transformers import pipeline

generator = pipeline("text-generation", model="gpt2")

prompt = "Harry looked at Ron and said"
result = generator(prompt, max_length=25, num_return_sequences=1, temperature=0.9)

print(result[0]['generated_text'])

```

### ğŸ§™â€â™‚ï¸ Example Output

> Harry looked at Ron and said quietly, â€œWe canâ€™t let anyone know about this.â€ The wind howled through the castle halls...
> 

### ğŸ” Try Custom Prompts

```python
prompts = [
    "Voldemort raised his wand and",
    "Hermione opened the book of spells and",
    "Hogwarts castle was silent until"
]

for p in prompts:
    print(generator(p, max_length=30, num_return_sequences=1)[0]['generated_text'])

```

### âš™ï¸ How It Works

1. Text â†’ tokens â†’ embeddings
2. Model predicts the **next likely word**
3. Adds it to text and repeats
4. Generates creative continuations

---

##