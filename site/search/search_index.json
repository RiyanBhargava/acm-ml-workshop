{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ACM Machine Learning Workshop","text":""},{"location":"#unlock-the-power-of-data-with-machine-learning","title":"Unlock the Power of Data with Machine Learning","text":"<p>Welcome to the ACM Machine Learning Workshop, where curiosity meets computation. This workshop is your gateway into the fascinating world of Machine Learning (ML) \u2014 a field that empowers computers to learn from data and make intelligent decisions without being explicitly programmed.</p>"},{"location":"#1-what-is-machine-learning","title":"1. What is Machine Learning?","text":"<p>Machine Learning is a branch of Artificial Intelligence (AI) that enables systems to identify patterns, make predictions, and improve automatically through experience. Unlike traditional programming where we explicitly code rules, ML allows computers to discover rules and patterns from data.</p>"},{"location":"#11-how-does-machine-learning-work","title":"1.1 How Does Machine Learning Work?","text":"<p>Think of ML like teaching a child to recognize animals. Instead of giving them a rulebook with detailed descriptions, you show them many pictures of cats and dogs. Over time, they learn to identify the patterns that distinguish cats from dogs \u2014 pointy ears, whiskers, size, behavior, etc.</p> <p>Similarly, ML systems: 1. Learn from data (training phase) 2. Identify patterns automatically 3. Make predictions on new, unseen data 4. Improve over time with more experience</p> <p>Data --&gt; ML Algorithm --&gt; Model --&gt; Predictions</p>"},{"location":"#12-real-world-applications-use-cases","title":"1.2 Real-World Applications &amp; Use Cases","text":"<p>Machine Learning is transforming industries and solving complex problems across diverse domains:</p>"},{"location":"#121-entertainment-media","title":"1.2.1 Entertainment &amp; Media","text":"<ul> <li>Netflix &amp; Spotify: Personalized recommendations based on viewing/listening history</li> <li>YouTube: Content suggestions and automatic video categorization</li> <li>Gaming: AI opponents that adapt to player behavior</li> </ul>"},{"location":"#122-healthcare-life-sciences","title":"1.2.2 Healthcare &amp; Life Sciences","text":"<ul> <li>Disease Diagnosis: Early detection of cancer, diabetes, and heart conditions from medical images</li> <li>Drug Discovery: Predicting molecular behavior to accelerate pharmaceutical research</li> <li>Patient Monitoring: Predicting patient deterioration in ICUs</li> <li>Personalized Treatment: Recommending treatments based on genetic profiles</li> </ul>"},{"location":"#123-finance-banking","title":"1.2.3 Finance &amp; Banking","text":"<ul> <li>Fraud Detection: Identifying suspicious transactions in real-time</li> <li>Credit Scoring: Assessing loan eligibility and risk</li> <li>Algorithmic Trading: Predicting stock market trends</li> <li>Customer Segmentation: Personalizing banking services</li> </ul>"},{"location":"#124-transportation-autonomous-vehicles","title":"1.2.4 Transportation &amp; Autonomous Vehicles","text":"<ul> <li>Self-Driving Cars: Tesla, Waymo using computer vision and sensor fusion</li> <li>Route Optimization: Uber, Google Maps predicting traffic and suggesting routes</li> <li>Predictive Maintenance: Anticipating vehicle component failures</li> </ul>"},{"location":"#125-manufacturing-industry","title":"1.2.5 Manufacturing &amp; Industry","text":"<ul> <li>Quality Control: Detecting defects in production lines</li> <li>Predictive Maintenance: Preventing equipment failures</li> <li>Supply Chain Optimization: Forecasting demand and managing inventory</li> <li>Energy Optimization: Reducing power consumption</li> </ul>"},{"location":"#13-what-kind-of-problems-can-ml-solve","title":"1.3 What Kind of Problems Can ML Solve?","text":"<p>Machine Learning excels at solving problems that fall into these categories:</p>"},{"location":"#131-classification-problems","title":"1.3.1 Classification Problems","text":"<p>Assigning data into predefined categories</p> <ul> <li>Is this email spam or not spam?</li> <li>Is this tumor malignant or benign?</li> <li>Which animal is in this photo?</li> <li>Will this customer churn or stay?</li> </ul> <p>Suggested Image: Classification visualization (e.g., scatter plot with different colored regions)</p>"},{"location":"#132-regression-problems","title":"1.3.2 Regression Problems","text":"<p>Predicting continuous numerical values</p> <ul> <li>What will be the house price?</li> <li>How much will sales be next month?</li> <li>What temperature will it be tomorrow?</li> <li>What's the expected lifetime value of a customer?</li> </ul>"},{"location":"#133-clustering-problems","title":"1.3.3 Clustering Problems","text":"<p>Finding natural groupings in data</p> <ul> <li>Customer segmentation for targeted marketing</li> <li>Document organization and topic modeling</li> <li>Genetic sequence analysis</li> <li>Social network community detection</li> </ul> <p>Suggested Image: Cluster visualization showing data points grouped into different clusters</p>"},{"location":"#134-anomaly-detection","title":"1.3.4 Anomaly Detection","text":"<p>Identifying unusual patterns</p> <ul> <li>Credit card fraud detection</li> <li>Network intrusion detection</li> <li>Manufacturing defect identification</li> <li>System health monitoring</li> </ul>"},{"location":"#135-recommendation-systems","title":"1.3.5 Recommendation Systems","text":"<p>Suggesting relevant items</p> <ul> <li>Movie/music recommendations</li> <li>Product suggestions</li> <li>Content personalization</li> <li>Friend suggestions on social media</li> </ul>"},{"location":"#136-natural-language-processing","title":"1.3.6 Natural Language Processing","text":"<p>Understanding and generating human language</p> <ul> <li>Sentiment analysis (is this review positive or negative?)</li> <li>Machine translation</li> <li>Text summarization</li> <li>Chatbots and virtual assistants</li> </ul> <p>Suggested Image: Word cloud or sentiment analysis visualization</p>"},{"location":"#137-computer-vision","title":"1.3.7 Computer Vision","text":"<p>Understanding visual information</p> <ul> <li>Face recognition</li> <li>Object detection and tracking</li> <li>Medical image analysis</li> <li>Autonomous navigation</li> </ul> <p>Suggested Image: Object detection example (image with bounding boxes and labels)</p>"},{"location":"#14-when-not-to-use-machine-learning","title":"1.4 When NOT to Use Machine Learning","text":"<p>While ML is powerful, it's not always the right solution:</p> <ul> <li>When simple rules suffice: If you can solve it with if-else statements, you probably don't need ML</li> <li>When data is scarce: ML needs substantial data to learn patterns</li> <li>When interpretability is critical: Some ML models are \"black boxes\" (though this is improving)</li> <li>When the problem is well-understood: Traditional algorithms may be more efficient</li> </ul>"},{"location":"#15-the-ml-workflow","title":"1.5 The ML Workflow","text":"<p>Every ML project follows a similar journey:</p> <ol> <li>Problem Definition: What are you trying to predict or understand?</li> <li>Data Collection: Gather relevant data</li> <li>Data Preparation: Clean, transform, and engineer features (Day 1)</li> <li>Model Selection: Choose appropriate algorithms</li> <li>Training: Let the model learn from data (will be taught on Day 2)</li> <li>Evaluation: Measure performance on test data (will be taught on Day 2)</li> <li>Deployment: Put the model into production (will be taught on Day 4)</li> <li>Monitoring: Track performance and retrain as needed</li> </ol>"},{"location":"#2-why-learn-machine-learning","title":"2. Why Learn Machine Learning?","text":"<p>Machine Learning has become one of the most in-demand skills across industries. By understanding ML, you can:</p> <ul> <li>Turn data into actionable insights.</li> <li>Build intelligent systems that adapt and improve.</li> <li>Contribute to innovations in AI, automation, and analytics.</li> </ul>"},{"location":"#3-about-this-workshop","title":"3. About This Workshop","text":"<p>This 4-day workshop is designed to blend theory with hands-on practice. You'll explore essential ML concepts, clean and prepare real datasets, build models, and even experiment with deep learning and natural language processing.</p>"},{"location":"#31-workshop-outline","title":"3.1 Workshop Outline","text":"Day Topic Highlights Day 1 Data Cleaning &amp; Feature Engineering Handle missing data, create features, prepare datasets Day 2 Model Training &amp; Analysis Train, test, and evaluate key ML models Day 3 Natural Language Processing Explore text data, embeddings, and GPT models Day 4 Deep Learning &amp; Deployment Build neural networks and learn model hosting basics"},{"location":"#4-resources-next-steps","title":"4. Resources &amp; Next Steps","text":"<ul> <li>Recommended reading and tutorials will be shared during the sessions.</li> <li>After the workshop, try applying what you've learned to a small end-to-end project.</li> <li>Questions? Feel free to ask during the sessions or reach out via the repository.</li> </ul> <p>We hope you enjoy the workshop and gain valuable insights into the world of machine learning!</p>"},{"location":"day1/day1/","title":"Day 1: Data Cleaning and Feature Engineering","text":""},{"location":"day1/day1/#workshop-resources","title":"\ud83d\udce5 Workshop Resources","text":"<p>For today's hands-on session, you'll need the following materials:</p> <p>\ud83d\udcd3 Colab Notebook: Open in Google Colab</p> <p>\ud83d\udcca Dataset: Download day1_materials.zip - Contains the Bangalore house prices dataset</p> <p>Important: Make a copy of the Colab notebook (File \u2192 Save a copy in Drive) before running it. We'll be using these resources throughout today's session.</p>"},{"location":"day1/day1/#1-introduction-to-real-estate-price-prediction","title":"1. Introduction to Real Estate Price Prediction","text":"<p>Welcome to Day 1 of our Machine Learning workshop! Today, we'll embark on an exciting journey to build a real estate price prediction model using data from Bangalore, India. Before we dive into coding, let's understand the fundamental concepts that make machine learning projects successful.</p>"},{"location":"day1/day1/#2-understanding-the-problem","title":"2. Understanding the Problem","text":"<p>Imagine you're a real estate agent or a home buyer trying to determine the fair price of a property. What factors would you consider? The location, size of the house, number of bedrooms, bathrooms, and many other features play crucial roles. Our goal is to teach a computer to understand these patterns and predict prices automatically.</p>"},{"location":"day1/day1/#3-the-machine-learning-pipeline","title":"3. The Machine Learning Pipeline","text":"<p>Every successful machine learning project follows a structured approach:</p> <ol> <li>Data Collection: Gathering relevant information</li> <li>Data Cleaning: Removing errors and inconsistencies</li> <li>Feature Engineering: Creating meaningful variables from raw data</li> <li>Exploratory Data Analysis: Understanding patterns in data</li> <li>Model Building: Training algorithms (we'll cover this in Day 2)</li> <li>Model Evaluation: Testing how well our model performs</li> </ol> <p>Today, we'll focus on the first four crucial steps - the foundation of any ML project.</p>"},{"location":"day1/day1/#4-data-cleaning-the-foundation-of-quality-models","title":"4. Data Cleaning: The Foundation of Quality Models","text":""},{"location":"day1/day1/#41-why-is-data-cleaning-important","title":"4.1 Why is Data Cleaning Important?","text":"<p>Think of data cleaning like preparing ingredients before cooking. You wouldn't use rotten vegetables or unwashed produce in a meal, right? Similarly, dirty data leads to poor predictions. Real-world data is messy - it has missing values, inconsistencies, duplicates, and errors that can mislead our model.</p>"},{"location":"day1/day1/#42-understanding-missing-values","title":"4.2 Understanding Missing Values","text":"<p>Example Scenario: Imagine a dataset of house listings where some entries don't have information about the number of bathrooms or the location. What should we do?</p> <p>Two Common Approaches:</p> <ol> <li>Deletion: Remove rows with missing data (when dataset is large)</li> <li>Imputation: Fill missing values with mean, median, or mode (when data is scarce)</li> </ol> <p>When to delete vs. impute? If you have 13,000 rows and only 1,000 have missing values, deletion is safe. But if 8,000 rows have missing values, you might want to impute to preserve information.</p> <p>Example DataFrame - Before:</p> Location BHK Bath Total_sqft Price Whitefield 3 2.0 1500 85 Marathahalli 2 NaN 1100 60 Electronic City 4 NaN 2000 120 Koramangala 3 3.0 1450 95 HSR Layout 2 NaN 950 55 Indiranagar 4 3.0 1800 110 <p>After Deletion:</p> Location BHK Bath Total_sqft Price Whitefield 3 2.0 1500 85 Koramangala 3 3.0 1450 95 Indiranagar 4 3.0 1800 110 <p>Result: Lost 3 rows of data</p> <p>After Imputation (with median = 2.5):</p> Location BHK Bath Total_sqft Price Whitefield 3 2.0 1500 85 Marathahalli 2 2.5 1100 60 Electronic City 4 2.5 2000 120 Koramangala 3 3.0 1450 95 HSR Layout 2 2.5 950 55 Indiranagar 4 3.0 1800 110 <p>Result: Preserved all 6 rows of data</p>"},{"location":"day1/day1/#43-dealing-with-irrelevant-features","title":"4.3 Dealing with Irrelevant Features","text":"<p>Not every piece of information is useful. Consider these columns in a house price dataset:</p> <ul> <li>Society Name: The specific housing society</li> </ul> <p>Question: Do these strongly influence price predictions? Often, the answer is no. Removing irrelevant features:</p> <ul> <li>Simplifies the model</li> <li>Reduces computational cost</li> <li>Prevents overfitting</li> <li>Improves model performance</li> </ul>"},{"location":"day1/day1/#44-standardizing-data-formats","title":"4.4 Standardizing Data Formats","text":"<p>The Problem: Your dataset has a \"size\" column with values like:</p> <ul> <li>\"2 BHK\"</li> <li>\"3 Bedroom\"</li> <li>\"4 BHK\"</li> </ul> <p>The Solution: Extract just the numeric part (2, 3, 4) to create a consistent \"bhk\" (Bedroom, Hall, Kitchen) column that machines can understand.</p> <p>Example - Before:</p> Location Size Bath Price Whitefield 2 BHK 2 60 Marathahalli 3 Bedroom 2 85 HSR Layout 4 BHK 3 120 Koramangala 2 Bedroom 1 55 Indiranagar 3 BHK 2 95 Electronic City 1 RK 1 35 <p>After Standardization:</p> Location Size BHK Bath Price Whitefield 2 BHK 2 2 60 Marathahalli 3 Bedroom 3 2 85 HSR Layout 4 BHK 4 3 120 Koramangala 2 Bedroom 2 1 55 Indiranagar 3 BHK 3 2 95 Electronic City 1 RK 1 1 35 <p>Now we have a clean numeric BHK column that machines can process!</p>"},{"location":"day1/day1/#45-handling-range-values","title":"4.5 Handling Range Values","text":"<p>Example: A property's size is listed as \"1133 - 1384 sq ft\" instead of a single number.</p> <p>Solution: Convert ranges to their average. For \"1133 - 1384\", we'd use (1133 + 1384) / 2 = 1258.5 sq ft.</p> <p>Example - Before:</p> Location BHK Total_sqft Price Whitefield 3 1200 85 Marathahalli 2 1133 - 1384 60 HSR Layout 2 950 - 1100 55 Koramangala 4 2200 - 2450 120 Indiranagar 3 1500 95 Electronic City 3 1350 - 1550 80 <p>After Range Conversion:</p> Location BHK Total_sqft Price Whitefield 3 1200.0 85 Marathahalli 2 1258.5 60 HSR Layout 2 1025.0 55 Koramangala 4 2325.0 120 Indiranagar 3 1500.0 95 Electronic City 3 1450.0 80 <p>All range values are now converted to single numeric values (averages)</p>"},{"location":"day1/day1/#46-cleaning-inconsistent-units","title":"4.6 Cleaning Inconsistent Units","text":"<p>Sometimes you'll find values like:</p> <ul> <li>\"2500 sq ft\"</li> <li>\"34.46 Sq. Meter\"</li> <li>\"4125 Perch\"</li> </ul> <p>These mixed units make comparison impossible. The best approach is to convert everything to a standard unit or exclude entries that can't be converted reliably.</p>"},{"location":"day1/day1/#5-feature-engineering-creating-meaningful-variables","title":"5. Feature Engineering: Creating Meaningful Variables","text":"<p>Feature engineering is the art of creating new, more informative variables from existing data. It's often the difference between a mediocre and an excellent model.</p>"},{"location":"day1/day1/#51-creating-price-per-square-foot","title":"5.1 Creating Price Per Square Foot","text":"<p>Why? Absolute price doesn't tell the whole story. A 3000 sq ft house costing \u20b960 lakhs might be a better deal than a 1000 sq ft house at \u20b930 lakhs.</p> <p>Calculation: Price per sq ft = (Price \u00d7 100,000) / Total Square Feet</p> <p>This normalized metric helps us compare properties of different sizes on equal footing.</p> <p>Example - Before:</p> Location BHK Total_sqft Price (Lakhs) Whitefield 2 1000 30 Marathahalli 3 3000 60 Koramangala 3 1500 50 HSR Layout 2 900 35 Indiranagar 4 2000 80 Electronic City 3 1200 45 <p>After Feature Engineering:</p> Location BHK Total_sqft Price (Lakhs) Price_per_sqft Whitefield 2 1000 30 3,000 Marathahalli 3 3000 60 2,000 Koramangala 3 1500 50 3,333 HSR Layout 2 900 35 3,889 Indiranagar 4 2000 80 4,000 Electronic City 3 1200 45 3,750 <p>Insights from Price per sqft: - Despite Marathahalli being expensive (\u20b960L), it has the lowest price per sqft (\u20b92,000) - HSR Layout offers better value at \u20b93,889 per sqft - Indiranagar is the most expensive at \u20b94,000 per sqft</p>"},{"location":"day1/day1/#52-grouping-rare-categories","title":"5.2 Grouping Rare Categories","text":"<p>The Problem: Your dataset has 1,293 unique locations, but 1,052 of them appear fewer than 10 times.</p> <p>The Solution: Group infrequent categories into an \"other\" category. Why?</p> <ol> <li>Statistical Significance: Locations with only 1-2 properties don't provide enough data for reliable patterns</li> <li>Model Simplicity: Fewer categories mean fewer variables to process</li> <li>Generalization: Helps the model focus on common patterns rather than rare exceptions</li> </ol> <p>Real-world analogy: If you're learning to recognize cars, you'd focus on common brands like Toyota, Honda, and Ford before worrying about rare vintage models.</p> <p>Example - Before:</p> Location Property_Count Avg_Price Whitefield 250 75 Marathahalli 180 65 Koramangala 220 85 HSR Layout 150 80 Indiranagar 200 95 Electronic City 120 55 Yelahanka 8 60 Devanahalli 5 45 Bagalur 3 40 Attibele 2 35 Hoskote 1 30 <p>After Grouping (threshold &lt; 10):</p> Location Property_Count Avg_Price Whitefield 250 75 Marathahalli 180 65 Koramangala 220 85 HSR Layout 150 80 Indiranagar 200 95 Electronic City 120 55 Other 19 42 <p>Result: Reduced from 11 unique locations to 7, grouping 5 rare locations with insufficient data</p>"},{"location":"day1/day1/#6-outlier-detection-and-removal","title":"6. Outlier Detection and Removal","text":"<p>Outliers are extreme values that don't fit the general pattern. They can severely distort your model's understanding of the data.</p>"},{"location":"day1/day1/#61-what-are-outliers","title":"6.1 What are Outliers?","text":"<p>Example 1: A 6-bedroom house with only 1,020 square feet total. That's roughly 170 sq ft per room - smaller than most bathrooms! This is clearly an error or exceptional case.</p> <p>Example 2: A property listed at \u20b912,000,000 per square foot when most properties in that area are \u20b95,000-10,000 per sq ft.</p> <p>Example DataFrame:</p> Location BHK Bath Total_sqft Price Price_per_sqft Sqft_per_room Whitefield 2 2 1200 65 5,417 600 Marathahalli 3 2 1500 90 6,000 500 Koramangala 6 2 1020 80 7,843 170 \u2190 Outlier! HSR Layout 3 2 1400 16,800 12,000,000 467 \u2190 Extreme! Indiranagar 4 3 2000 115 5,750 500 Electronic City 2 1 1100 62 5,636 550 Whitefield 8 3 1200 95 7,917 150 \u2190 Outlier! <p>Problems Identified: - Row 3: 6 BHK in only 1020 sqft = 170 sqft per room (impossible!) - Row 4: \u20b912 million per sqft (data entry error, probably meant \u20b912,000) - Row 7: 8 BHK in 1200 sqft = 150 sqft per room (unrealistic)</p>"},{"location":"day1/day1/#62-why-remove-outliers","title":"6.2 Why Remove Outliers?","text":"<p>Imagine teaching someone about typical house prices by showing them:</p> <ul> <li>99 normal houses (\u20b930-80 lakhs)</li> <li>1 ultra-luxury mansion (\u20b9500 lakhs)</li> </ul> <p>They might develop a skewed understanding. Similarly, outliers can mislead machine learning models.</p>"},{"location":"day1/day1/#63-domain-based-outlier-removal","title":"6.3 Domain-Based Outlier Removal","text":"<p>Rule of Thumb: In urban Indian housing, a reasonable minimum is about 300 square feet per bedroom.</p> <p>Logic: </p> <ul> <li>1 BHK should have at least 300 sq ft</li> <li>2 BHK should have at least 600 sq ft</li> <li>3 BHK should have at least 900 sq ft</li> </ul> <p>Properties below these thresholds are likely data entry errors or exceptional cases we should exclude.</p> <p>Example - Before:</p> Location BHK Total_sqft Price Sqft_per_room Status Whitefield 2 1200 65 600 \u2713 Valid Marathahalli 3 1500 90 500 \u2713 Valid Koramangala 6 1020 80 170 \u2717 Remove HSR Layout 4 1800 110 450 \u2713 Valid Indiranagar 4 800 70 200 \u2717 Remove Electronic City 2 950 55 475 \u2713 Valid Yelahanka 5 1200 75 240 \u2717 Remove Hebbal 3 1350 85 450 \u2713 Valid <p>After Domain-Based Removal (minimum 300 sqft/room):</p> Location BHK Total_sqft Price Sqft_per_room Status Whitefield 2 1200 65 600 \u2713 Valid Marathahalli 3 1500 90 500 \u2713 Valid HSR Layout 4 1800 110 450 \u2713 Valid Electronic City 2 950 55 475 \u2713 Valid Hebbal 3 1350 85 450 \u2713 Valid <p>Result: Removed 3 properties with unrealistic sqft per room ratios</p>"},{"location":"day1/day1/#64-outlier-removal-using-box-plots-and-iqr","title":"6.4 Outlier Removal using Box Plots and IQR","text":""},{"location":"day1/day1/#box-plot-visualization","title":"Box Plot Visualization","text":"<p>A box plot (or whisker plot) is a graphical representation that helps visualize the spread and skewness of numerical data. It displays:</p> <ul> <li>Median (Q2) \u2014 The midpoint of the dataset, dividing it into two equal halves.</li> <li>First Quartile (Q1) \u2014 The 25th percentile \u2014 25% of the data lies below this value.</li> <li>Third Quartile (Q3) \u2014 The 75th percentile \u2014 75% of the data lies below this value.</li> <li>Interquartile Range (IQR) \u2014 The range between Q3 and Q1, calculated as IQR = Q3 - Q1.</li> <li>Whiskers: Extend from Q1 and Q3 to show variability outside the upper and lower quartiles.</li> <li>Outliers: Points plotted beyond the whiskers that indicate unusually high or low values.</li> </ul> <p>A box plot makes it easy to spot outliers visually, as they appear as isolated points away from the main data cluster.</p> <p>Steps:</p> <ol> <li> <p>Compute Q1 and Q3 \u2014 Find the 25th and 75th percentiles of the data.</p> <p>Suppose you have house prices (in lakhs):</p> <p>[10, 48, 55, 120, 125, 185, 600]</p> <p>Q1 (First Quartile / 25th Percentile)</p> <p>The value below which 25% of the data falls.</p> <p>Here, Q1 \u2248 48 \u2192 one-fourth of the data is below \u20b948 L.</p> <p>Q3 (Third Quartile / 75th Percentile)</p> <p>The value below which 75% of the data falls.</p> <p>Here, Q3 \u2248 185 \u2192 most data (three-fourths) is below \u20b9185 L.</p> </li> <li> <p>Calculate IQR</p> <p><code>\ud835\udc3c\ud835\udc44\ud835\udc45=\ud835\udc443\u2212\ud835\udc441</code></p> </li> <li> <p>Determine cutoff limits</p> <p><code>Lower bound = Q1 - 1.5 \u00d7 IQR</code></p> <p><code>Upper bound = Q3 + 1.5 \u00d7 IQR</code></p> </li> <li> <p>Identify and remove outliers \u2014 Any value less than the lower bound or greater than the upper bound is considered an outlier.</p> </li> </ol> <p></p> <p></p>"},{"location":"day1/day1/#7-data-normalization-and-standardization","title":"7. Data Normalization and Standardization","text":"<p>Before feeding data to machine learning models, we often need to scale our features to ensure they're on similar ranges.</p>"},{"location":"day1/day1/#71-why-scale-features","title":"7.1 Why Scale Features?","text":"<p>The Problem: Features with larger ranges can dominate the learning process.</p> <p>Example - Unscaled Data:</p> Location BHK Bath Total_sqft Price Whitefield 2 2 1200 60 Marathahalli 3 3 2500 120 Koramangala 2 1 800 40 HSR Layout 4 3 3000 150 Indiranagar 3 2 1800 95 Electronic City 2 2 1000 50 <p>Feature Ranges:</p> <ul> <li><code>Total_sqft</code>: 800 to 3000 (range = 2200)</li> <li><code>Bath</code>: 1 to 3 (range = 2)</li> <li><code>BHK</code>: 2 to 4 (range = 2)</li> </ul> <p>Notice how <code>Total_sqft</code> has a much larger range! Without scaling, models might give it disproportionate importance simply because of its larger numeric values.</p>"},{"location":"day1/day1/#72-standardization-z-score-normalization","title":"7.2 Standardization (Z-score Normalization)","text":"<p>Transforms data to have mean = 0 and standard deviation = 1.</p> <p>Formula: z = (x - \u03bc) / \u03c3</p> <p>When to use: When your data follows a normal distribution or when using algorithms like SVM, Linear Regression, or Logistic Regression.</p> <p>Example - After Standardization:</p> Location Total_sqft Standardized_sqft Bath Standardized_bath Whitefield 1200 -0.27 2 0.00 Marathahalli 2500 1.46 3 1.22 Koramangala 800 -1.18 1 -1.22 HSR Layout 3000 2.19 3 1.22 Indiranagar 1800 0.64 2 0.00 Electronic City 1000 -0.64 2 0.00 <p>Result:</p> <ul> <li>Mean \u2248 0 for both features</li> <li>Standard deviation = 1</li> <li>Values can be negative or positive</li> <li>Preserves outliers' relationships</li> </ul>"},{"location":"day1/day1/#73-normalization-min-max-scaling","title":"7.3 Normalization (Min-Max Scaling)","text":"<p>Scales data to a fixed range, typically [0, 1].</p> <p>Formula: x_scaled = (x - x_min) / (x_max - x_min)</p> <p>When to use: When you need a bounded range, especially for neural networks or when data doesn't follow normal distribution.</p> <p>Example - After Normalization:</p> Location Total_sqft Normalized_sqft Bath Normalized_bath Whitefield 1200 0.18 2 0.50 Marathahalli 2500 0.77 3 1.00 Koramangala 800 0.00 1 0.00 HSR Layout 3000 1.00 3 1.00 Indiranagar 1800 0.45 2 0.50 Electronic City 1000 0.09 2 0.50 <p>Result:</p> <ul> <li>All values between 0 and 1</li> <li>Minimum value becomes 0</li> <li>Maximum value becomes 1</li> <li>Preserves the relative distances between values</li> </ul> <p>Key Difference: </p> <ul> <li>Standardization: Values can be negative or &gt; 1; preserves outliers better</li> <li>Normalization: Always between [0,1]; more affected by outliers</li> </ul>"},{"location":"day1/day1/#8-preparing-data-for-machine-learning","title":"8. Preparing Data for Machine Learning","text":""},{"location":"day1/day1/#81-encoding-categorical-variables","title":"8.1 Encoding Categorical Variables","text":"<p>Machine learning algorithms work with numbers, not text. We need to convert categorical variables like \"location\" into numerical format.</p>"},{"location":"day1/day1/#811-one-hot-encoding","title":"8.1.1 One-Hot Encoding","text":"<p>Creates separate binary (0 or 1) columns for each category.</p> <p>Example - Before One-Hot Encoding:</p> Location BHK Bath Total_sqft Price Rajaji Nagar 2 2 1200 50 Hebbal 3 2 1500 75 Koramangala 2 1 1000 60 Rajaji Nagar 3 3 1400 70 Whitefield 4 3 2000 95 Hebbal 2 2 1100 55 <p>After One-Hot Encoding:</p> BHK Bath Total_sqft Price Rajaji_Nagar Hebbal Koramangala Whitefield 2 2 1200 50 1 0 0 0 3 2 1500 75 0 1 0 0 2 1 1000 60 0 0 1 0 3 3 1400 70 1 0 0 0 4 3 2000 95 0 0 0 1 2 2 1100 55 0 1 0 0 <p>Each location now has its own binary column. A \"1\" indicates the property is in that location.</p> <p>Advantages:</p> <ul> <li>Works well with algorithms that assume linear relationships</li> <li>No ordinal relationship assumed between categories</li> <li>Widely supported</li> </ul> <p>Disadvantages:</p> <ul> <li>Creates many columns for high-cardinality features</li> <li>Increases memory usage and computation time</li> <li>Can lead to sparse matrices</li> </ul> <p>Note: We don't create a column for \"other\" because if all location columns are 0, the model knows it's \"other.\"</p>"},{"location":"day1/day1/#812-label-encoding","title":"8.1.2 Label Encoding","text":"<p>Assigns a unique integer to each category.</p> <p>Example:</p> Location BHK Price Label_Encoded Rajaji Nagar 2 50 0 Hebbal 3 75 1 Koramangala 2 60 2 Rajaji Nagar 3 70 0 Whitefield 4 95 3 Hebbal 2 55 1 <p>Each unique location gets a single integer. Memory efficient but implies order.</p> <p>Advantages:</p> <ul> <li>Memory efficient (single column)</li> <li>Simple and fast</li> </ul> <p>Disadvantages:</p> <ul> <li>Implies ordinal relationship (Koramangala &gt; Hebbal &gt; Rajaji Nagar)</li> <li>Can mislead algorithms like Linear Regression</li> <li>Best for: Ordinal data (e.g., Low, Medium, High) or tree-based models</li> </ul>"},{"location":"day1/day1/#813-targetmean-encoding","title":"8.1.3 Target/Mean Encoding","text":"<p>Replaces categories with the mean of the target variable for that category.</p> <p>Example - Original Data:</p> Location BHK Price Rajaji Nagar 2 50 Hebbal 3 75 Koramangala 2 60 Rajaji Nagar 3 70 Hebbal 2 80 Koramangala 4 65 <p>After Target Encoding (using mean price per location):</p> Location BHK Price Location_Encoded Rajaji Nagar 2 50 60.0 Hebbal 3 75 77.5 Koramangala 2 60 62.5 Rajaji Nagar 3 70 60.0 Hebbal 2 80 77.5 Koramangala 4 65 62.5 <p>Each location is replaced by its average price (Rajaji Nagar = 60, Hebbal = 77.5, Koramangala = 62.5)</p> <p>Advantages:</p> <ul> <li>Captures relationship between category and target</li> <li>Single column (memory efficient)</li> </ul> <p>Disadvantages:</p> <ul> <li>Risk of data leakage</li> <li>Can overfit</li> <li>Best for: High-cardinality features in tree-based models</li> </ul>"},{"location":"day1/day1/#814-frequency-encoding","title":"8.1.4 Frequency Encoding","text":"<p>Replaces categories with their frequency count or percentage.</p> <p>Example:</p> Location BHK Price Frequency_Count Frequency_Pct Rajaji Nagar 2 50 250 0.25 Hebbal 3 75 180 0.18 Koramangala 2 60 220 0.22 Rajaji Nagar 3 70 250 0.25 Whitefield 4 95 200 0.20 Hebbal 2 55 180 0.18 <p>Popular locations get higher frequency values (Rajaji Nagar appears 250 times = 25% of dataset)</p> <p>Best for: When frequency itself is predictive (e.g., popular locations might be more expensive)</p> <p>Comparison Summary:</p> Method Columns Created Memory Preserves Info Risk of Overfitting Best Use Case One-Hot Many (n-1) High Yes Low Linear models, nominal data Label 1 Low Partial Low Tree models, ordinal data Target 1 Low Yes High Tree models, high cardinality Frequency 1 Low Partial Medium When frequency matters <p>For our project, we'll use One-Hot Encoding as it works well with linear models and doesn't assume any ordinal relationship between locations.</p>"},{"location":"day1/day1/#82-separating-features-and-target","title":"8.2 Separating Features and Target","text":"<p>Features (X): The input variables we use to make predictions - Total square feet - Number of bathrooms - Number of bedrooms (BHK) - Location (one-hot encoded)</p> <p>Target (y): What we're trying to predict - Price</p> <p>This separation is crucial because we train the model to find patterns between X and y.</p>"},{"location":"day1/day1/#9-data-visualization-seeing-patterns","title":"9. Data Visualization: Seeing Patterns","text":"<p>Visualization helps us understand our data intuitively.</p> <p></p>"},{"location":"day1/day1/#91-histogram-of-price-per-square-foot","title":"9.1 Histogram of Price Per Square Foot","text":"<p>A histogram shows the distribution of values: - X-axis: Price ranges (e.g., \u20b93,000-4,000, \u20b94,000-5,000) - Y-axis: How many properties fall in each range</p> <p>What to look for: - Where most properties are concentrated - Whether the distribution is normal (bell-shaped) - Presence of extreme values</p> <p></p>"},{"location":"day1/day1/#92-scatter-plots-for-outlier-detection","title":"9.2 Scatter Plots for Outlier Detection","text":"<p>Purpose: Compare 2 BHK vs. 3 BHK properties in the same location.</p> <p>Axes: - X-axis: Total square feet - Y-axis: Price</p> <p>What we expect:  - 3 BHK properties (green) should generally be above 2 BHK properties (blue) for the same square footage - Both should show an upward trend (more sq ft = higher price)</p> <p>Red Flags: - Blue dots (2 BHK) above green crosses (3 BHK) at the same square footage - Properties that don't follow the general upward trend</p> <p></p>"},{"location":"day1/day1/#10-summary-of-day-1-concepts","title":"10. Summary of Day 1 Concepts","text":"<p>Today we've learned that successful machine learning requires careful preparation:</p> <ol> <li>Clean your data: Remove inconsistencies, handle missing values, standardize formats</li> <li>Engineer features: Create meaningful variables like price per sq ft</li> <li>Remove outliers: Eliminate extreme values using domain knowledge and statistical methods</li> <li>Prepare for algorithms: Convert categories to numbers, separate features from targets</li> </ol>"},{"location":"day1/day1/#11-now-lets-begin-to-code","title":"11. Now let's begin to Code!","text":"<p>Open the Colab Link, Make a Copy and Upload the dataset on Colab</p> <p>\ud83d\udcd3 Colab Notebook: Open in Google Colab</p> <p>\ud83d\udcca Dataset: day1.zip - Contains the Bangalore house prices dataset</p> <p>See you next week! \ud83d\ude80</p>"},{"location":"day2/day2/","title":"Coming Soon","text":""},{"location":"day3/day3/","title":"Coming Soon","text":""},{"location":"day4/day4/","title":"Coming Soon","text":""},{"location":"files/day4/code_which/","title":"Code which","text":"In\u00a0[\u00a0]: Copied! <pre>import torch\nimport torch.nn as nn\nfrom torchvision import datasets, transforms, models\n\n# \u2705 Device configuration (GPU if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# \u2705 Data transforms\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.Grayscale(num_output_channels=3),\n    transforms.ToTensor(),\n    transforms.Normalize((0.485, 0.456, 0.406),\n                         (0.229, 0.224, 0.225))\n])\n\n# \u2705 Datasets &amp; DataLoader\ntrain_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n\n# \u2705 Model setup\nmodel = models.resnet18(weights=None)\nmodel.fc = nn.Linear(model.fc.in_features, 10)\nmodel = model.to(device)\n\n# \u2705 Loss &amp; optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\n# \u2705 Training loop with progress display\nnum_epochs = \ntotal_steps = len(train_loader)\ntotal_images = len(train_dataset)\n\nfor epoch in range(num_epochs):\n    running_loss = 0.0\n    for batch_idx, (imgs, labels) in enumerate(train_loader):\n        imgs, labels = imgs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(imgs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n        # Print progress\n        processed = (batch_idx + 1) * imgs.size(0)\n        print(f\"Epoch [{epoch+1}/{num_epochs}] | \"\n              f\"Step [{batch_idx+1}/{total_steps}] | \"\n              f\"Images: {processed}/{total_images} | \"\n              f\"Loss: {loss.item():.4f}\", end=\"\\r\")\n\n    print(f\"\\n\u2705 Epoch [{epoch+1}/{num_epochs}] completed. \"\n          f\"Average Loss: {running_loss / total_steps:.4f}\\n\")\n\nprint(\"\ud83c\udf89 Training complete!\")\n</pre> import torch import torch.nn as nn from torchvision import datasets, transforms, models  # \u2705 Device configuration (GPU if available) device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") print(f\"Using device: {device}\")  # \u2705 Data transforms transform = transforms.Compose([     transforms.Resize((224, 224)),     transforms.Grayscale(num_output_channels=3),     transforms.ToTensor(),     transforms.Normalize((0.485, 0.456, 0.406),                          (0.229, 0.224, 0.225)) ])  # \u2705 Datasets &amp; DataLoader train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform) train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)  # \u2705 Model setup model = models.resnet18(weights=None) model.fc = nn.Linear(model.fc.in_features, 10) model = model.to(device)  # \u2705 Loss &amp; optimizer criterion = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)  # \u2705 Training loop with progress display num_epochs =  total_steps = len(train_loader) total_images = len(train_dataset)  for epoch in range(num_epochs):     running_loss = 0.0     for batch_idx, (imgs, labels) in enumerate(train_loader):         imgs, labels = imgs.to(device), labels.to(device)          optimizer.zero_grad()         outputs = model(imgs)         loss = criterion(outputs, labels)         loss.backward()         optimizer.step()          running_loss += loss.item()          # Print progress         processed = (batch_idx + 1) * imgs.size(0)         print(f\"Epoch [{epoch+1}/{num_epochs}] | \"               f\"Step [{batch_idx+1}/{total_steps}] | \"               f\"Images: {processed}/{total_images} | \"               f\"Loss: {loss.item():.4f}\", end=\"\\r\")      print(f\"\\n\u2705 Epoch [{epoch+1}/{num_epochs}] completed. \"           f\"Average Loss: {running_loss / total_steps:.4f}\\n\")  print(\"\ud83c\udf89 Training complete!\") In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport torch\n\n# Assuming 'imgs' and 'outputs' are from your last training batch\n# Get predicted class\n_, predicted = torch.max(outputs, 1)\n\n# \u2705 Denormalize images (since you used ImageNet normalization)\nmean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\nstd = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n\n# Move tensors to CPU for plotting\nimgs_cpu = imgs.detach().cpu()\npredicted_cpu = predicted.detach().cpu()\n\n# \u2705 Denormalize\nimages_denorm = imgs_cpu * std + mean\nimages_denorm = torch.clamp(images_denorm, 0, 1)  # keep pixel range valid\n\n# \u2705 Plot 15 images with predictions\nfig, axes = plt.subplots(1, 15, figsize=(15, 3))\nfor i in range(15):\n    ax = axes[i]\n    ax.imshow(images_denorm[i].permute(1, 2, 0))  # C\u00d7H\u00d7W \u2192 H\u00d7W\u00d7C\n    ax.set_title(f\"Pred: {predicted_cpu[i].item()}\", fontsize=8)\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()\n</pre> import matplotlib.pyplot as plt import torch  # Assuming 'imgs' and 'outputs' are from your last training batch # Get predicted class _, predicted = torch.max(outputs, 1)  # \u2705 Denormalize images (since you used ImageNet normalization) mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1) std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)  # Move tensors to CPU for plotting imgs_cpu = imgs.detach().cpu() predicted_cpu = predicted.detach().cpu()  # \u2705 Denormalize images_denorm = imgs_cpu * std + mean images_denorm = torch.clamp(images_denorm, 0, 1)  # keep pixel range valid  # \u2705 Plot 15 images with predictions fig, axes = plt.subplots(1, 15, figsize=(15, 3)) for i in range(15):     ax = axes[i]     ax.imshow(images_denorm[i].permute(1, 2, 0))  # C\u00d7H\u00d7W \u2192 H\u00d7W\u00d7C     ax.set_title(f\"Pred: {predicted_cpu[i].item()}\", fontsize=8)     ax.axis('off')  plt.tight_layout() plt.show()"},{"location":"files/day4/simple_cnn/","title":"Simple cnn","text":"<p>First, we start off with importing all the models required, for this particular workshop, we will be using pytorch</p> In\u00a0[\u00a0]: Copied! <pre>import torch\nimport torch.nn as nn\nfrom torchvision import datasets, transforms, models\nimport matplotlib.pyplot as plt\n</pre> import torch import torch.nn as nn from torchvision import datasets, transforms, models import matplotlib.pyplot as plt <p>The above cell is used to detect if there's CUDA functionality, basically if you have an Nvidia GPU, you can use that for model training, instead of using CPU power completely.</p> In\u00a0[7]: Copied! <pre># \u2705 Device configuration (GPU if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n</pre> # \u2705 Device configuration (GPU if available) device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") print(f\"Using device: {device}\") <pre>Using device: cuda\n</pre> <p>Now, the model we will be focusing on is called Resnet-50. For resnet-50, the input required is 224 x 224 x 3, but then the image size of the dataset is 28 x 28 x 1. So, for resnet-50, we will need to transform to 224 x 224, and now, its only 1 channel present in the image, but then the input taken by the model is 3 channels, so we use the greyscale function to convert to 3 channels, and then we convert images to tensor values, and we normalize it.</p> In\u00a0[10]: Copied! <pre>transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.Grayscale(num_output_channels=3),\n    transforms.ToTensor(),\n    transforms.Normalize((0.485, 0.456, 0.406),\n                         (0.229, 0.224, 0.225))\n])\n</pre> transform = transforms.Compose([     transforms.Resize((224, 224)),     transforms.Grayscale(num_output_channels=3),     transforms.ToTensor(),     transforms.Normalize((0.485, 0.456, 0.406),                          (0.229, 0.224, 0.225)) ])  <p>This is to load the MNIST dataset (which is a digit classification dataset, has multiple hand written images from 0-9).</p> In\u00a0[\u00a0]: Copied! <pre>train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n</pre> train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform) train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True) <p>This is to load the resnet-18 model, and the resnet-18 model initially outputs 1000 classes, but then we only need 10 (0-9), so we mention 10 as well to output only 10 classes.</p> In\u00a0[\u00a0]: Copied! <pre>model = models.resnet18(weights=None)\nmodel.fc = nn.Linear(model.fc.in_features, 10)\nmodel = model.to(device)\n</pre> model = models.resnet18(weights=None) model.fc = nn.Linear(model.fc.in_features, 10) model = model.to(device) <p>This initializes the loss and optimizer required.</p> In\u00a0[\u00a0]: Copied! <pre># \u2705 Loss &amp; optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n</pre> # \u2705 Loss &amp; optimizer criterion = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=1e-4) <p>Initializes the number of epochs ( the number of times the model trains ), and total images</p> In\u00a0[\u00a0]: Copied! <pre>num_epochs = 2\ntotal_steps = len(train_loader)\ntotal_images = len(train_dataset)\n</pre> num_epochs = 2 total_steps = len(train_loader) total_images = len(train_dataset)  <p>Training with log information as well</p> In\u00a0[\u00a0]: Copied! <pre>for epoch in range(num_epochs):\n    running_loss = 0.0\n    for batch_idx, (imgs, labels) in enumerate(train_loader):\n        imgs, labels = imgs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(imgs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n        # Print progress\n        processed = (batch_idx + 1) * imgs.size(0)\n        print(f\"Epoch [{epoch+1}/{num_epochs}] | \"\n              f\"Step [{batch_idx+1}/{total_steps}] | \"\n              f\"Images: {processed}/{total_images} | \"\n              f\"Loss: {loss.item():.4f}\", end=\"\\r\")\n\n    print(f\"\\n\u2705 Epoch [{epoch+1}/{num_epochs}] completed. \"\n          f\"Average Loss: {running_loss / total_steps:.4f}\\n\")\n\nprint(\"\ud83c\udf89 Training complete!\")\n</pre> for epoch in range(num_epochs):     running_loss = 0.0     for batch_idx, (imgs, labels) in enumerate(train_loader):         imgs, labels = imgs.to(device), labels.to(device)          optimizer.zero_grad()         outputs = model(imgs)         loss = criterion(outputs, labels)         loss.backward()         optimizer.step()          running_loss += loss.item()          # Print progress         processed = (batch_idx + 1) * imgs.size(0)         print(f\"Epoch [{epoch+1}/{num_epochs}] | \"               f\"Step [{batch_idx+1}/{total_steps}] | \"               f\"Images: {processed}/{total_images} | \"               f\"Loss: {loss.item():.4f}\", end=\"\\r\")      print(f\"\\n\u2705 Epoch [{epoch+1}/{num_epochs}] completed. \"           f\"Average Loss: {running_loss / total_steps:.4f}\\n\")  print(\"\ud83c\udf89 Training complete!\") <p>To visualize 15 predictions, using matplotlib</p> In\u00a0[\u00a0]: Copied! <pre># Assuming 'imgs' and 'outputs' are from your last training batch\n# Get predicted class\n_, predicted = torch.max(outputs, 1)\n\n# \u2705 Denormalize images (since you used ImageNet normalization)\nmean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\nstd = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n\n# Move tensors to CPU for plotting\nimgs_cpu = imgs.detach().cpu()\npredicted_cpu = predicted.detach().cpu()\n\n# \u2705 Denormalize\nimages_denorm = imgs_cpu * std + mean\nimages_denorm = torch.clamp(images_denorm, 0, 1)  # keep pixel range valid\n\n# \u2705 Plot 15 images with predictions\nfig, axes = plt.subplots(1, 15, figsize=(15, 3))\nfor i in range(15):\n    ax = axes[i]\n    ax.imshow(images_denorm[i].permute(1, 2, 0))  # C\u00d7H\u00d7W \u2192 H\u00d7W\u00d7C\n    ax.set_title(f\"Pred: {predicted_cpu[i].item()}\", fontsize=8)\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()\n</pre> # Assuming 'imgs' and 'outputs' are from your last training batch # Get predicted class _, predicted = torch.max(outputs, 1)  # \u2705 Denormalize images (since you used ImageNet normalization) mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1) std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)  # Move tensors to CPU for plotting imgs_cpu = imgs.detach().cpu() predicted_cpu = predicted.detach().cpu()  # \u2705 Denormalize images_denorm = imgs_cpu * std + mean images_denorm = torch.clamp(images_denorm, 0, 1)  # keep pixel range valid  # \u2705 Plot 15 images with predictions fig, axes = plt.subplots(1, 15, figsize=(15, 3)) for i in range(15):     ax = axes[i]     ax.imshow(images_denorm[i].permute(1, 2, 0))  # C\u00d7H\u00d7W \u2192 H\u00d7W\u00d7C     ax.set_title(f\"Pred: {predicted_cpu[i].item()}\", fontsize=8)     ax.axis('off')  plt.tight_layout() plt.show()"}]}