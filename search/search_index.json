{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ACM ML Workshop \u2014 Home","text":"<p>Welcome to the ACM Machine Learning Workshop! This workshop is designed to introduce you to the fundamentals of machine learning, with a focus on practical applications and hands-on exercises. Each day builds on the previous one, covering key concepts and techniques.</p>"},{"location":"#workshop-goals","title":"Workshop Goals","text":"<ul> <li>Understand the machine learning pipeline from data preparation to deployment.</li> <li>Learn and implement key ML algorithms and techniques.</li> <li>Gain hands-on experience with real-world datasets and tools.</li> </ul>"},{"location":"#schedule-overview","title":"Schedule Overview","text":"<ul> <li>Day 1: Data Cleaning and Feature Engineering</li> <li>Learn the importance of data cleaning and how to handle missing values, outliers, and inconsistent formats.</li> <li>Perform feature engineering to create meaningful variables like price per square foot.</li> <li> <p>Prepare data for machine learning by encoding categories and removing irrelevant features.</p> </li> <li> <p>Day 2: Model Training and Analysis</p> </li> <li>Explore the fundamentals of supervised learning, including regression and classification problems.</li> <li>Learn about overfitting, underfitting, and the importance of train-test splits.</li> <li> <p>Implement and evaluate models such as Linear Regression, Decision Trees, Random Forests, and Support Vector Machines.</p> </li> <li> <p>Day 3: Natural Language Processing (NLP)</p> </li> <li>Understand the basics of NLP and its applications.</li> <li>Learn about word embeddings like Word2Vec, GloVe, and FastText.</li> <li>Explore RNNs, LSTMs, and Transformer models like BERT and GPT.</li> <li> <p>Implement next-word prediction using a pre-trained GPT model.</p> </li> <li> <p>Day 4: Intro to Deep Learning &amp; Deployment Basics</p> </li> <li>Understand the basics of Artificial Neural Networks (ANNs) and Convolutional Neural Networks (CNNs).</li> <li>Learn about famous CNN architectures like LeNet, AlexNet, and ResNet.</li> <li>Get an introduction to hosting models on platforms like Hugging Face.</li> </ul>"},{"location":"#resources-next-steps","title":"Resources &amp; Next Steps","text":"<ul> <li>Recommended reading and tutorials will be shared during the sessions.</li> <li>After the workshop, try applying what you've learned to a small end-to-end project.</li> <li>Questions? Feel free to ask during the sessions or reach out via the repository.</li> </ul> <p>We hope you enjoy the workshop and gain valuable insights into the world of machine learning!</p>"},{"location":"day1/day1/","title":"Day 1: Data Cleaning and Feature Engineering","text":""},{"location":"day1/day1/#introduction-to-real-estate-price-prediction","title":"Introduction to Real Estate Price Prediction","text":"<p>Welcome to Day 1 of our Machine Learning workshop! Today, we'll embark on an exciting journey to build a real estate price prediction model using data from Bangalore, India. Before we dive into coding, let's understand the fundamental concepts that make machine learning projects successful.</p>"},{"location":"day1/day1/#understanding-the-problem","title":"Understanding the Problem","text":"<p>Imagine you're a real estate agent or a home buyer trying to determine the fair price of a property. What factors would you consider? The location, size of the house, number of bedrooms, bathrooms, and many other features play crucial roles. Our goal is to teach a computer to understand these patterns and predict prices automatically.</p>"},{"location":"day1/day1/#the-machine-learning-pipeline","title":"The Machine Learning Pipeline","text":"<p>Every successful machine learning project follows a structured approach:</p> <ol> <li>Data Collection: Gathering relevant information</li> <li>Data Cleaning: Removing errors and inconsistencies</li> <li>Feature Engineering: Creating meaningful variables from raw data</li> <li>Exploratory Data Analysis: Understanding patterns in data</li> <li>Model Building: Training algorithms (we'll cover this in Day 2)</li> <li>Model Evaluation: Testing how well our model performs</li> </ol> <p>Today, we'll focus on the first four crucial steps - the foundation of any ML project.</p>"},{"location":"day1/day1/#1-data-cleaning-the-foundation-of-quality-models","title":"1. Data Cleaning: The Foundation of Quality Models","text":""},{"location":"day1/day1/#why-is-data-cleaning-important","title":"Why is Data Cleaning Important?","text":"<p>Think of data cleaning like preparing ingredients before cooking. You wouldn't use rotten vegetables or unwashed produce in a meal, right? Similarly, dirty data leads to poor predictions. Real-world data is messy - it has missing values, inconsistencies, duplicates, and errors that can mislead our model.</p>"},{"location":"day1/day1/#understanding-missing-values","title":"Understanding Missing Values","text":"<p>Example Scenario: Imagine a dataset of house listings where some entries don't have information about the number of bathrooms or the location. What should we do?</p> <p>Two Common Approaches: 1. Deletion: Remove rows with missing data (when dataset is large) 2. Imputation: Fill missing values with mean, median, or mode (when data is scarce)</p> <p>When to delete vs. impute? If you have 13,000 rows and only 1,000 have missing values, deletion is safe. But if 8,000 rows have missing values, you might want to impute to preserve information.</p>"},{"location":"day1/day1/#dealing-with-irrelevant-features","title":"Dealing with Irrelevant Features","text":"<p>Not every piece of information is useful. Consider these columns in a house price dataset: - Area Type: The type of area measurement (built-up, plot area, etc.) - Society Name: The specific housing society - Balcony Count: Number of balconies - Availability: When the house is available</p> <p>Question: Do these strongly influence price predictions? Often, the answer is no. Removing irrelevant features: - Simplifies the model - Reduces computational cost - Prevents overfitting - Improves model performance</p>"},{"location":"day1/day1/#standardizing-data-formats","title":"Standardizing Data Formats","text":"<p>The Problem: Your dataset has a \"size\" column with values like: - \"2 BHK\" - \"3 Bedroom\" - \"4 BHK\"</p> <p>The Solution: Extract just the numeric part (2, 3, 4) to create a consistent \"bhk\" (Bedroom, Hall, Kitchen) column that machines can understand.</p>"},{"location":"day1/day1/#handling-range-values","title":"Handling Range Values","text":"<p>Example: A property's size is listed as \"1133 - 1384 sq ft\" instead of a single number.</p> <p>Solution: Convert ranges to their average. For \"1133 - 1384\", we'd use (1133 + 1384) / 2 = 1258.5 sq ft.</p>"},{"location":"day1/day1/#cleaning-inconsistent-units","title":"Cleaning Inconsistent Units","text":"<p>Sometimes you'll find values like: - \"2500 sq ft\" - \"34.46 Sq. Meter\" - \"4125 Perch\"</p> <p>These mixed units make comparison impossible. The best approach is to convert everything to a standard unit or exclude entries that can't be converted reliably.</p>"},{"location":"day1/day1/#2-feature-engineering-creating-meaningful-variables","title":"2. Feature Engineering: Creating Meaningful Variables","text":"<p>Feature engineering is the art of creating new, more informative variables from existing data. It's often the difference between a mediocre and an excellent model.</p>"},{"location":"day1/day1/#creating-price-per-square-foot","title":"Creating Price Per Square Foot","text":"<p>Why? Absolute price doesn't tell the whole story. A 3000 sq ft house costing \u20b960 lakhs might be a better deal than a 1000 sq ft house at \u20b930 lakhs.</p> <p>Calculation: Price per sq ft = (Price \u00d7 100,000) / Total Square Feet</p> <p>This normalized metric helps us compare properties of different sizes on equal footing.</p>"},{"location":"day1/day1/#grouping-rare-categories","title":"Grouping Rare Categories","text":"<p>The Problem: Your dataset has 1,293 unique locations, but 1,052 of them appear fewer than 10 times.</p> <p>The Solution: Group infrequent categories into an \"other\" category. Why?</p> <ol> <li>Statistical Significance: Locations with only 1-2 properties don't provide enough data for reliable patterns</li> <li>Model Simplicity: Fewer categories mean fewer variables to process</li> <li>Generalization: Helps the model focus on common patterns rather than rare exceptions</li> </ol> <p>Real-world analogy: If you're learning to recognize cars, you'd focus on common brands like Toyota, Honda, and Ford before worrying about rare vintage models.</p>"},{"location":"day1/day1/#3-outlier-detection-and-removal","title":"3. Outlier Detection and Removal","text":"<p>Outliers are extreme values that don't fit the general pattern. They can severely distort your model's understanding of the data.</p>"},{"location":"day1/day1/#what-are-outliers","title":"What are Outliers?","text":"<p>Example 1: A 6-bedroom house with only 1,020 square feet total. That's roughly 170 sq ft per room - smaller than most bathrooms! This is clearly an error or exceptional case.</p> <p>Example 2: A property listed at \u20b912,000,000 per square foot when most properties in that area are \u20b95,000-10,000 per sq ft.</p>"},{"location":"day1/day1/#why-remove-outliers","title":"Why Remove Outliers?","text":"<p>Imagine teaching someone about typical house prices by showing them: - 99 normal houses (\u20b930-80 lakhs) - 1 ultra-luxury mansion (\u20b9500 lakhs)</p> <p>They might develop a skewed understanding. Similarly, outliers can mislead machine learning models.</p>"},{"location":"day1/day1/#domain-based-outlier-removal","title":"Domain-Based Outlier Removal","text":"<p>Rule of Thumb: In urban Indian housing, a reasonable minimum is about 300 square feet per bedroom.</p> <p>Logic:  - 1 BHK should have at least 300 sq ft - 2 BHK should have at least 600 sq ft - 3 BHK should have at least 900 sq ft</p> <p>Properties below these thresholds are likely data entry errors or exceptional cases we should exclude.</p>"},{"location":"day1/day1/#outlier-removal-using-box-plots-and-iqr","title":"Outlier Removal using Box Plots and IQR","text":""},{"location":"day1/day1/#box-plot-visualization","title":"Box Plot Visualization","text":"<p>A box plot (or whisker plot) is a graphical representation that helps visualize the spread and skewness of numerical data. It displays:</p> <ul> <li>Median (Q2) \u2014 The middle value of the dataset.</li> <li>First Quartile (Q1) \u2014 The 25th percentile (lower quartile).</li> <li>Third Quartile (Q3) \u2014 The 75th percentile (upper quartile).</li> <li>Interquartile Range (IQR) \u2014 The difference between Q3 and Q1 (IQR = Q3 - Q1).</li> <li>Whiskers and Points \u2014 Points outside the whiskers represent potential outliers.</li> </ul> <p>A box plot makes it easy to visually identify outliers \u2014 these are the points that appear outside the whiskers (i.e., far from the main cluster of data).</p>"},{"location":"day1/day1/#interquartile-range-iqr-method","title":"Interquartile Range (IQR) Method","text":"<p>The IQR method is a statistical technique used to detect and remove outliers.</p> <p>Steps:</p> <ol> <li> <p>Compute Q1 and Q3 \u2014 Find the 25th and 75th percentiles of the data.</p> </li> <li> <p>Calculate IQR</p> <p><code>\ud835\udc3c\ud835\udc44\ud835\udc45=\ud835\udc443\u2212\ud835\udc441</code></p> </li> <li> <p>Determine cutoff limits</p> <p><code>Lower bound = Q1 - 1.5 \u00d7 IQR</code></p> <p><code>Upper bound = Q3 + 1.5 \u00d7 IQR</code></p> </li> <li> <p>Identify and remove outliers \u2014 Any value less than the lower bound or greater than the upper bound is considered an outlier.</p> </li> </ol>"},{"location":"day1/day1/#4-preparing-data-for-machine-learning","title":"4. Preparing Data for Machine Learning","text":""},{"location":"day1/day1/#one-hot-encoding-converting-categories-to-numbers","title":"One-Hot Encoding: Converting Categories to Numbers","text":"<p>The Challenge: Machine learning algorithms work with numbers, not text. How do we handle the \"location\" column?</p> <p>Example: You have three locations: - Rajaji Nagar - Hebbal - Koramangala</p> <p>One-Hot Encoding Solution: Create separate binary (0 or 1) columns for each location:</p> Price BHK Rajaji_Nagar Hebbal Koramangala 50 2 1 0 0 75 3 0 1 0 60 2 0 0 1 <p>The \"Other\" Category: We don't create a column for \"other\" because if all location columns are 0, the model knows it's \"other.\"</p>"},{"location":"day1/day1/#separating-features-and-target","title":"Separating Features and Target","text":"<p>Features (X): The input variables we use to make predictions - Total square feet - Number of bathrooms - Number of bedrooms (BHK) - Location (one-hot encoded)</p> <p>Target (y): What we're trying to predict - Price</p> <p>This separation is crucial because we train the model to find patterns between X and y.</p>"},{"location":"day1/day1/#data-visualization-seeing-patterns","title":"Data Visualization: Seeing Patterns","text":"<p>Visualization helps us understand our data intuitively.</p>"},{"location":"day1/day1/#histogram-of-price-per-square-foot","title":"Histogram of Price Per Square Foot","text":"<p>A histogram shows the distribution of values: - X-axis: Price ranges (e.g., \u20b93,000-4,000, \u20b94,000-5,000) - Y-axis: How many properties fall in each range</p> <p>What to look for: - Where most properties are concentrated - Whether the distribution is normal (bell-shaped) - Presence of extreme values</p>"},{"location":"day1/day1/#scatter-plots-for-outlier-detection","title":"Scatter Plots for Outlier Detection","text":"<p>Purpose: Compare 2 BHK vs. 3 BHK properties in the same location.</p> <p>Axes: - X-axis: Total square feet - Y-axis: Price</p> <p>What we expect:  - 3 BHK properties (green) should generally be above 2 BHK properties (blue) for the same square footage - Both should show an upward trend (more sq ft = higher price)</p> <p>Red Flags: - Blue dots (2 BHK) above green crosses (3 BHK) at the same square footage - Properties that don't follow the general upward trend</p>"},{"location":"day1/day1/#summary-of-day-1-concepts","title":"Summary of Day 1 Concepts","text":"<p>Today we've learned that successful machine learning requires careful preparation:</p> <ol> <li>Clean your data: Remove inconsistencies, handle missing values, standardize formats</li> <li>Engineer features: Create meaningful variables like price per sq ft</li> <li>Remove outliers: Eliminate extreme values using domain knowledge and statistical methods</li> <li>Prepare for algorithms: Convert categories to numbers, separate features from targets</li> </ol> <p>Key Takeaway: \"Garbage in, garbage out.\" The quality of your data directly determines the quality of your predictions. Spending time on data cleaning and feature engineering is not optional - it's essential.</p> <p>Tomorrow, we'll take this cleaned dataset and build machine learning models to predict house prices!</p>"},{"location":"day1/day1/#download-material","title":"\ud83d\udce5 Download Material","text":"<p>\ufffd Download All Day 1 Materials (ZIP): day1_materials.zip - Contains the Jupyter notebook and dataset</p> <p>Run each cell step by step to see data cleaning and feature engineering in action!</p>"},{"location":"day1/day1/#whats-next","title":"What's Next?","text":"<p>In Day 2, we'll take this beautifully cleaned dataset and: - Build machine learning models - Compare different algorithms - Evaluate model performance - Make actual price predictions</p> <p>See you nect week! \ud83d\ude80</p>"},{"location":"day2/day2/","title":"ML Workshop Day 2: Model Training and Analysis","text":""},{"location":"day2/day2/#complete-documentation-guide","title":"Complete Documentation Guide","text":""},{"location":"day2/day2/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Overview</li> <li>Dataset Information</li> <li>Overfitting and Underfitting</li> <li>Train-Test Split</li> <li>Types of ML Problems</li> <li>Regression Models</li> <li>Classification Models</li> <li>Evaluation Metrics</li> <li>Model Comparison</li> <li>Key Insights</li> </ol>"},{"location":"day2/day2/#overview","title":"\ud83c\udfaf Overview","text":"<p>This workshop introduces fundamental machine learning concepts with practical implementation. You'll learn how to: - Split data for training and testing - Implement multiple regression and classification models - Evaluate model performance - Compare and select the best model</p> <p>Dataset: Real estate pricing data </p>"},{"location":"day2/day2/#dataset-information","title":"\ud83d\udcca Dataset Information","text":""},{"location":"day2/day2/#original-dataset","title":"Original Dataset","text":"<ul> <li>Total Records: 10,835 properties</li> <li>Features: 246 (after preprocessing)</li> <li>Target Variable: <code>price</code> (continuous numerical value) - that we want to predict</li> </ul>"},{"location":"day2/day2/#feature-preparation","title":"Feature Preparation","text":"<pre><code># Separating features and target\nX = df[feature_cols]  # All columns except 'price'\ny = df['price']       # Target variable\n\n# Dataset shape\nFeatures: (10835, 244)\nTarget: (10835,)\n</code></pre>"},{"location":"day2/day2/#overfitting-and-underfitting","title":"Overfitting and Underfitting","text":"<p>When building machine learning models, the goal is to capture the true underlying patterns in data so the model can generalize to new, unseen examples.  </p> <p>However, models can sometimes go wrong in two common ways:</p> <ol> <li>Underfitting</li> <li>Overfitting </li> </ol> <p></p> <p>Striking the right balance between underfitting and overfitting is key to building robust machine learning models.</p>"},{"location":"day2/day2/#overfitting","title":"Overfitting","text":"<p>Overfitting happens when a model learns too much from the training data, including details that don\u2019t matter (like noise or outliers).</p> <p>Example : - Imagine fitting a very complicated curve to a set of points. The curve will go through every point, but it won\u2019t represent the actual pattern. - As a result, the model works great on training data but fails when tested on new data.</p> <p></p> <p>Reasons for Overfitting:</p> <ol> <li>High variance and low bias.</li> <li>The model is too complex.</li> <li>The size of the training data.</li> </ol>"},{"location":"day2/day2/#underfitting","title":"Underfitting","text":"<p>Underfitting is the opposite of overfitting. It happens when a model is too simple to capture what\u2019s going on in the data.</p> <p>Example: - Imagine drawing a straight line to fit points that actually follow a curve. The line misses most of the pattern. - In this case, the model doesn\u2019t work well on either the training or testing data.</p> <p></p> <p>Reasons for Underfitting:</p> <ol> <li>The model is too simple, So it may be not capable to represent the complexities in the data.</li> <li>The input features which is used to train the model is not the adequate representations of underlying factors influencing the target variable.</li> <li>The size of the training dataset used is not enough.</li> <li>Features are not scaled.</li> </ol>"},{"location":"day2/day2/#train-test-split","title":"\ud83d\udd00 Train-Test Split","text":""},{"location":"day2/day2/#what-is-train-test-split","title":"What is Train-Test Split?","text":"<p>Train-test split divides your dataset into two parts:</p> <p></p> <p>Training Set (80%): Used to teach the model - Model learns patterns from this data - Used for fitting/training algorithms</p> <p>Testing Set (20%): Used to evaluate the model - Model has never seen this data - Tests how well model generalizes to new data</p>"},{"location":"day2/day2/#implementation","title":"Implementation","text":"<pre><code>X_train, X_test, y_train, y_test = train_test_split(\n    X, y, \n    test_size=0.2,      # 20% for testing\n    random_state=42     # For reproducibility\n)\n</code></pre>"},{"location":"day2/day2/#why-split-data","title":"Why Split Data?","text":"<ul> <li>Prevents Overfitting: Model doesn't memorize training data</li> <li>Tests Generalization: Evaluates performance on unseen data</li> <li>Realistic Performance: Simulates real-world predictions</li> </ul>"},{"location":"day2/day2/#types-of-machine-learning-problems","title":"Types of Machine Learning Problems","text":"<p>In supervised learning, problems are usually divided into two types :  - Regression Problem - Classification Problem</p>"},{"location":"day2/day2/#regression-problem","title":"Regression Problem","text":"<ul> <li>Goal : To predict a continuous numeric value.</li> <li> <p>Regression models try to find relationships between input variables (features) and a continuous output.</p> </li> <li> <p>Examples:</p> <ul> <li>Predicting house prices \ud83c\udfe0</li> <li>Estimating temperature \ud83c\udf21\ufe0f</li> <li>Forecasting stock prices \ud83d\udcc8  </li> </ul> </li> <li> <p>Common Algorithms:</p> <ol> <li>Linear Regression    </li> <li>Decision Tree Regressor</li> <li>Random Forest Regressor</li> <li>Support Vector Regressor (SVR)</li> </ol> </li> <li> <p>Evaluation Metrics:</p> <ol> <li>Mean Squared Error (MSE)</li> <li>Root Mean Squared Error (RMSE)</li> <li>Mean Absolute Error (MAE)</li> <li>R\u00b2 Score</li> </ol> </li> </ul>"},{"location":"day2/day2/#classification-problem","title":"Classification Problem","text":"<ul> <li>Goal : To predict a discrete label or category.</li> <li> <p>Classification models learn to separate data into different classes.</p> </li> <li> <p>Examples:</p> <ul> <li>Email spam detection \u2709\ufe0f</li> <li>Disease diagnosis (positive/negative) \ud83e\uddec</li> <li>Image recognition (cat vs. dog) \ud83d\udc31\ud83d\udc36 </li> </ul> </li> <li> <p>Common Algorithms:</p> <ol> <li>Logistic Regression</li> <li>Decision Tree Classifier</li> <li>Random Forest Classifier</li> <li>Support Vector Machine (SVM)</li> <li>k-Nearest Neighbors (KNN)</li> </ol> </li> <li> <p>Evaluation Metrics:</p> <ol> <li>Accuracy</li> <li>Precision &amp; Recall</li> <li>F1 Score</li> <li>Confusion Matrix</li> </ol> </li> </ul>"},{"location":"day2/day2/#regression-models","title":"\ud83d\udcc8 Regression Models","text":"<p>Regression predicts continuous numerical values (e.g., house prices, temperature, sales).</p>"},{"location":"day2/day2/#1-linear-regression","title":"1. Linear Regression","text":"<p>How it works: Finds the best straight line through your data points.</p> <p>Mathematical Formula: </p> <pre><code>\u0177 = \u03b2\u2080 + \u03b2\u2081x\u2081 + \u03b2\u2082x\u2082 + ... + \u03b2\u2099x\u2099\n\nWhere:\n\u0177 = predicted value\n\u03b2\u2080 = intercept (bias)\n\u03b2\u2081, \u03b2\u2082, ..., \u03b2\u2099 = coefficients (weights)\nx\u2081, x\u2082, ..., x\u2099 = feature values\n</code></pre> <p>Simple form: <code>y = mx + b</code> - Simple and interpretable - Assumes linear relationship between features and target</p> <p></p> <p>Strengths: - Fast to train - Easy to interpret - Works well with linear relationships</p> <p>Weaknesses: - Cannot capture complex non-linear patterns - Sensitive to outliers - basically those values that are much out of range when compared to normal values</p> <p>Use cases : - Predicting house prices based on area, location, etc. - Estimating sales revenue from advertising spend. - Forecasting demand or performance metrics. </p>"},{"location":"day2/day2/#2-decision-tree-regressor","title":"2. Decision Tree Regressor","text":"<p>How it works: Creates a tree of yes/no questions to make predictions.</p> <p>Example:</p> <pre><code>Is size &gt; 2000 sq ft?\n  \u251c\u2500 Yes \u2192 Is location = downtown?\n  \u2502         \u251c\u2500 Yes \u2192 Predict $500k\n  \u2502         \u2514\u2500 No \u2192 Predict $350k\n  \u2514\u2500 No \u2192 Predict $250k\n</code></pre> <p></p> <p>Mathematical Formula :</p> <pre><code>Prediction at leaf node = (1/n) \u03a3\u1d62\u208c\u2081\u207f y\u1d62\n\nWhere:\nn = number of samples in the leaf\ny\u1d62 = actual values in the leaf\n(Takes the mean of training samples that reach that leaf)\n\nSplit criterion (MSE):\nMSE = (1/n) \u03a3\u1d62\u208c\u2081\u207f (y\u1d62 - \u0177)\u00b2\n</code></pre> <p>Strengths: - Handles non-linear relationships - Easy to visualize and understand - No feature scaling needed</p> <p>Weaknesses: - Can overfit easily - Sensitive to small data changes - May create overly complex trees</p> <p>Use cases : - Predicting sales based on season, location, and marketing. - Modeling complex, non-linear data patterns.  </p>"},{"location":"day2/day2/#3-random-forest-regressor","title":"3. Random Forest Regressor","text":"<p>How it works: Creates many decision trees and averages their predictions.</p> <p>Think of it as: A committee of experts voting on the answer - Each tree sees slightly different data - Final prediction = average of all trees - Reduces overfitting compared to single tree</p> <p></p> <p>Mathematical Formula :</p> <pre><code>\u0177 = (1/T) \u03a3\u209c\u208c\u2081\u1d40 h\u209c(x)\n\nWhere:\nT = number of trees in the forest\nh\u209c(x) = prediction from tree t\n\u0177 = final prediction (average of all trees)\n</code></pre> <p>Strengths: - More accurate than single decision tree - Handles complex relationships - Reduces overfitting - Shows feature importance</p> <p>Weaknesses: - Slower to train - Less interpretable - Requires more memory</p> <p>Use Cases : - Predicting house prices, insurance claim amounts. - Forecasting demand or energy consumption. </p>"},{"location":"day2/day2/#4-support-vector-regressor-svr","title":"4. Support Vector Regressor (SVR)","text":"<p>How it works:  - Fits a line or curve that predicts most of the data within a \u201ctube\u201d of tolerance (\ud835\udf16). - Focuses only on points that lie on or outside the tube (support vectors). - Can handle nonlinear patterns using kernel functions (like RBF).</p> <p>Key Concept:  - Uses kernel tricks for non-linear patterns - Focuses on data points that define boundaries - RBF kernel used in your implementation</p> <p>Mathematical Formula :</p> <pre><code>Minimize: (1/2)||w||\u00b2 + C \u03a3\u1d62\u208c\u2081\u207f (\u03be\u1d62 + \u03be\u1d62*)\n\nSubject to:\n|y\u1d62 - (w\u00b7x\u1d62 + b)| \u2264 \u03b5 + \u03be\u1d62\n\nWhere:\nw = weight vector\n\u03b5 = epsilon (tube width)\nC = penalty parameter\n\u03be\u1d62 = slack variables\n</code></pre> <p></p> <p>Strengths: - Effective in high-dimensional spaces - Memory efficient - Robust to outliers</p> <p>Weaknesses: - Slower on large datasets - Needs feature scaling - Difficult to interpret</p> <p>Use Cases : - Predicting stock prices or exchange rates. - Estimating real estate prices where outliers exist. </p>"},{"location":"day2/day2/#5-k-nearest-neighbors-knn-regressor","title":"5. K-Nearest Neighbors (KNN) Regressor","text":"<p>How it works: Predicts based on the K closest training examples.</p> <p>Example (K=5): - Find 5 nearest houses to your property - Average their prices - That's your prediction</p> <p>Mathematical Formula :</p> <pre><code>\u0177 = (1/K) \u03a3\u1d62\u208c\u2081\u1d37 y\u1d62\n\nWhere:\nK = number of nearest neighbors\ny\u1d62 = value of i-th nearest neighbor\n\nDistance (Euclidean):\nd(x, x\u1d62) = \u221a(\u03a3\u2c7c\u208c\u2081\u207f (x\u2c7c - x\u1d62\u2c7c)\u00b2)\n</code></pre> <p></p> <p>Strengths: - Simple to understand - No training phase (lazy learning) - Naturally handles non-linear patterns</p> <p>Weaknesses: - Slow predictions on large datasets - Needs feature scaling - Sensitive to irrelevant features</p> <p>Use Cases : - Estimating house rent based on nearby similar properties. - Predicting temperature using data from nearby weather stations.</p>"},{"location":"day2/day2/#classification-models","title":"\ud83c\udfaf Classification Models","text":"<p>Classification predicts categories/classes (e.g., spam/not spam, disease/healthy, high/medium/low price).</p>"},{"location":"day2/day2/#1-logistic-regression","title":"1. Logistic Regression","text":"<p>How it works: Despite the name, it's for classification! Predicts probability of belonging to a class.</p> <p>Example: Predicting if house is \"expensive\" or \"affordable\"</p> <pre><code>Probability = 1 / (1 + e^(-score))\nIf probability &gt; 0.5 \u2192 Expensive\nIf probability \u2264 0.5 \u2192 Affordable\n</code></pre> <p>Mathematical Formula :</p> <pre><code>P(y=1|x) = 1 / (1 + e^(-z))\n\nWhere:\nz = \u03b2\u2080 + \u03b2\u2081x\u2081 + \u03b2\u2082x\u2082 + ... + \u03b2\u2099x\u2099\nP(y=1|x) = probability of class 1\ne = Euler's number (\u22482.718)\n\nDecision: If P(y=1|x) &gt; 0.5 \u2192 Class 1\n          If P(y=1|x) \u2264 0.5 \u2192 Class 0\n</code></pre> <p></p> <p>Strengths: - Fast and efficient - Provides probability scores - Easy to interpret</p> <p>Weaknesses: - Assumes linear decision boundary - Not effective for complex relationships</p> <p>When to use: Binary classification with linearly separable data</p>"},{"location":"day2/day2/#2-decision-tree-classifier","title":"2. Decision Tree Classifier","text":"<p>How it works: Same tree structure as regression, but predicts categories.</p> <p>Example: Is size &gt; 2000 sq ft?   \u251c\u2500 Yes \u2192 Is location = downtown?   \u2502         \u251c\u2500 Yes \u2192 Class: Luxury   \u2502         \u2514\u2500 No \u2192 Class: Standard   \u2514\u2500 No \u2192 Class: Budget</p> <p>Mathematical Formula :</p> <pre><code>Gini Impurity = 1 - \u03a3\u1d62\u208c\u2081\u1d9c p\u1d62\u00b2\n\nWhere:\nc = number of classes\np\u1d62 = proportion of class i in node\n\nEntropy (alternative):\nH = -\u03a3\u1d62\u208c\u2081\u1d9c p\u1d62 log\u2082(p\u1d62)\n\n(Tree splits to minimize impurity)\n</code></pre> <p></p> <p>Strengths: - Handles non-linear boundaries - Interpretable - Works with categorical data</p> <p>Weaknesses: - Overfits easily - Unstable with small data changes</p> <p>When to use: When you need interpretability and have categorical data</p>"},{"location":"day2/day2/#3-random-forest-classifier","title":"3. Random Forest Classifier","text":"<p>How it works: Ensemble of decision trees voting on the class.</p> <p>Voting Example (5 trees): - Tree 1: Luxury - Tree 2: Standard - Tree 3: Luxury - Tree 4: Luxury - Tree 5: Standard - Final Prediction: Luxury (majority vote: 3/5)</p> <p>Mathematical Formula :</p> <pre><code>\u0177 = mode{h\u2081(x), h\u2082(x), ..., h\u209c(x)}\n\nWhere:\nT = number of trees\nh\u209c(x) = prediction from tree t\nmode = most frequent class (majority vote)\n\nFor probabilities:\nP(class=c|x) = (1/T) \u03a3\u209c\u208c\u2081\u1d40 I(h\u209c(x) = c)\n</code></pre> <p></p> <p>Strengths: - High accuracy - Reduces overfitting - Shows feature importance - Handles imbalanced data well</p> <p>Weaknesses: - Slower than single tree - Less interpretable - More memory intensive</p> <p>When to use: When accuracy is priority and you have sufficient data</p>"},{"location":"day2/day2/#4-support-vector-machine-svm-classifier","title":"4. Support Vector Machine (SVM) Classifier","text":"<p>How it works: Finds the best boundary (hyperplane) that separates classes with maximum margin.</p> <p>Mathematical Formula :</p> <pre><code>Minimize: (1/2)||w||\u00b2 + C \u03a3\u1d62\u208c\u2081\u207f \u03be\u1d62\n\nSubject to:\ny\u1d62(w\u00b7x\u1d62 + b) \u2265 1 - \u03be\u1d62\n\nWhere:\nw = weight vector (perpendicular to hyperplane)\nb = bias\nC = penalty parameter\n\u03be\u1d62 = slack variables\ny\u1d62 \u2208 {-1, +1} = class labels\n</code></pre> <p></p> <p>Strengths: - Effective in high dimensions - Works well with clear margins - Memory efficient</p> <p>Weaknesses: - Slow on large datasets - Needs parameter tuning - Requires feature scaling</p> <p>When to use: High-dimensional data with clear separation</p>"},{"location":"day2/day2/#5-k-nearest-neighbors-knn-classifier","title":"5. K-Nearest Neighbors (KNN) Classifier","text":"<p>How it works: Assigns class based on K nearest neighbors' majority vote.</p> <p>Example (K=5): - Find 5 nearest houses - 3 are \"Luxury\", 2 are \"Standard\" - Predict: \"Luxury\" (majority)</p> <p>Mathematical Formula :</p> <pre><code>\u0177 = mode{y\u2081, y\u2082, ..., y\u2096}\n\nWhere:\nK = number of nearest neighbors\ny\u1d62 = class of i-th nearest neighbor\nmode = most frequent class\n\nDistance (Euclidean):\nd(x, x\u1d62) = \u221a(\u03a3\u2c7c\u208c\u2081\u207f (x\u2c7c - x\u1d62\u2c7c)\u00b2)\n</code></pre> <p></p> <p>Strengths: - Simple and intuitive - No training needed - Naturally handles multi-class</p> <p>Weaknesses: - Slow for large datasets - Sensitive to feature scaling - Curse of dimensionality</p> <p>When to use: Small to medium datasets with good feature engineering</p>"},{"location":"day2/day2/#6-naive-bayes-classifier","title":"6. Naive Bayes Classifier","text":"<p>How it works: Uses Bayes' Theorem assuming features are independent.</p> <p>Mathematical Formula: </p> <p>Bayes' Theorem: <code>P(A | B) = [ P(B | A) * P(A) ] / P(B)</code></p> <p>For Na\u00efve Bayes Classification:</p> <pre><code>P(C | X) = [ P(X | C) * P(C) ] / P(X)\n\nWhere:\n- P(C | X) \u2192 Posterior probability of class C given predictor X  \n- P(X | C) \u2192 Likelihood of predictor given class  \n- P(C) \u2192 Prior probability of class  \n- P(X) \u2192 Probability of predictor (same for all classes)\n</code></pre> <p>Na\u00efve (Independence) Assumption: <code>P(X | C) = P(x\u2081, x\u2082, ..., x\u2099 | C) = \u03a0 P(x\u1d62 | C)</code></p> <p>Final Formula: <code>P(C | X) \u221d P(C) * \u03a0 P(x\u1d62 | C)</code></p> <p></p> <p>Example: Weather Prediction (Na\u00efve Bayes)</p> <ul> <li> <p>We want to predict whether someone will play tennis (Yes/No) based on the weather conditions.</p> </li> <li> <p>Suppose we have features:</p> </li> <li>Outlook = Sunny, Overcast, or Rain  </li> <li>Temperature = Hot, Mild, or Cool  </li> <li> <p>Humidity = High or Normal  </p> </li> <li> <p>We want to find: <code>P(Play = Yes | Outlook = Sunny, Humidity = High)</code></p> </li> <li> <p>Using Na\u00efve Bayes:</p> </li> </ul> <pre><code>P(Play = Yes | Outlook = Sunny, Humidity = High) \u221d \nP(Play = Yes) * P(Outlook = Sunny | Play = Yes) * P(Humidity = High | Play = Yes)\n</code></pre> <ul> <li>Each probability is estimated from training data (frequency counts).  </li> <li>The class (Yes or No) with the higher probability becomes the prediction.</li> </ul> <p>Strengths: - Very fast - Works well with text data - Needs little training data - Handles multi-class naturally</p> <p>Weaknesses: - Assumes feature independence (often unrealistic) - Cannot learn feature interactions</p> <p>When to use: Text classification, spam detection, sentiment analysis</p>"},{"location":"day2/day2/#evaluation-metrics","title":"\ud83d\udcca Evaluation Metrics","text":""},{"location":"day2/day2/#regression-metrics","title":"Regression Metrics","text":""},{"location":"day2/day2/#1-mean-squared-error-mse","title":"1. Mean Squared Error (MSE)","text":"<p>Formula: Average of squared differences between predictions and actual values <code>MSE = (1/n) * \u03a3 (y\u1d62 - \u0177\u1d62)\u00b2</code></p> <p>Interpretation: - Lower is better - Heavily penalizes large errors - Units are squared (e.g., dollars\u00b2)</p> <p>Example:  - Actual: $300k, Predicted: $310k \u2192 Error\u00b2: (10k)\u00b2 = 100M - Actual: $300k, Predicted: $320k \u2192 Error\u00b2: (20k)\u00b2 = 400M - MSE = (100M + 400M) / 2 = 250M</p>"},{"location":"day2/day2/#2-root-mean-squared-error-rmse","title":"2. Root Mean Squared Error (RMSE)","text":"<p>Formula: Square root of MSE <code>RMSE = \u221a[ (1/n) * \u03a3 (y\u1d62 - \u0177\u1d62)\u00b2 ]</code></p> <p>Interpretation: - Lower is better - Same units as target (dollars, not dollars\u00b2) - More interpretable than MSE</p> <p>Example: - From above, MSE = 250M - RMSE = \u221a250M \u2248 15.8k</p>"},{"location":"day2/day2/#3-mean-absolute-error-mae","title":"3. Mean Absolute Error (MAE)","text":"<p>Formula: Average of absolute differences <code>MAE = (1/n) * \u03a3 |y\u1d62 - \u0177\u1d62|</code></p> <p>Interpretation: - Lower is better - Less sensitive to outliers than RMSE - Direct average error</p> <p>Example: - Actual: $300k, Predicted: $310k \u2192 |Error| = 10k - Actual: $300k, Predicted: $320k \u2192 |Error| = 20k - MAE = (10k + 20k) / 2 = 15k</p>"},{"location":"day2/day2/#4-r2-score-r-squared","title":"4. R\u00b2 Score (R-Squared)","text":"<p>Formula: 1 - (Sum of Squared Residuals / Total Sum of Squares) <code>R\u00b2 = 1 - [ \u03a3 (y\u1d62 - \u0177\u1d62)\u00b2 / \u03a3 (y\u1d62 - \u0233)\u00b2 ]</code></p> <p>Interpretation: - Range: -\u221e to 1.0 - 1.0 = Perfect predictions - 0.0 = Model no better than predicting mean - &lt; 0 = Model worse than predicting mean</p> <p>Example: - Total variance (\u03a3(y\u1d62 - \u0233)\u00b2) = 1000M - Residual variance (\u03a3(y\u1d62 - \u0177\u1d62)\u00b2) = 250M - R\u00b2 = 1 - (250 / 1000) = 0.75 \u2192 Model explains 75% of variance</p>"},{"location":"day2/day2/#classification-metrics","title":"Classification Metrics","text":"<p>Let us assume we have:</p> Actual Predicted Positive (1) Positive (1) Negative (0) Positive (1) Positive (1) Negative (0) Negative (0) Negative (0) <p>So: TP = 1, TN = 1, FP = 1, FN = 1</p>"},{"location":"day2/day2/#1-accuracy","title":"1. Accuracy","text":"<p>Formula: (Correct Predictions) / (Total Predictions) <code>Accuracy = (TP + TN) / (TP + TN + FP + FN)</code></p> <p>Example: Accuracy = (1 + 1) / (1 + 1 + 1 + 1) = 0.5 \u2192 50% accuracy</p> <p>Limitation: Misleading with imbalanced classes</p>"},{"location":"day2/day2/#2-confusion-matrix","title":"2. Confusion Matrix","text":"<p>Compares predictions vs actual:</p> <pre><code>                Predicted\n              No    Yes\nActual  No   [TN]  [FP]\n        Yes  [FN]  [TP]\n</code></pre> <ul> <li>TP: True Positives (correctly predicted Yes)</li> <li>TN: True Negatives (correctly predicted No)</li> <li>FP: False Positives (predicted Yes, actually No)</li> <li>FN: False Negatives (predicted No, actually Yes)</li> </ul>"},{"location":"day2/day2/#3-precision","title":"3. Precision","text":"<p>Formula:  <code>TP / (TP + FP)</code></p> <p>Meaning: \"Of all positive predictions, how many were correct?\"</p> <p>Example:  = 1 / (1 + 1) = 0.5 \u2192 50% of predicted positives are correct</p>"},{"location":"day2/day2/#4-recall-sensitivity","title":"4. Recall (Sensitivity)","text":"<p>Formula:  <code>TP / (TP + FN)</code></p> <p>Meaning: \"Of all actual positives, how many did we catch?\"</p> <p>Example : = 1 / (1 + 1) = 0.5 \u2192 50% of actual positives identified</p>"},{"location":"day2/day2/#5-f1-score","title":"5. F1-Score","text":"<p>Formula:  <code>F1 = 2 * (Precision * Recall) / (Precision + Recall)</code></p> <p>Meaning: Harmonic mean of precision and recall</p> <p>Example : F1 = 2 * (0.5 * 0.5) / (0.5 + 0.5) = 0.5</p> <p>When to use: Balances precision and recall, especially with imbalanced data</p>"},{"location":"day2/day2/#model-comparison-regression-results","title":"\ud83c\udfc6 Model Comparison (Regression Results)","text":""},{"location":"day2/day2/#regression-performance-ranking","title":"Regression Performance Ranking","text":"Rank Model R\u00b2 Score RMSE MAE \ud83e\udd47 1 Linear Regression 0.7904 30.79 9.95 \ud83e\udd48 2 Random Forest 0.7375 34.45 1.77 \ud83e\udd49 3 KNN 0.6585 39.29 1.91 4 Decision Tree 0.6268 41.08 3.14 5 SVR 0.5188 46.64 3.88"},{"location":"day2/day2/#key-observations","title":"Key Observations","text":"<p>Linear Regression wins because:  </p> <ul> <li>\u2705 Highest R\u00b2 score (79.04%)  </li> <li>\u2705 Lowest RMSE (best average error)  </li> <li>\u2705 Fast training and prediction  </li> <li>\u2705 Easy to interpret  </li> </ul> <p>Interesting finding: Despite lower MAE, Random Forest has better overall performance metrics than simpler models.</p>"},{"location":"day2/day2/#model-comparison-classification-results","title":"\ud83c\udfc6 Model Comparison (Classification Results)","text":""},{"location":"day2/day2/#classification-performance-ranking","title":"Classification Performance Ranking","text":"Model Accuracy Precision Recall F1 Score Logistic Regression 0.9231 0.9479 0.9231 0.9271 Decision Tree 1.0000 1.0000 1.0000 1.0000 Random Forest 1.0000 1.0000 1.0000 1.0000 SVM 0.6667 0.5041 0.6667 0.5698 KNN 0.6410 0.6282 0.6410 0.6197 Naive Bayes 0.9487 0.9615 0.9487 0.9508"},{"location":"day2/day2/#confusion-matrix-for-all-models","title":"Confusion Matrix for all models","text":""},{"location":"day2/day2/#key-observations_1","title":"Key Observations","text":"<p>\ud83c\udfc6 BEST CLASSIFICATION MODELS</p> Model Accuracy Precision Recall F1 Score Decision Tree 1.0 1.0 1.0 1.0 Random Forest 1.0 1.0 1.0 1.0 <p>Decision Tree &amp; Random Forest win because:</p> <p>\u2705 Perfect test accuracy on this dataset \u2705 Can capture complex, non-linear relationships \u2705 Handle both categorical and numerical features naturally \u2705 Robust and flexible for small datasets</p>"},{"location":"day2/day2/#general-ml-best-practices","title":"General ML Best Practices","text":"<ol> <li>Always split your data</li> <li>Train-test split prevents overfitting</li> <li> <p>Use cross-validation for robust evaluation</p> </li> <li> <p>Try multiple models</p> </li> <li>Different models work better for different data</li> <li> <p>No \"one size fits all\" solution</p> </li> <li> <p>Understand your metrics</p> </li> <li>R\u00b2 for overall model fit</li> <li>RMSE for average prediction error</li> <li> <p>MAE for median error magnitude</p> </li> <li> <p>Consider the business context</p> </li> <li>Is $31k error acceptable for your use case?</li> <li>Sometimes a simple, interpretable model is better than a complex one</li> </ol>"},{"location":"day2/day2/#summary","title":"\ud83d\udcda Summary","text":"<p>You've learned:</p> <ul> <li>\u2705 Train-test split methodology</li> <li>\u2705 5 regression algorithms</li> <li>\u2705 6 classification algorithms </li> <li>\u2705 Multiple evaluation metrics</li> <li>\u2705 Model comparison techniques</li> </ul>"},{"location":"day2/day2/#download-material","title":"\ud83d\udce5 Download Material","text":"<ul> <li>\ud83d\udcd3 Download Notebook and Dataset for Regression and Classification Problem: day2.zip</li> </ul> <p>Run each cell step by step to see data cleaning and feature engineering in action!</p> <p>Remember:  The best model isn't always the most complex one.  Choose based on: - Performance on test data - Interpretability needs - Computational resources - Business requirements</p> <p>Created for ML Workshop Day 2 | Happy Learning! \ud83c\udf93</p>"},{"location":"day3/day3/","title":"DAY 3-DEEP LEARNING &amp; NLP WORKSHOP","text":""},{"location":"day3/day3/#1-introduction-to-deep-learning","title":"\ud83d\udd39 1. Introduction to Deep Learning","text":"<p>Deep Learning (DL) is a subset of Machine Learning inspired by the structure of the human brain \u2014 the Artificial Neural Network (ANN).</p> <p>It enables models to automatically extract features and patterns from raw data such as images, audio, or text without manual feature engineering.</p>"},{"location":"day3/day3/#key-features","title":"Key Features:","text":"<ul> <li>Learns from large amounts of data</li> <li>Uses multiple layers (hence deep) to model complex relationships</li> <li>Common frameworks: TensorFlow, PyTorch, Keras</li> </ul>"},{"location":"day3/day3/#why-deep-learning-over-traditional-ml","title":"\u2696\ufe0f Why Deep Learning over Traditional ML","text":"Aspect Traditional Machine Learning Deep Learning Feature Engineering Manual \u2014 you select features Automatic \u2014 model learns best features Data Requirement Works on small datasets Requires large data, but performs better Performance Plateaus with complex data Improves with more data &amp; compute Examples Linear Regression, SVM, Decision Trees CNN, LSTM, Transformers Applications Simple classification/regression Image, speech, NLP, autonomous systems"},{"location":"day3/day3/#example","title":"\ud83d\udca1 Example","text":"<ul> <li>ML: You manually count words and predict sentiment.</li> <li>DL: The model understands meaning (e.g., \u201cnot bad\u201d = positive) automatically.</li> </ul>"},{"location":"day3/day3/#2-what-is-nlp","title":"\ud83d\udd39 2. What is NLP?","text":"<p>Natural Language Processing (NLP) is a field of AI that allows computers to understand, interpret, and generate human language.</p> <p>It bridges human communication and machine understanding.</p>"},{"location":"day3/day3/#real-life-applications","title":"Real-life Applications:","text":"<ul> <li>Chatbots (e.g., ChatGPT, Alexa)</li> <li>Machine translation (e.g., Google Translate)</li> <li>Sentiment analysis (e.g., analyzing reviews)</li> <li>Text summarization, auto-completion, spam detection</li> </ul>"},{"location":"day3/day3/#3-nlp-workflow-overview","title":"\ud83d\udd39 3. NLP Workflow Overview","text":"<ol> <li>Text Preprocessing: Cleaning and preparing raw text</li> <li>Tokenization: Splitting text into words or subwords</li> <li>Vectorization: Converting text into numerical form</li> <li>Modeling: Using DL models like LSTM, GRU, or Transformer</li> <li>Evaluation &amp; Inference: Predicting and generating new text</li> </ol>"},{"location":"day3/day3/#4-basics-of-nlp-concepts","title":"\ud83d\udd39 4. Basics of NLP Concepts","text":""},{"location":"day3/day3/#41-text-preprocessing","title":"\ud83e\udde9 4.1 Text Preprocessing","text":"<p>Before feeding data to models, we clean it:</p> <ul> <li>Lowercasing text</li> <li>Removing punctuation &amp; stopwords</li> <li>Tokenization (splitting into words)</li> <li>Stemming/Lemmatization (reducing words to base form)</li> </ul> <p>Example:</p> <pre><code>from nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport nltk, re\nnltk.download('punkt'); nltk.download('stopwords')\n\ntext = \"Harry Potter and Hermione went to Hogwarts! It's magical.\"\ntext = text.lower()\ntext = re.sub(r'[^\\w\\s]', '', text)\ntokens = word_tokenize(text)\ntokens = [t for t in tokens if t not in stopwords.words('english')]\nprint(tokens)\n\n</code></pre>"},{"location":"day3/day3/#5-word-embeddings","title":"\ud83d\udd39 5. Word Embeddings","text":"<p>Traditional ML used \u201cbag of words,\u201d which ignored word meaning.</p> <p>Word embeddings solve this \u2014 representing words as dense numerical vectors that capture semantic meaning.</p>"},{"location":"day3/day3/#example_1","title":"Example:","text":"<p>The embeddings make:</p> <ul> <li><code>king - man + woman \u2248 queen</code></li> </ul>"},{"location":"day3/day3/#common-techniques","title":"Common Techniques:","text":"Technique Description Word2Vec Learns embeddings using context windows GloVe Embeddings based on word co-occurrence FastText Considers subword information Transformer-based embeddings (BERT) Contextual \u2014 same word has different embeddings based on context <p>Word2Vec Example:</p> <pre><code>from gensim.models import Word2Vec\nsentences = [[\"harry\", \"potter\", \"is\", \"a\", \"wizard\"],\n             [\"hermione\", \"is\", \"brilliant\"]]\nmodel = Word2Vec(sentences, vector_size=50, window=3, min_count=1, sg=1)\nprint(model.wv.most_similar(\"harry\"))\n\n</code></pre>"},{"location":"day3/day3/#6-deep-learning-in-nlp","title":"\ud83d\udd39 6. Deep Learning in NLP","text":"<p>Deep Learning models process text sequentially or contextually to capture patterns, syntax, and semantics.</p> <p>Two major architectures are used:</p>"},{"location":"day3/day3/#61-lstms-long-short-term-memory-networks","title":"\u2699\ufe0f 6.1 LSTMs (Long Short-Term Memory Networks)","text":"<p>Purpose: Handle long-term dependencies in text sequences.</p> <p>Problem Solved:</p> <p>Traditional RNNs forget earlier information. LSTMs use memory cells and gates (input, output, forget) to retain or discard information as needed.</p> <p>Example use case:</p> <p>Predicting the next word in a sentence like \u201cHarry looked at Ron and said ___\u201d</p> <p>Simple LSTM Code Example:</p> <pre><code>from keras.models import Sequential\nfrom keras.layers import Embedding, LSTM, Dense\n\nmodel = Sequential([\n    Embedding(input_dim=5000, output_dim=64, input_length=10),\n    LSTM(100),\n    Dense(5000, activation='softmax')\n])\nmodel.summary()\n\n</code></pre> <p>Working:</p> <ul> <li>Embedding layer converts words to vectors.</li> <li>LSTM learns temporal relationships.</li> <li>Dense layer outputs probability for the next word.</li> </ul>"},{"location":"day3/day3/#62-transformers","title":"\u2699\ufe0f 6.2 Transformers","text":"<p>Transformers revolutionized NLP by introducing self-attention, allowing the model to see the entire sequence at once, not just previous words.</p> <p>Key Components:</p> <ul> <li>Encoder \u2192 Reads input text</li> <li>Decoder \u2192 Generates output text</li> <li>Self-Attention Mechanism \u2192 Learns which words relate to which others</li> </ul> <p>Advantages:</p> <ul> <li>Parallel processing (faster training)</li> <li>Captures global context</li> <li>Forms the backbone of modern models (BERT, GPT, T5, etc.)</li> </ul>"},{"location":"day3/day3/#7-bert-and-gpt-models","title":"\ud83d\udd39 7. BERT and GPT Models","text":""},{"location":"day3/day3/#bert-bidirectional-encoder-representations-from-transformers","title":"\ud83e\udde0 BERT (Bidirectional Encoder Representations from Transformers)","text":"<ul> <li>Reads text both directions (left \u2192 right and right \u2192 left)</li> <li>Used for understanding-based tasks (like sentiment, classification)</li> </ul>"},{"location":"day3/day3/#gpt-generative-pretrained-transformer","title":"\ud83e\udd16 GPT (Generative Pretrained Transformer)","text":"<ul> <li>Reads text left-to-right</li> <li>Used for text generation, next-word prediction, story completion</li> </ul>"},{"location":"day3/day3/#8-practical-implementation-next-word-prediction-using-pre-trained-model","title":"\ud83d\udd39 8. Practical Implementation: Next-Word Prediction using Pre-Trained Model","text":"<p>We\u2019ll use a Transformer-based model (GPT-2) to generate text continuation from the Harry Potter corpus.</p>"},{"location":"day3/day3/#step-1-install-import-libraries","title":"\ud83e\udde9 Step 1: Install &amp; Import Libraries","text":"<pre><code>!pip install transformers torch\nfrom transformers import pipeline\n</code></pre>"},{"location":"day3/day3/#step-2-load-the-text-generation-pipeline","title":"\ud83e\udde9 Step 2: Load the Text Generation Pipeline","text":"<pre><code>generator = pipeline(\"text-generation\", model=\"gpt2\")\n</code></pre>"},{"location":"day3/day3/#step-3-give-a-harry-potter-style-prompt","title":"\ud83e\udde9 Step 3: Give a Harry Potter-style Prompt","text":"<pre><code>prompt = \"Harry looked at Ron and said\"\nresult = generator(prompt, max_length=30, num_return_sequences=1, temperature=0.9)\nprint(result[0]['generated_text'])\n</code></pre> <p>Example Output:</p> <p>\u201cHarry looked at Ron and said he could feel something strange in the air, a whisper of magic that made the room glow faintly.\u201d </p>"},{"location":"day3/day3/#step-4-try-other-prompts","title":"\ud83e\udde9 Step 4: Try Other Prompts","text":"<pre><code>prompts = [\n    \"Voldemort raised his wand and\",\n    \"Hermione opened the book of spells and\",\n    \"Hogwarts castle was quiet until\"\n]\n\nfor p in prompts:\n    print(generator(p, max_length=25, num_return_sequences=1)[0]['generated_text'])\n</code></pre>"},{"location":"day3/day3/#step-5-explain-the-parameters","title":"\ud83e\udde9 Step 5: Explain the Parameters","text":"Parameter Description prompt Starting phrase for generation max_length Number of tokens to generate temperature Controls randomness (0.7 = focused, 1.0 = creative) num_return_sequences Number of different outputs to generate"},{"location":"day3/day3/#step-6-optional-sentiment-analysis-bonus-demo","title":"\ud83e\udde9 Step 6: Optional \u2014 Sentiment Analysis (bonus demo)","text":"<pre><code>from transformers import pipeline\nsentiment = pipeline(\"sentiment-analysis\")\nprint(sentiment(\"I love Hogwarts but hate exams!\"))\n</code></pre> <p>Output:</p> <p>[{'label': 'POSITIVE', 'score': 0.99}] </p>"},{"location":"day3/day3/#9-key-learnings","title":"\ud83d\udd39 9. Key Learnings","text":"Concept Summary Word Embeddings Represent words as dense numerical vectors capturing meaning LSTM Sequential model that remembers long-term dependencies Transformer Parallel model that captures context using attention BERT Bidirectional model for understanding text GPT-2 Generative model for predicting and generating next words Application Next-word prediction using pre-trained model (Harry Potter demo)"},{"location":"day3/day3/#_1","title":"DAY 3","text":""},{"location":"day4/day4/","title":"Sai Uday's Content","text":""},{"location":"day4/day4/#welcome-to-day-4-of-the-bootcamp","title":"Welcome to Day 4 of the Bootcamp!!!","text":"<p>Today, we'll be learning how image recognition happens, and how Deep Learning (DL) works!</p>"},{"location":"day4/day4/#what-are-anns","title":"What are ANN's?","text":"<p>An Artificial Neural Network (ANN) is a type of model in Deep Learning that tries to work like the human brain \u2014 it learns by finding patterns in data.</p> <ul> <li>Just like our brain has neurons that send signals to each other, an ANN has artificial neurons (nodes) connected in layers that pass information forward and adjust themselves to learn.</li> <li>ANN is made up of layers of neurons:</li> <li>Input Layer: Receives the data.</li> <li>Hidden Layers: Where the actual learning happens \u2014 the model adjusts its weights and biases for each neuron during training.</li> <li>Output Layer: Produces the final prediction.</li> </ul> <p></p> <pre><code># Define the model\nmodel = Sequential([\n    Input(shape=(4,)),            # Input layer (4 features)\n    Dense(4, activation='relu'),  # Hidden layer 1\n    Dense(4, activation='relu'),  # Hidden layer 2\n    Dense(3, activation='sigmoid') # Output layer (multi-class classification)\n])\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n</code></pre>"},{"location":"day4/day4/#key-terms","title":"Key Terms:","text":"<ul> <li>Activation Function: Adds non-linearity (mathematically) for a neuron, so the model can learn complex patterns instead of simple linear ones.</li> <li>Optimizers: Algorithms that improve model learning by adjusting weights and biases during training to reduce error.</li> <li>Loss Function: Measures how wrong the model\u2019s predictions are \u2014 the model tries to minimize this loss while learning.</li> </ul>"},{"location":"day4/day4/#what-are-cnns","title":"What are CNN's?","text":"<p>A Convolutional Neural Network (CNN) is a type of Deep Learning model specifically designed to handle grid-like data, such as images.</p> <ul> <li>Instead of looking at the whole image at once, CNNs look at small local regions (patches) to learn patterns like edges, textures, shapes, and then combine them to recognize higher-level features.</li> <li>Applications: Image classification, object detection, face recognition, medical imaging, etc.</li> </ul>"},{"location":"day4/day4/#how-cnns-work","title":"How CNNs work?","text":"<p>A CNN has three main types of layers:</p>"},{"location":"day4/day4/#convolution-layer","title":"Convolution Layer:","text":"<ul> <li>Responsible for finding features in the data, such as edges, shapes, and textures using strides and kernels.</li> </ul>"},{"location":"day4/day4/#pooling-layer","title":"Pooling Layer:","text":"<ul> <li>Reduces the size of the feature maps to make computation easier while retaining the most important features.</li> </ul>"},{"location":"day4/day4/#fully-connected-layer","title":"Fully Connected Layer:","text":"<ul> <li>Predicts the outcome based on the extracted features passed to it.</li> </ul>"},{"location":"day4/day4/#basic-evolution-of-cnn","title":"Basic Evolution of CNN","text":""},{"location":"day4/day4/#lenet-1998","title":"LeNet (1998):","text":"<ul> <li>One of the earliest CNNs, designed for handwritten digit recognition (e.g., MNIST dataset).</li> <li>Introduced convolution and pooling layers.</li> </ul>"},{"location":"day4/day4/#alexnet-2012","title":"AlexNet (2012):","text":"<ul> <li>Popularized deep CNNs by winning the ImageNet competition.</li> <li>Used ReLU activation, dropout, and GPU training.</li> </ul>"},{"location":"day4/day4/#vggnet-2014","title":"VGGNet (2014):","text":"<ul> <li>Used very deep networks with small (3\u00d73) convolution filters.</li> <li>Demonstrated that increasing depth improves performance.</li> </ul>"},{"location":"day4/day4/#googlenetinception-2014","title":"GoogLeNet/Inception (2014):","text":"<ul> <li>Introduced inception modules for multi-scale feature extraction.</li> <li>Used global average pooling instead of fully connected layers.</li> </ul>"},{"location":"day4/day4/#resnet-2015","title":"ResNet (2015):","text":"<ul> <li>Introduced residual connections (skip connections) to solve vanishing gradient problems.</li> <li>Enabled training of very deep networks (50+ layers).</li> </ul>"},{"location":"day4/day4/#differences-between-an-ann-and-cnn","title":"Differences between an ANN and CNN","text":"Aspect ANN (Artificial Neural Network) CNN (Convolutional Neural Network) Main Idea Fully connected layers that learn global patterns Uses convolutional filters to learn local spatial patterns Typical Input 1D feature vectors (tabular data) 2D/3D grid data (images, videos, volumes) Connectivity Dense connections between neurons Local connectivity (receptive fields) + sparse connections Parameter Sharing No (each weight is unique) Yes (same filter applied across spatial locations) Feature Learning Learns global relationships Learns hierarchical features (edges \u2192 textures \u2192 objects) Translation Invariance Limited Stronger (due to convolutions and pooling) Common Layers Dense (fully connected), activation Convolution, pooling, batch norm, fully connected Typical Use Cases Tabular data, simple classification/regression Image classification, object detection, segmentation"},{"location":"day4/day4/#different-famous-cnn-architectures-concise-specs","title":"Different Famous CNN Architectures (Concise Specs)","text":""},{"location":"day4/day4/#lenet-5-1998","title":"LeNet-5 (1998):","text":"<ul> <li>Input: 32\u00d732 grayscale</li> <li>Convs: 3 conv layers (C1: 6 filters, C3: 16 filters, C5: 120 filters) + pooling layers</li> <li>Fully Connected: 2 (F6 and output)</li> <li>Total Params: ~60k</li> <li>Notes: Designed for MNIST; simple, small model for digit recognition. </li> </ul>"},{"location":"day4/day4/#alexnet-2012_1","title":"AlexNet (2012):","text":"<ul> <li>Input: 224\u00d7224 RGB (original used 227\u00d7227)</li> <li>Convs: 5 conv layers + max-pooling layers</li> <li>Fully Connected: 3 FC layers</li> <li>Total Params: ~60M</li> <li>Notes: ReLU, dropout, GPU training, large kernels in early layers (11\u00d711, 5\u00d75). </li> </ul>"},{"location":"day4/day4/#vgg-2014-eg-vgg-16-vgg-19","title":"VGG (2014) \u2014 e.g., VGG-16 / VGG-19:","text":"<ul> <li>Input: 224\u00d7224 RGB</li> <li>Convs: VGG-16 = 13 conv layers (stacked 3\u00d73 filters) + 5 max-pool layers; VGG-19 = 16 conv</li> <li>Fully Connected: 3 FC layers</li> <li>Total Params: VGG-16 \u2248 138M</li> <li>Notes: Very deep with uniform 3\u00d73 filters; high parameter count. </li> </ul>"},{"location":"day4/day4/#resnet-2015_1","title":"ResNet (2015):","text":"<ul> <li>Input: 224\u00d7224 RGB</li> <li>Convs: ResNet-50 uses bottleneck blocks totaling 49 conv layers + 1 FC (counted as 50); ResNet-101 deeper</li> <li>Fully Connected: 1 final FC (after global pooling)</li> <li>Total Params: ResNet-50 \u2248 25M</li> <li>Notes: Residual (skip) connections that enable very deep networks and ease training. </li> </ul>"},{"location":"day4/day4/#advantages-of-cnn","title":"Advantages of CNN","text":"<ul> <li>Learns hierarchical spatial features (edges \u2192 textures \u2192 objects).  </li> <li>Parameter sharing &amp; local connectivity \u2192 fewer parameters and efficient learning for images.  </li> <li>State-of-the-art for vision tasks and has efficient variants for edge/mobile.</li> </ul>"},{"location":"day4/day4/#disadvantages-of-cnn","title":"Disadvantages of CNN","text":"<ul> <li>Data hungry \u2014 needs large labeled datasets for best performance.  </li> <li>Compute and memory intensive (training often requires GPUs/TPUs).  </li> <li>Can overfit on small datasets.  </li> </ul>"},{"location":"day4/day4/#hosting-on-hugging-face","title":"Hosting on Hugging Face","text":""},{"location":"day4/day4/#prerequisites","title":"Prerequisites","text":"<ul> <li>Hugging Face account: Sign up here</li> <li>Git and Git LFS installed:</li> <li>Windows: <code>git lfs install</code></li> <li>Python packages:   <code>bash   pip install huggingface_hub torch torchvision</code></li> <li>Train your CNN locally and save artifacts (weights, optional config, and helper code).</li> </ul>"},{"location":"day4/day4/#steps-to-host","title":"Steps to Host:","text":"<ol> <li>Save model artifacts (e.g., <code>pytorch_model.bin</code>, <code>config.json</code>, <code>model.py</code>).</li> <li>Create a repo on Hugging Face (via CLI or web).</li> <li>Push files using Git + LFS or Python API.</li> <li>Add a README (model card) with usage instructions.</li> <li>Enable Inference API (optional).</li> </ol>"},{"location":"day4/day4/#notes-tips","title":"Notes / Tips","text":"<ul> <li>Use Git LFS for large weight files (&gt;10 MB).</li> <li>Provide a model.py file to help others reconstruct the architecture.</li> <li>Add a README.md with usage, dataset, license, and metrics.</li> <li>Enable the Inference API for remote inference directly on Hugging Face.</li> </ul>"},{"location":"files/day1/banglore_home_prices_final/","title":"Replacing null values with mean or median of values :","text":"<p>Real Estate Price Prediction Model</p> In\u00a0[\u00a0]: Copied! <pre>#Importing the necessary libraries\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport matplotlib\nmatplotlib.rcParams[\"figure.figsize\"] = (20,10)\n</pre> #Importing the necessary libraries  import pandas as pd import numpy as np from matplotlib import pyplot as plt import seaborn as sns %matplotlib inline import matplotlib matplotlib.rcParams[\"figure.figsize\"] = (20,10) In\u00a0[\u00a0]: Copied! <pre>from google.colab import files\nuploaded = files.upload()\n</pre> from google.colab import files uploaded = files.upload() In\u00a0[\u00a0]: Copied! <pre>df1 = pd.read_csv(\"/content/bengaluru_house_prices.csv\")\ndf1.head()\n</pre> df1 = pd.read_csv(\"/content/bengaluru_house_prices.csv\") df1.head() In\u00a0[\u00a0]: Copied! <pre>df1.shape #For knowing number of rows and columns\n</pre> df1.shape #For knowing number of rows and columns In\u00a0[\u00a0]: Copied! <pre>#To know the different types of elements in the column 'area_types' we use the below code:-\ndf1.groupby('area_type')['area_type'].agg('count')\n</pre> #To know the different types of elements in the column 'area_types' we use the below code:- df1.groupby('area_type')['area_type'].agg('count') In\u00a0[\u00a0]: Copied! <pre>#Assuming \"area_type\", \"society\", \"balcony\", \"availability\" to be not very important, so we can drop those columns\n\ndf2 = df1.drop([\"area_type\", \"society\", \"balcony\", \"availability\"],axis='columns')\n\ndf2.head()\n</pre> #Assuming \"area_type\", \"society\", \"balcony\", \"availability\" to be not very important, so we can drop those columns  df2 = df1.drop([\"area_type\", \"society\", \"balcony\", \"availability\"],axis='columns')  df2.head()  In\u00a0[\u00a0]: Copied! <pre>#Now to know at which places in our dataset, there are empty/NA values,\ndf2.isnull().sum()\n</pre> #Now to know at which places in our dataset, there are empty/NA values, df2.isnull().sum() <p>Since there are about 13,000 rows in the dataset, it is safe to delete the empty values instead of filling with their mean/median values</p> In\u00a0[\u00a0]: Copied! <pre>df3 = df2.dropna()\n\ndf3.isnull().sum() #This shows that now there are no empty elements in our dataset\n</pre> df3 = df2.dropna()  df3.isnull().sum() #This shows that now there are no empty elements in our dataset In\u00a0[\u00a0]: Copied! <pre>df3.shape\n</pre> df3.shape In\u00a0[\u00a0]: Copied! <pre>df3['size'].unique() #This code will return all the unique values in the column 'size'\n</pre> df3['size'].unique() #This code will return all the unique values in the column 'size' <p>Now, in order to clean this dataset, we will use the below code, where we will first split each element from 'size' column, and select the element at 0th position, which are nothing but the integers...the number of bedrooms present</p> In\u00a0[\u00a0]: Copied! <pre>df3['bhk'] = df3['size'].str.split(' ').str[0].astype(int)\n</pre> df3['bhk'] = df3['size'].str.split(' ').str[0].astype(int) In\u00a0[\u00a0]: Copied! <pre>df3.head()\n</pre> df3.head() <p>Now, we shall clean 'total_sqft' column</p> In\u00a0[\u00a0]: Copied! <pre>df3.total_sqft.unique()\n</pre> df3.total_sqft.unique() <p>Here, we notice that there are values such as '1133 - 1384' which is basically a range value</p> <p>We also need to figure out, whether there are any float values in 'total_sqft'</p> <p>So what we are gonna do is, we will try to convert all the elements to float value, and whichever value throws an exception, those values can be categorized as '1133 - 1384' kind of values</p> In\u00a0[\u00a0]: Copied! <pre>def is_float(x):\n    try:\n        float(x)\n    except:\n        return False\n    return True\n</pre> def is_float(x):     try:         float(x)     except:         return False     return True In\u00a0[\u00a0]: Copied! <pre>df3[~df3['total_sqft'].apply(is_float)]\n\n#The '~' is used to print wherever the function returned False\n</pre> df3[~df3['total_sqft'].apply(is_float)]  #The '~' is used to print wherever the function returned False <p>In 'total_sqft' we also get to see values such as '34.46Sq. Meter' and '4125Perch' etc, so we are going to just ignore those values</p> <p>We are going to now deal with only range values for now</p> In\u00a0[\u00a0]: Copied! <pre>def convert_sqft_to_num(x):\n    tokens = x.split('-')\n    if len(tokens) == 2:\n        return (float(tokens[0])+float(tokens[1]))/2\n    try:\n        return float(x)\n    except:\n        return None\n</pre> def convert_sqft_to_num(x):     tokens = x.split('-')     if len(tokens) == 2:         return (float(tokens[0])+float(tokens[1]))/2     try:         return float(x)     except:         return None In\u00a0[\u00a0]: Copied! <pre>convert_sqft_to_num('1133 - 1384')\n</pre> convert_sqft_to_num('1133 - 1384') In\u00a0[\u00a0]: Copied! <pre>df4 = df3.copy()\ndf4['total_sqft'] = df4['total_sqft'].apply(convert_sqft_to_num)\n\ndf4\n</pre> df4 = df3.copy() df4['total_sqft'] = df4['total_sqft'].apply(convert_sqft_to_num)  df4 Feature Engineering In\u00a0[\u00a0]: Copied! <pre>df5 = df4.copy()\n\n#Now we will be adding a new column which will be price per sq ft, for later purposes of outlier detection\n\ndf5['price_per_sqft'] = df5['price']*100000 / df5['total_sqft']\n\ndf5.head()\n</pre> df5 = df4.copy()  #Now we will be adding a new column which will be price per sq ft, for later purposes of outlier detection  df5['price_per_sqft'] = df5['price']*100000 / df5['total_sqft']  df5.head() <p>Now, we shall explain the location column</p> In\u00a0[\u00a0]: Copied! <pre>len(df5.location.unique())\n</pre> len(df5.location.unique()) In\u00a0[\u00a0]: Copied! <pre>df5['location'] = df5['location'].str.strip()\n#This code removes leading and trailing whitespace from each value in the 'location' column\nlocation_stats = df5['location'].value_counts()\n\nlocation_stats\n</pre> df5['location'] = df5['location'].str.strip() #This code removes leading and trailing whitespace from each value in the 'location' column location_stats = df5['location'].value_counts()  location_stats <p>We shall witness that there are some locations, that has repetition of only 1 or 2 times....so we shall set a particular margin, below which, the locations shall be names as 'Others'</p> In\u00a0[\u00a0]: Copied! <pre>len(location_stats[location_stats &lt;= 10]) #We can observe that there are 1052 locations whose repetition is less than 10 out of 1293 locations...So we can classify them as 'Others'\n</pre> len(location_stats[location_stats &lt;= 10]) #We can observe that there are 1052 locations whose repetition is less than 10 out of 1293 locations...So we can classify them as 'Others' In\u00a0[\u00a0]: Copied! <pre># Assuming location_stats is a Series like: location -&gt; count\nlocation_stats_less_than_10 = location_stats[location_stats &lt;= 10].index  # use .index\n\n# Now replace rare locations\ndf5['location'] = df5['location'].where(~df5['location'].isin(location_stats_less_than_10), 'other')\n</pre> # Assuming location_stats is a Series like: location -&gt; count location_stats_less_than_10 = location_stats[location_stats &lt;= 10].index  # use .index  # Now replace rare locations df5['location'] = df5['location'].where(~df5['location'].isin(location_stats_less_than_10), 'other')  In\u00a0[\u00a0]: Copied! <pre>len(df5.location.unique())\n</pre> len(df5.location.unique()) Outliers Detection and Removal  Outlier Detection is nothing but detecting extremely deviating values in the dataset    <p>An example of outlier is...at row 9, total_sqft is 1020 and bhk is 6 which is unusual... So what we can do is, we can remove all those rows where total_sqft / bhk &lt; 300</p> In\u00a0[\u00a0]: Copied! <pre>df6 = df5[~(df5.total_sqft/df5.bhk&lt;300)]\n\ndf6.shape\n</pre> df6 = df5[~(df5.total_sqft/df5.bhk&lt;300)]  df6.shape Outlier Removal Using Box plot and IQR In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(18,5))\nplt.subplot(1,3,1)\nsns.boxplot(df6['bath'])\nplt.title('Bathrooms')\n\nplt.subplot(1,3,2)\nsns.boxplot(df6['bhk'])\nplt.title('BHK')\n\nplt.subplot(1,3,3)\nsns.boxplot(df6['price_per_sqft'])\nplt.title('price_per_sqft')\n\nplt.tight_layout()\nplt.show()\n</pre> plt.figure(figsize=(18,5)) plt.subplot(1,3,1) sns.boxplot(df6['bath']) plt.title('Bathrooms')  plt.subplot(1,3,2) sns.boxplot(df6['bhk']) plt.title('BHK')  plt.subplot(1,3,3) sns.boxplot(df6['price_per_sqft']) plt.title('price_per_sqft')  plt.tight_layout() plt.show()   In\u00a0[\u00a0]: Copied! <pre># Remove outliers using IQR for bath, bhk, price_per_sqft\ndef remove_iqr_outliers(df, column):\n    Q1 = df[column].quantile(0.25)\n    Q3 = df[column].quantile(0.75)\n    IQR = Q3 - Q1\n    return df[(df[column] &gt;= Q1 - 1.5 * IQR) &amp; (df[column] &lt;= Q3 + 1.5 * IQR)]\n\ndf6_no_bath_outliers = remove_iqr_outliers(df6, 'bath')\ndf6_no_bhk_outliers = remove_iqr_outliers(df6_no_bath_outliers, 'bhk')\ndf7 = remove_iqr_outliers(df6_no_bhk_outliers, 'price_per_sqft')\n\nprint(\"Shape after outlier removal:\", df7.shape)\n</pre> # Remove outliers using IQR for bath, bhk, price_per_sqft def remove_iqr_outliers(df, column):     Q1 = df[column].quantile(0.25)     Q3 = df[column].quantile(0.75)     IQR = Q3 - Q1     return df[(df[column] &gt;= Q1 - 1.5 * IQR) &amp; (df[column] &lt;= Q3 + 1.5 * IQR)]  df6_no_bath_outliers = remove_iqr_outliers(df6, 'bath') df6_no_bhk_outliers = remove_iqr_outliers(df6_no_bath_outliers, 'bhk') df7 = remove_iqr_outliers(df6_no_bhk_outliers, 'price_per_sqft')  print(\"Shape after outlier removal:\", df7.shape)   In\u00a0[\u00a0]: Copied! <pre># Print boxplots after removal\nplt.figure(figsize=(18,5))\nplt.subplot(1,3,1)\nsns.boxplot(df7['bath'])\nplt.title('Bathrooms (After Removal)')\n\nplt.subplot(1,3,2)\nsns.boxplot(df7['bhk'])\nplt.title('BHK (After Removal)')\n\nplt.subplot(1,3,3)\nsns.boxplot(df7['price_per_sqft'])\nplt.title('Price per Sqft (After Removal)')\n\nplt.tight_layout()\nplt.show()\n</pre> # Print boxplots after removal plt.figure(figsize=(18,5)) plt.subplot(1,3,1) sns.boxplot(df7['bath']) plt.title('Bathrooms (After Removal)')  plt.subplot(1,3,2) sns.boxplot(df7['bhk']) plt.title('BHK (After Removal)')  plt.subplot(1,3,3) sns.boxplot(df7['price_per_sqft']) plt.title('Price per Sqft (After Removal)')  plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre>#Performing one hot encoding on location\ndummies = pd.get_dummies(df7.location) #One-hot encoding\ndummies.head(3)\n</pre> #Performing one hot encoding on location dummies = pd.get_dummies(df7.location) #One-hot encoding dummies.head(3) In\u00a0[\u00a0]: Copied! <pre>df8 = pd.concat([df7, dummies.drop('other',axis=\"columns\")],axis='columns')\n\ndf8.head()\n</pre> df8 = pd.concat([df7, dummies.drop('other',axis=\"columns\")],axis='columns')  df8.head() In\u00a0[\u00a0]: Copied! <pre>df9 =df8.drop('location', axis=\"columns\")\n\ndf9.head()\n</pre> df9 =df8.drop('location', axis=\"columns\")  df9.head() In\u00a0[\u00a0]: Copied! <pre>df9.to_csv('preprocessed_data.csv', index=False)\nfrom google.colab import files\nfiles.download('preprocessed_data.csv')\n</pre> df9.to_csv('preprocessed_data.csv', index=False) from google.colab import files files.download('preprocessed_data.csv')"},{"location":"files/day1/banglore_home_prices_final/#replacing-null-values-with-mean-or-median-of-values","title":"Replacing null values with mean or median of values :\u00b6","text":""},{"location":"files/day1/banglore_home_prices_final/#mean","title":"Mean\u00b6","text":"<pre><code>df['column_name'] = df['column_name'].fillna(df['column_name'].mean())\n</code></pre>"},{"location":"files/day1/banglore_home_prices_final/#median","title":"Median\u00b6","text":"<pre><code>df['column_name'] = df['column_name'].fillna(df['column_name'].mean()) ```\n</code></pre>"},{"location":"files/day2/ML_Workshop_Day2_Classification/","title":"ML Workshop Day2 Classification","text":"In\u00a0[\u00a0]: Copied! <pre>#Importing necesssary Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n</pre> #Importing necesssary Libraries import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report In\u00a0[\u00a0]: Copied! <pre>#Importing all required models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\n</pre> #Importing all required models from sklearn.linear_model import LogisticRegression from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.svm import SVC from sklearn.neighbors import KNeighborsClassifier from sklearn.naive_bayes import GaussianNB In\u00a0[\u00a0]: Copied! <pre>#Uploading the dataset\nfrom google.colab import files\nuploaded = files.upload()\n</pre> #Uploading the dataset from google.colab import files uploaded = files.upload() In\u00a0[\u00a0]: Copied! <pre>df = pd.read_csv('preprocessed_data.csv')\ndf.shape\n</pre> df = pd.read_csv('preprocessed_data.csv') df.shape In\u00a0[\u00a0]: Copied! <pre>df.head()\n</pre> df.head() In\u00a0[\u00a0]: Copied! <pre>feature_cols = [col for col in df.columns if col != 'Drug']\nX = df[feature_cols]\ny = df['Drug']\n</pre> feature_cols = [col for col in df.columns if col != 'Drug'] X = df[feature_cols] y = df['Drug'] In\u00a0[\u00a0]: Copied! <pre># ===================================================\n# SECTION 3: TRAIN-TEST SPLIT &amp; SCALING\n# ===================================================\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n</pre> # =================================================== # SECTION 3: TRAIN-TEST SPLIT &amp; SCALING # =================================================== X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)  In\u00a0[\u00a0]: Copied! <pre># ===================================================\n# SECTION 4: CLASSIFICATION MODELS\n# ===================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"SECTION 4: TRAINING CLASSIFICATION MODELS\")\nprint(\"=\"*80)\nprint(\"Target: Predict categorical class labels\")\nprint(\"-\"*80)\n\nclassification_results = {}\n</pre> # =================================================== # SECTION 4: CLASSIFICATION MODELS # =================================================== print(\"\\n\" + \"=\"*80) print(\"SECTION 4: TRAINING CLASSIFICATION MODELS\") print(\"=\"*80) print(\"Target: Predict categorical class labels\") print(\"-\"*80)  classification_results = {} In\u00a0[\u00a0]: Copied! <pre># 1. LOGISTIC REGRESSION\nprint(\"\\n1. Logistic Regression\")\nprint(\"-\"*40)\nlr = LogisticRegression(max_iter=1000)\nlr.fit(X_train, y_train)\ny_pred_lr = lr.predict(X_test)\n\nclassification_results['Logistic Regression'] = {\n    'Accuracy': accuracy_score(y_test, y_pred_lr),\n    'Precision': precision_score(y_test, y_pred_lr, average='weighted'),\n    'Recall': recall_score(y_test, y_pred_lr, average='weighted'),\n    'F1 Score': f1_score(y_test, y_pred_lr, average='weighted')\n}\n\nprint(classification_report(y_test, y_pred_lr))\n</pre> # 1. LOGISTIC REGRESSION print(\"\\n1. Logistic Regression\") print(\"-\"*40) lr = LogisticRegression(max_iter=1000) lr.fit(X_train, y_train) y_pred_lr = lr.predict(X_test)  classification_results['Logistic Regression'] = {     'Accuracy': accuracy_score(y_test, y_pred_lr),     'Precision': precision_score(y_test, y_pred_lr, average='weighted'),     'Recall': recall_score(y_test, y_pred_lr, average='weighted'),     'F1 Score': f1_score(y_test, y_pred_lr, average='weighted') }  print(classification_report(y_test, y_pred_lr)) In\u00a0[\u00a0]: Copied! <pre># 2. DECISION TREE CLASSIFIER\nprint(\"\\n2. Decision Tree Classifier\")\nprint(\"-\"*40)\ndt = DecisionTreeClassifier(random_state=42, max_depth=10)\ndt.fit(X_train, y_train)\ny_pred_dt = dt.predict(X_test)\n\nclassification_results['Decision Tree'] = {\n    'Accuracy': accuracy_score(y_test, y_pred_dt),\n    'Precision': precision_score(y_test, y_pred_dt, average='weighted'),\n    'Recall': recall_score(y_test, y_pred_dt, average='weighted'),\n    'F1 Score': f1_score(y_test, y_pred_dt, average='weighted')\n}\n\nprint(classification_report(y_test, y_pred_dt))\n</pre> # 2. DECISION TREE CLASSIFIER print(\"\\n2. Decision Tree Classifier\") print(\"-\"*40) dt = DecisionTreeClassifier(random_state=42, max_depth=10) dt.fit(X_train, y_train) y_pred_dt = dt.predict(X_test)  classification_results['Decision Tree'] = {     'Accuracy': accuracy_score(y_test, y_pred_dt),     'Precision': precision_score(y_test, y_pred_dt, average='weighted'),     'Recall': recall_score(y_test, y_pred_dt, average='weighted'),     'F1 Score': f1_score(y_test, y_pred_dt, average='weighted') }  print(classification_report(y_test, y_pred_dt)) In\u00a0[\u00a0]: Copied! <pre># 3. RANDOM FOREST CLASSIFIER\nprint(\"\\n3. Random Forest Classifier\")\nprint(\"-\"*40)\nrf = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)\nrf.fit(X_train, y_train)\ny_pred_rf = rf.predict(X_test)\n\nclassification_results['Random Forest'] = {\n    'Accuracy': accuracy_score(y_test, y_pred_rf),\n    'Precision': precision_score(y_test, y_pred_rf, average='weighted'),\n    'Recall': recall_score(y_test, y_pred_rf, average='weighted'),\n    'F1 Score': f1_score(y_test, y_pred_rf, average='weighted')\n}\n\nprint(classification_report(y_test, y_pred_rf))\n</pre> # 3. RANDOM FOREST CLASSIFIER print(\"\\n3. Random Forest Classifier\") print(\"-\"*40) rf = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10) rf.fit(X_train, y_train) y_pred_rf = rf.predict(X_test)  classification_results['Random Forest'] = {     'Accuracy': accuracy_score(y_test, y_pred_rf),     'Precision': precision_score(y_test, y_pred_rf, average='weighted'),     'Recall': recall_score(y_test, y_pred_rf, average='weighted'),     'F1 Score': f1_score(y_test, y_pred_rf, average='weighted') }  print(classification_report(y_test, y_pred_rf)) In\u00a0[\u00a0]: Copied! <pre># 4. SUPPORT VECTOR MACHINE (SVM)\nprint(\"\\n4. Support Vector Machine (SVM)\")\nprint(\"-\"*40)\nsvm = SVC(kernel='rbf', probability=True)\nsvm.fit(X_train, y_train)\ny_pred_svm = svm.predict(X_test)\n\nclassification_results['SVM'] = {\n    'Accuracy': accuracy_score(y_test, y_pred_svm),\n    'Precision': precision_score(y_test, y_pred_svm, average='weighted'),\n    'Recall': recall_score(y_test, y_pred_svm, average='weighted'),\n    'F1 Score': f1_score(y_test, y_pred_svm, average='weighted')\n}\n\nprint(classification_report(y_test, y_pred_svm))\n</pre> # 4. SUPPORT VECTOR MACHINE (SVM) print(\"\\n4. Support Vector Machine (SVM)\") print(\"-\"*40) svm = SVC(kernel='rbf', probability=True) svm.fit(X_train, y_train) y_pred_svm = svm.predict(X_test)  classification_results['SVM'] = {     'Accuracy': accuracy_score(y_test, y_pred_svm),     'Precision': precision_score(y_test, y_pred_svm, average='weighted'),     'Recall': recall_score(y_test, y_pred_svm, average='weighted'),     'F1 Score': f1_score(y_test, y_pred_svm, average='weighted') }  print(classification_report(y_test, y_pred_svm)) In\u00a0[\u00a0]: Copied! <pre># 5. K-NEAREST NEIGHBORS (KNN)\nprint(\"\\n5. K-Nearest Neighbors\")\nprint(\"-\"*40)\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\ny_pred_knn = knn.predict(X_test)\n\nclassification_results['KNN'] = {\n    'Accuracy': accuracy_score(y_test, y_pred_knn),\n    'Precision': precision_score(y_test, y_pred_knn, average='weighted'),\n    'Recall': recall_score(y_test, y_pred_knn, average='weighted'),\n    'F1 Score': f1_score(y_test, y_pred_knn, average='weighted')\n}\n\nprint(classification_report(y_test, y_pred_knn))\n</pre> # 5. K-NEAREST NEIGHBORS (KNN) print(\"\\n5. K-Nearest Neighbors\") print(\"-\"*40) knn = KNeighborsClassifier(n_neighbors=5) knn.fit(X_train, y_train) y_pred_knn = knn.predict(X_test)  classification_results['KNN'] = {     'Accuracy': accuracy_score(y_test, y_pred_knn),     'Precision': precision_score(y_test, y_pred_knn, average='weighted'),     'Recall': recall_score(y_test, y_pred_knn, average='weighted'),     'F1 Score': f1_score(y_test, y_pred_knn, average='weighted') }  print(classification_report(y_test, y_pred_knn)) In\u00a0[\u00a0]: Copied! <pre># 6. NAIVE BAYES\nprint(\"\\n6. Naive Bayes Classifier\")\nprint(\"-\"*40)\nnb = GaussianNB()\nnb.fit(X_train, y_train)\ny_pred_nb = nb.predict(X_test)\n\nclassification_results['Naive Bayes'] = {\n    'Accuracy': accuracy_score(y_test, y_pred_nb),\n    'Precision': precision_score(y_test, y_pred_nb, average='weighted'),\n    'Recall': recall_score(y_test, y_pred_nb, average='weighted'),\n    'F1 Score': f1_score(y_test, y_pred_nb, average='weighted')\n}\n\nprint(classification_report(y_test, y_pred_nb))\n</pre> # 6. NAIVE BAYES print(\"\\n6. Naive Bayes Classifier\") print(\"-\"*40) nb = GaussianNB() nb.fit(X_train, y_train) y_pred_nb = nb.predict(X_test)  classification_results['Naive Bayes'] = {     'Accuracy': accuracy_score(y_test, y_pred_nb),     'Precision': precision_score(y_test, y_pred_nb, average='weighted'),     'Recall': recall_score(y_test, y_pred_nb, average='weighted'),     'F1 Score': f1_score(y_test, y_pred_nb, average='weighted') }  print(classification_report(y_test, y_pred_nb)) In\u00a0[\u00a0]: Copied! <pre># ===================================================\n# SECTION 6: MODEL COMPARISON\n# ===================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"SECTION 6: MODEL COMPARISON &amp; RESULTS\")\nprint(\"=\"*80)\n\nprint(\"\\nCLASSIFICATION MODELS COMPARISON\")\nprint(\"-\"*80)\nclass_df = pd.DataFrame(classification_results).T\nclass_df = class_df.round(4)\nprint(class_df.to_string())\n\nprint(\"\\n\ud83c\udfc6 BEST CLASSIFICATION MODEL:\")\nbest_clf_model = class_df['Accuracy'].idxmax()\nprint(class_df[class_df['Accuracy'] == class_df['Accuracy'].max()])\n</pre> # =================================================== # SECTION 6: MODEL COMPARISON # =================================================== print(\"\\n\" + \"=\"*80) print(\"SECTION 6: MODEL COMPARISON &amp; RESULTS\") print(\"=\"*80)  print(\"\\nCLASSIFICATION MODELS COMPARISON\") print(\"-\"*80) class_df = pd.DataFrame(classification_results).T class_df = class_df.round(4) print(class_df.to_string())  print(\"\\n\ud83c\udfc6 BEST CLASSIFICATION MODEL:\") best_clf_model = class_df['Accuracy'].idxmax() print(class_df[class_df['Accuracy'] == class_df['Accuracy'].max()])  In\u00a0[\u00a0]: Copied! <pre># ===================================================\n# SECTION 7: VISUALIZATIONS\n# ===================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"SECTION 7: GENERATING COMPARISON VISUALIZATIONS\")\nprint(\"=\"*80)\n\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\nfig.suptitle('Classification Models Performance Comparison', fontsize=16, fontweight='bold')\n\naxes[0, 0].barh(class_df.index, class_df['Accuracy'], color='steelblue')\naxes[0, 0].set_xlabel('Accuracy')\naxes[0, 0].set_title('Accuracy Comparison')\n\naxes[0, 1].barh(class_df.index, class_df['Precision'], color='coral')\naxes[0, 1].set_xlabel('Precision')\naxes[0, 1].set_title('Precision Comparison')\n\naxes[1, 0].barh(class_df.index, class_df['Recall'], color='mediumseagreen')\naxes[1, 0].set_xlabel('Recall')\naxes[1, 0].set_title('Recall Comparison')\n\naxes[1, 1].barh(class_df.index, class_df['F1 Score'], color='mediumpurple')\naxes[1, 1].set_xlabel('F1 Score')\naxes[1, 1].set_title('F1 Score Comparison')\n\nfor ax in axes.flat:\n    ax.grid(axis='x', alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\u2713 Visualizations generated successfully!\")\n</pre> # =================================================== # SECTION 7: VISUALIZATIONS # =================================================== print(\"\\n\" + \"=\"*80) print(\"SECTION 7: GENERATING COMPARISON VISUALIZATIONS\") print(\"=\"*80)  fig, axes = plt.subplots(2, 2, figsize=(15, 10)) fig.suptitle('Classification Models Performance Comparison', fontsize=16, fontweight='bold')  axes[0, 0].barh(class_df.index, class_df['Accuracy'], color='steelblue') axes[0, 0].set_xlabel('Accuracy') axes[0, 0].set_title('Accuracy Comparison')  axes[0, 1].barh(class_df.index, class_df['Precision'], color='coral') axes[0, 1].set_xlabel('Precision') axes[0, 1].set_title('Precision Comparison')  axes[1, 0].barh(class_df.index, class_df['Recall'], color='mediumseagreen') axes[1, 0].set_xlabel('Recall') axes[1, 0].set_title('Recall Comparison')  axes[1, 1].barh(class_df.index, class_df['F1 Score'], color='mediumpurple') axes[1, 1].set_xlabel('F1 Score') axes[1, 1].set_title('F1 Score Comparison')  for ax in axes.flat:     ax.grid(axis='x', alpha=0.3)  plt.tight_layout() plt.show()  print(\"\u2713 Visualizations generated successfully!\")  In\u00a0[\u00a0]: Copied! <pre># ===================================================\n# Confusion Matrices for All Classification Models\n# ===================================================\n\nfrom sklearn.metrics import confusion_matrix\n\n# Get predictions from all classification models\n# You need to store these predictions when training each model\nclassification_predictions = {\n    'Logistic Regression': y_pred_lr,\n    'Decision Tree': y_pred_dt,\n    'Random Forest': y_pred_rf,\n    'SVM': y_pred_svm,\n    'KNN': y_pred_knn,\n    'Naive Bayes': y_pred_nb\n}\n\n# Create figure with subplots for each model\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\naxes = axes.ravel()\n\n# Get unique class labels\nclass_labels = sorted(y_test.unique())\n\nfor idx, (model_name, y_pred) in enumerate(classification_predictions.items()):\n    # Calculate confusion matrix\n    cm = confusion_matrix(y_test, y_pred, labels=class_labels)\n\n    # Calculate percentages\n    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n\n    # Create annotations with both count and percentage\n    annotations = []\n    for i in range(cm.shape[0]):\n        row = []\n        for j in range(cm.shape[1]):\n            count = cm[i, j]\n            percent = cm_percent[i, j]\n            row.append(f'{count}\\n({percent:.1f}%)')\n        annotations.append(row)\n\n    # Plot heatmap\n    sns.heatmap(cm, annot=annotations, fmt='', cmap='Blues',\n                xticklabels=class_labels, yticklabels=class_labels,\n                ax=axes[idx], cbar_kws={'label': 'Count'},\n                linewidths=2, linecolor='white')\n\n    # Add accuracy to title\n    accuracy = classification_results[model_name]['Accuracy']\n    axes[idx].set_title(f'{model_name}\\nAccuracy: {accuracy:.4f}',\n                       fontsize=12, fontweight='bold')\n    axes[idx].set_ylabel('Actual', fontsize=10, fontweight='bold')\n    axes[idx].set_xlabel('Predicted', fontsize=10, fontweight='bold')\n\n    # Highlight diagonal (correct predictions) with thicker border\n    for i in range(len(class_labels)):\n        axes[idx].add_patch(plt.Rectangle((i, i), 1, 1, fill=False,\n                                         edgecolor='green', lw=3))\n\nplt.suptitle('\ud83c\udfaf CONFUSION MATRICES: All Classification Models',\n             fontsize=16, fontweight='bold', y=0.995)\nplt.tight_layout()\nplt.show()\n\nprint(\"\u2705 Confusion Matrices for All Models Generated!\")\n\n# Print interpretation guide\nprint(\"\\n\" + \"=\"*80)\nprint(\"\ud83d\udcca HOW TO READ CONFUSION MATRICES:\")\nprint(\"=\"*80)\nprint(\"\u2022 Diagonal (green boxes) = CORRECT predictions\")\nprint(\"\u2022 Off-diagonal = INCORRECT predictions\")\nprint(\"\u2022 Darker colors = More predictions\")\nprint(\"\u2022 Format: Count (Percentage of actual class)\")\nprint(\"\\nExample interpretation:\")\nprint(\"  If cell [Drug A, Drug B] = 5 (10%)\")\nprint(\"  \u2192 5 instances of actual Drug A were incorrectly predicted as Drug B\")\nprint(\"  \u2192 This represents 10% of all actual Drug A cases\")\nprint(\"=\"*80)\n</pre> # =================================================== # Confusion Matrices for All Classification Models # ===================================================  from sklearn.metrics import confusion_matrix  # Get predictions from all classification models # You need to store these predictions when training each model classification_predictions = {     'Logistic Regression': y_pred_lr,     'Decision Tree': y_pred_dt,     'Random Forest': y_pred_rf,     'SVM': y_pred_svm,     'KNN': y_pred_knn,     'Naive Bayes': y_pred_nb }  # Create figure with subplots for each model fig, axes = plt.subplots(2, 3, figsize=(18, 12)) axes = axes.ravel()  # Get unique class labels class_labels = sorted(y_test.unique())  for idx, (model_name, y_pred) in enumerate(classification_predictions.items()):     # Calculate confusion matrix     cm = confusion_matrix(y_test, y_pred, labels=class_labels)      # Calculate percentages     cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100      # Create annotations with both count and percentage     annotations = []     for i in range(cm.shape[0]):         row = []         for j in range(cm.shape[1]):             count = cm[i, j]             percent = cm_percent[i, j]             row.append(f'{count}\\n({percent:.1f}%)')         annotations.append(row)      # Plot heatmap     sns.heatmap(cm, annot=annotations, fmt='', cmap='Blues',                 xticklabels=class_labels, yticklabels=class_labels,                 ax=axes[idx], cbar_kws={'label': 'Count'},                 linewidths=2, linecolor='white')      # Add accuracy to title     accuracy = classification_results[model_name]['Accuracy']     axes[idx].set_title(f'{model_name}\\nAccuracy: {accuracy:.4f}',                        fontsize=12, fontweight='bold')     axes[idx].set_ylabel('Actual', fontsize=10, fontweight='bold')     axes[idx].set_xlabel('Predicted', fontsize=10, fontweight='bold')      # Highlight diagonal (correct predictions) with thicker border     for i in range(len(class_labels)):         axes[idx].add_patch(plt.Rectangle((i, i), 1, 1, fill=False,                                          edgecolor='green', lw=3))  plt.suptitle('\ud83c\udfaf CONFUSION MATRICES: All Classification Models',              fontsize=16, fontweight='bold', y=0.995) plt.tight_layout() plt.show()  print(\"\u2705 Confusion Matrices for All Models Generated!\")  # Print interpretation guide print(\"\\n\" + \"=\"*80) print(\"\ud83d\udcca HOW TO READ CONFUSION MATRICES:\") print(\"=\"*80) print(\"\u2022 Diagonal (green boxes) = CORRECT predictions\") print(\"\u2022 Off-diagonal = INCORRECT predictions\") print(\"\u2022 Darker colors = More predictions\") print(\"\u2022 Format: Count (Percentage of actual class)\") print(\"\\nExample interpretation:\") print(\"  If cell [Drug A, Drug B] = 5 (10%)\") print(\"  \u2192 5 instances of actual Drug A were incorrectly predicted as Drug B\") print(\"  \u2192 This represents 10% of all actual Drug A cases\") print(\"=\"*80)"},{"location":"files/day2/ML_Workshop_Day_2/","title":"ML WORSHOP DAY 2 : MODEL TRAINING AND ANALYSIS","text":"In\u00a0[\u00a0]: Copied! <pre>#Importing the necessary libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n</pre> #Importing the necessary libraries  import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error In\u00a0[\u00a0]: Copied! <pre>#Importing the required models\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\n</pre> #Importing the required models from sklearn.linear_model import LinearRegression from sklearn.tree import DecisionTreeRegressor from sklearn.ensemble import RandomForestRegressor from sklearn.svm import SVR from sklearn.neighbors import KNeighborsRegressor In\u00a0[\u00a0]: Copied! <pre>#To import the pre processed dataset\n\nfrom google.colab import files\nuploaded = files.upload()\n</pre> #To import the pre processed dataset  from google.colab import files uploaded = files.upload() In\u00a0[\u00a0]: Copied! <pre>#Loading the preprocessed dataset into a pandas dataframe\ndf = pd.read_csv('preprocessed_data.csv')\ndf.shape\n</pre> #Loading the preprocessed dataset into a pandas dataframe df = pd.read_csv('preprocessed_data.csv') df.shape <p>Now in our dataset, price is the target i.e. what we are predicting. So we will seperate price column from the other columns</p> In\u00a0[\u00a0]: Copied! <pre>df.head()\n</pre> df.head() In\u00a0[\u00a0]: Copied! <pre>#Dropping the size column as it is a text based column and not required because we have already extracted the numerical features from it\ndf1=df.drop(['size'],axis='columns')\n</pre> #Dropping the size column as it is a text based column and not required because we have already extracted the numerical features from it df1=df.drop(['size'],axis='columns') In\u00a0[\u00a0]: Copied! <pre># Features: drop 'price' (target), keep all others\nfeature_cols = [col for col in df1.columns if col != 'price']\nX = df1[feature_cols]\ny = df1['price']\n</pre> # Features: drop 'price' (target), keep all others feature_cols = [col for col in df1.columns if col != 'price'] X = df1[feature_cols] y = df1['price']  In\u00a0[\u00a0]: Copied! <pre>print(f\"\u2713 Feature matrix shape: {X.shape}\")\nprint(f\"\u2713 Target (price) shape: {y.shape}\")\n</pre> print(f\"\u2713 Feature matrix shape: {X.shape}\") print(f\"\u2713 Target (price) shape: {y.shape}\") <p>Performing train test split - to split our dataset into training and testing values. Training values - the values that our model will learn and understand Testing values - the values that our model will predict</p> In\u00a0[\u00a0]: Copied! <pre>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n</pre> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  In\u00a0[\u00a0]: Copied! <pre>#We will store the results of regression into this dictionary\nregression_results = {}\n</pre> #We will store the results of regression into this dictionary regression_results = {} In\u00a0[\u00a0]: Copied! <pre># 1. LINEAR REGRESSION\nprint(\"\\n1. Linear Regression\")\nprint(\"-\"*40)\nlr = LinearRegression()\nlr.fit(X_train, y_train)\ny_pred_lr = lr.predict(X_test)\n\nmse_lr = mean_squared_error(y_test, y_pred_lr)\nrmse_lr = np.sqrt(mse_lr)\nmae_lr = mean_absolute_error(y_test, y_pred_lr)\nr2_lr = r2_score(y_test, y_pred_lr)\n\nregression_results['Linear Regression'] = {\n    'MSE': mse_lr, 'RMSE': rmse_lr, 'MAE': mae_lr, 'R\u00b2 Score': r2_lr\n}\n\nprint(f\"  Mean Squared Error: {mse_lr:.4f}\")\nprint(f\"  Root Mean Squared Error: {rmse_lr:.4f}\")\nprint(f\"  Mean Absolute Error: {mae_lr:.4f}\")\nprint(f\"  R\u00b2 Score: {r2_lr:.4f}\")\n</pre> # 1. LINEAR REGRESSION print(\"\\n1. Linear Regression\") print(\"-\"*40) lr = LinearRegression() lr.fit(X_train, y_train) y_pred_lr = lr.predict(X_test)  mse_lr = mean_squared_error(y_test, y_pred_lr) rmse_lr = np.sqrt(mse_lr) mae_lr = mean_absolute_error(y_test, y_pred_lr) r2_lr = r2_score(y_test, y_pred_lr)  regression_results['Linear Regression'] = {     'MSE': mse_lr, 'RMSE': rmse_lr, 'MAE': mae_lr, 'R\u00b2 Score': r2_lr }  print(f\"  Mean Squared Error: {mse_lr:.4f}\") print(f\"  Root Mean Squared Error: {rmse_lr:.4f}\") print(f\"  Mean Absolute Error: {mae_lr:.4f}\") print(f\"  R\u00b2 Score: {r2_lr:.4f}\")  In\u00a0[\u00a0]: Copied! <pre># 2. DECISION TREE REGRESSOR\nprint(\"\\n2. Decision Tree Regressor\")\nprint(\"-\"*40)\ndtr = DecisionTreeRegressor(random_state=42, max_depth=10)\ndtr.fit(X_train, y_train)\ny_pred_dtr = dtr.predict(X_test)\n\nmse_dtr = mean_squared_error(y_test, y_pred_dtr)\nrmse_dtr = np.sqrt(mse_dtr)\nmae_dtr = mean_absolute_error(y_test, y_pred_dtr)\nr2_dtr = r2_score(y_test, y_pred_dtr)\n\nregression_results['Decision Tree'] = {\n    'MSE': mse_dtr, 'RMSE': rmse_dtr, 'MAE': mae_dtr, 'R\u00b2 Score': r2_dtr\n}\n\nprint(f\"  Mean Squared Error: {mse_dtr:.4f}\")\nprint(f\"  Root Mean Squared Error: {rmse_dtr:.4f}\")\nprint(f\"  Mean Absolute Error: {mae_dtr:.4f}\")\nprint(f\"  R\u00b2 Score: {r2_dtr:.4f}\")\n</pre> # 2. DECISION TREE REGRESSOR print(\"\\n2. Decision Tree Regressor\") print(\"-\"*40) dtr = DecisionTreeRegressor(random_state=42, max_depth=10) dtr.fit(X_train, y_train) y_pred_dtr = dtr.predict(X_test)  mse_dtr = mean_squared_error(y_test, y_pred_dtr) rmse_dtr = np.sqrt(mse_dtr) mae_dtr = mean_absolute_error(y_test, y_pred_dtr) r2_dtr = r2_score(y_test, y_pred_dtr)  regression_results['Decision Tree'] = {     'MSE': mse_dtr, 'RMSE': rmse_dtr, 'MAE': mae_dtr, 'R\u00b2 Score': r2_dtr }  print(f\"  Mean Squared Error: {mse_dtr:.4f}\") print(f\"  Root Mean Squared Error: {rmse_dtr:.4f}\") print(f\"  Mean Absolute Error: {mae_dtr:.4f}\") print(f\"  R\u00b2 Score: {r2_dtr:.4f}\")  In\u00a0[\u00a0]: Copied! <pre># 3. RANDOM FOREST REGRESSOR\nprint(\"\\n3. Random Forest Regressor\")\nprint(\"-\"*40)\nrfr = RandomForestRegressor(n_estimators=100, random_state=42, max_depth=10)\nrfr.fit(X_train, y_train)\ny_pred_rfr = rfr.predict(X_test)\n\nmse_rfr = mean_squared_error(y_test, y_pred_rfr)\nrmse_rfr = np.sqrt(mse_rfr)\nmae_rfr = mean_absolute_error(y_test, y_pred_rfr)\nr2_rfr = r2_score(y_test, y_pred_rfr)\n\nregression_results['Random Forest'] = {\n    'MSE': mse_rfr, 'RMSE': rmse_rfr, 'MAE': mae_rfr, 'R\u00b2 Score': r2_rfr\n}\n\nprint(f\"  Mean Squared Error: {mse_rfr:.4f}\")\nprint(f\"  Root Mean Squared Error: {rmse_rfr:.4f}\")\nprint(f\"  Mean Absolute Error: {mae_rfr:.4f}\")\nprint(f\"  R\u00b2 Score: {r2_rfr:.4f}\")\n</pre> # 3. RANDOM FOREST REGRESSOR print(\"\\n3. Random Forest Regressor\") print(\"-\"*40) rfr = RandomForestRegressor(n_estimators=100, random_state=42, max_depth=10) rfr.fit(X_train, y_train) y_pred_rfr = rfr.predict(X_test)  mse_rfr = mean_squared_error(y_test, y_pred_rfr) rmse_rfr = np.sqrt(mse_rfr) mae_rfr = mean_absolute_error(y_test, y_pred_rfr) r2_rfr = r2_score(y_test, y_pred_rfr)  regression_results['Random Forest'] = {     'MSE': mse_rfr, 'RMSE': rmse_rfr, 'MAE': mae_rfr, 'R\u00b2 Score': r2_rfr }  print(f\"  Mean Squared Error: {mse_rfr:.4f}\") print(f\"  Root Mean Squared Error: {rmse_rfr:.4f}\") print(f\"  Mean Absolute Error: {mae_rfr:.4f}\") print(f\"  R\u00b2 Score: {r2_rfr:.4f}\")  In\u00a0[\u00a0]: Copied! <pre># 4. SUPPORT VECTOR REGRESSOR (SVR)\nprint(\"\\n4. Support Vector Regressor (SVR)\")\nprint(\"-\"*40)\nsvr = SVR(kernel='rbf')\nsvr.fit(X_train, y_train)\ny_pred_svr = svr.predict(X_test)\n\nmse_svr = mean_squared_error(y_test, y_pred_svr)\nrmse_svr = np.sqrt(mse_svr)\nmae_svr = mean_absolute_error(y_test, y_pred_svr)\nr2_svr = r2_score(y_test, y_pred_svr)\n\nregression_results['SVR'] = {\n    'MSE': mse_svr, 'RMSE': rmse_svr, 'MAE': mae_svr, 'R\u00b2 Score': r2_svr\n}\n\nprint(f\"  Mean Squared Error: {mse_svr:.4f}\")\nprint(f\"  Root Mean Squared Error: {rmse_svr:.4f}\")\nprint(f\"  Mean Absolute Error: {mae_svr:.4f}\")\nprint(f\"  R\u00b2 Score: {r2_svr:.4f}\")\n</pre> # 4. SUPPORT VECTOR REGRESSOR (SVR) print(\"\\n4. Support Vector Regressor (SVR)\") print(\"-\"*40) svr = SVR(kernel='rbf') svr.fit(X_train, y_train) y_pred_svr = svr.predict(X_test)  mse_svr = mean_squared_error(y_test, y_pred_svr) rmse_svr = np.sqrt(mse_svr) mae_svr = mean_absolute_error(y_test, y_pred_svr) r2_svr = r2_score(y_test, y_pred_svr)  regression_results['SVR'] = {     'MSE': mse_svr, 'RMSE': rmse_svr, 'MAE': mae_svr, 'R\u00b2 Score': r2_svr }  print(f\"  Mean Squared Error: {mse_svr:.4f}\") print(f\"  Root Mean Squared Error: {rmse_svr:.4f}\") print(f\"  Mean Absolute Error: {mae_svr:.4f}\") print(f\"  R\u00b2 Score: {r2_svr:.4f}\")  In\u00a0[\u00a0]: Copied! <pre># 5. K-NEAREST NEIGHBORS REGRESSOR\nprint(\"\\n5. K-Nearest Neighbors Regressor\")\nprint(\"-\"*40)\nknr = KNeighborsRegressor(n_neighbors=5)\nknr.fit(X_train, y_train)\ny_pred_knr = knr.predict(X_test)\n\nmse_knr = mean_squared_error(y_test, y_pred_knr)\nrmse_knr = np.sqrt(mse_knr)\nmae_knr = mean_absolute_error(y_test, y_pred_knr)\nr2_knr = r2_score(y_test, y_pred_knr)\n\nregression_results['KNN'] = {\n    'MSE': mse_knr, 'RMSE': rmse_knr, 'MAE': mae_knr, 'R\u00b2 Score': r2_knr\n}\n\nprint(f\"  Mean Squared Error: {mse_knr:.4f}\")\nprint(f\"  Root Mean Squared Error: {rmse_knr:.4f}\")\nprint(f\"  Mean Absolute Error: {mae_knr:.4f}\")\nprint(f\"  R\u00b2 Score: {r2_knr:.4f}\")\n</pre> # 5. K-NEAREST NEIGHBORS REGRESSOR print(\"\\n5. K-Nearest Neighbors Regressor\") print(\"-\"*40) knr = KNeighborsRegressor(n_neighbors=5) knr.fit(X_train, y_train) y_pred_knr = knr.predict(X_test)  mse_knr = mean_squared_error(y_test, y_pred_knr) rmse_knr = np.sqrt(mse_knr) mae_knr = mean_absolute_error(y_test, y_pred_knr) r2_knr = r2_score(y_test, y_pred_knr)  regression_results['KNN'] = {     'MSE': mse_knr, 'RMSE': rmse_knr, 'MAE': mae_knr, 'R\u00b2 Score': r2_knr }  print(f\"  Mean Squared Error: {mse_knr:.4f}\") print(f\"  Root Mean Squared Error: {rmse_knr:.4f}\") print(f\"  Mean Absolute Error: {mae_knr:.4f}\") print(f\"  R\u00b2 Score: {r2_knr:.4f}\")  In\u00a0[\u00a0]: Copied! <pre># ===================================================\n# SECTION 6: MODEL COMPARISON\n# ===================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"SECTION 6: MODEL COMPARISON &amp; RESULTS\")\nprint(\"=\"*80)\n\nprint(\"\\nREGRESSION MODELS COMPARISON\")\nprint(\"-\"*80)\nreg_df = pd.DataFrame(regression_results).T\nreg_df = reg_df.round(4)\nprint(reg_df.to_string())\n\nprint(\"\\n\ud83c\udfc6 BEST REGRESSION MODEL:\")\nbest_reg_model = reg_df['R\u00b2 Score'].idxmax()\nprint(f\"  {best_reg_model} with R\u00b2 Score of {reg_df.loc[best_reg_model, 'R\u00b2 Score']:.4f}\")\n</pre> # =================================================== # SECTION 6: MODEL COMPARISON # =================================================== print(\"\\n\" + \"=\"*80) print(\"SECTION 6: MODEL COMPARISON &amp; RESULTS\") print(\"=\"*80)  print(\"\\nREGRESSION MODELS COMPARISON\") print(\"-\"*80) reg_df = pd.DataFrame(regression_results).T reg_df = reg_df.round(4) print(reg_df.to_string())  print(\"\\n\ud83c\udfc6 BEST REGRESSION MODEL:\") best_reg_model = reg_df['R\u00b2 Score'].idxmax() print(f\"  {best_reg_model} with R\u00b2 Score of {reg_df.loc[best_reg_model, 'R\u00b2 Score']:.4f}\")  In\u00a0[\u00a0]: Copied! <pre># ===================================================\n# SECTION 7: VISUALIZATIONS\n# ===================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"SECTION 7: GENERATING COMPARISON VISUALIZATIONS\")\nprint(\"=\"*80)\n\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\nfig.suptitle('Regression Models Performance Comparison', fontsize=16, fontweight='bold')\n\naxes[0, 0].barh(reg_df.index, reg_df['R\u00b2 Score'], color='steelblue')\naxes[0, 0].set_xlabel('R\u00b2 Score')\naxes[0, 0].set_title('R\u00b2 Score Comparison')\naxes[0, 0].grid(axis='x', alpha=0.3)\n\naxes[0, 1].barh(reg_df.index, reg_df['RMSE'], color='coral')\naxes[0, 1].set_xlabel('RMSE (lower is better)')\naxes[0, 1].set_title('RMSE Comparison')\naxes[0, 1].grid(axis='x', alpha=0.3)\n\naxes[1, 0].barh(reg_df.index, reg_df['MAE'], color='mediumseagreen')\naxes[1, 0].set_xlabel('MAE (lower is better)')\naxes[1, 0].set_title('MAE Comparison')\naxes[1, 0].grid(axis='x', alpha=0.3)\n\naxes[1, 1].barh(reg_df.index, reg_df['MSE'], color='mediumpurple')\naxes[1, 1].set_xlabel('MSE (lower is better)')\naxes[1, 1].set_title('MSE Comparison')\naxes[1, 1].grid(axis='x', alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\u2713 Visualizations generated successfully!\")\n</pre> # =================================================== # SECTION 7: VISUALIZATIONS # =================================================== print(\"\\n\" + \"=\"*80) print(\"SECTION 7: GENERATING COMPARISON VISUALIZATIONS\") print(\"=\"*80)  fig, axes = plt.subplots(2, 2, figsize=(15, 10)) fig.suptitle('Regression Models Performance Comparison', fontsize=16, fontweight='bold')  axes[0, 0].barh(reg_df.index, reg_df['R\u00b2 Score'], color='steelblue') axes[0, 0].set_xlabel('R\u00b2 Score') axes[0, 0].set_title('R\u00b2 Score Comparison') axes[0, 0].grid(axis='x', alpha=0.3)  axes[0, 1].barh(reg_df.index, reg_df['RMSE'], color='coral') axes[0, 1].set_xlabel('RMSE (lower is better)') axes[0, 1].set_title('RMSE Comparison') axes[0, 1].grid(axis='x', alpha=0.3)  axes[1, 0].barh(reg_df.index, reg_df['MAE'], color='mediumseagreen') axes[1, 0].set_xlabel('MAE (lower is better)') axes[1, 0].set_title('MAE Comparison') axes[1, 0].grid(axis='x', alpha=0.3)  axes[1, 1].barh(reg_df.index, reg_df['MSE'], color='mediumpurple') axes[1, 1].set_xlabel('MSE (lower is better)') axes[1, 1].set_title('MSE Comparison') axes[1, 1].grid(axis='x', alpha=0.3)  plt.tight_layout() plt.show()  print(\"\u2713 Visualizations generated successfully!\")   In\u00a0[\u00a0]: Copied! <pre># ===================================================\n# SECTION 8: KEY INSIGHTS &amp; RECOMMENDATIONS\n# ===================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"SECTION 8: KEY INSIGHTS &amp; RECOMMENDATIONS\")\nprint(\"=\"*80)\n\nprint(\"\\n\ud83d\udcca REGRESSION ANALYSIS:\")\nprint(f\"  \u2022 Best Model: {best_reg_model}\")\nprint(f\"  \u2022 R\u00b2 Score: {reg_df.loc[best_reg_model, 'R\u00b2 Score']:.4f}\")\nprint(f\"  \u2022 This model explains {reg_df.loc[best_reg_model, 'R\u00b2 Score']*100:.2f}% of variance in house prices\")\n</pre> # =================================================== # SECTION 8: KEY INSIGHTS &amp; RECOMMENDATIONS # =================================================== print(\"\\n\" + \"=\"*80) print(\"SECTION 8: KEY INSIGHTS &amp; RECOMMENDATIONS\") print(\"=\"*80)  print(\"\\n\ud83d\udcca REGRESSION ANALYSIS:\") print(f\"  \u2022 Best Model: {best_reg_model}\") print(f\"  \u2022 R\u00b2 Score: {reg_df.loc[best_reg_model, 'R\u00b2 Score']:.4f}\") print(f\"  \u2022 This model explains {reg_df.loc[best_reg_model, 'R\u00b2 Score']*100:.2f}% of variance in house prices\")"},{"location":"files/day2/ML_Workshop_Day_2/#ml-worshop-day-2-model-training-and-analysis","title":"ML WORSHOP DAY 2 : MODEL TRAINING AND ANALYSIS\u00b6","text":""},{"location":"files/day2/ML_Workshop_Day_2/#objectives","title":"Objectives :\u00b6","text":"<ul> <li>Understand train test split</li> <li>Understand various regression models : Linear, Decision Tree, Random Forest, KNN, SVM, Naive Bayes</li> <li>Evaluate and compare performance of model based on various evaluation metrics</li> </ul>"},{"location":"files/day4/code_which/","title":"Code which","text":"In\u00a0[\u00a0]: Copied! <pre>import torch\nimport torch.nn as nn\nfrom torchvision import datasets, transforms, models\n\n# \u2705 Device configuration (GPU if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# \u2705 Data transforms\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.Grayscale(num_output_channels=3),\n    transforms.ToTensor(),\n    transforms.Normalize((0.485, 0.456, 0.406),\n                         (0.229, 0.224, 0.225))\n])\n\n# \u2705 Datasets &amp; DataLoader\ntrain_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n\n# \u2705 Model setup\nmodel = models.resnet18(weights=None)\nmodel.fc = nn.Linear(model.fc.in_features, 10)\nmodel = model.to(device)\n\n# \u2705 Loss &amp; optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\n# \u2705 Training loop with progress display\nnum_epochs = \ntotal_steps = len(train_loader)\ntotal_images = len(train_dataset)\n\nfor epoch in range(num_epochs):\n    running_loss = 0.0\n    for batch_idx, (imgs, labels) in enumerate(train_loader):\n        imgs, labels = imgs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(imgs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n        # Print progress\n        processed = (batch_idx + 1) * imgs.size(0)\n        print(f\"Epoch [{epoch+1}/{num_epochs}] | \"\n              f\"Step [{batch_idx+1}/{total_steps}] | \"\n              f\"Images: {processed}/{total_images} | \"\n              f\"Loss: {loss.item():.4f}\", end=\"\\r\")\n\n    print(f\"\\n\u2705 Epoch [{epoch+1}/{num_epochs}] completed. \"\n          f\"Average Loss: {running_loss / total_steps:.4f}\\n\")\n\nprint(\"\ud83c\udf89 Training complete!\")\n</pre> import torch import torch.nn as nn from torchvision import datasets, transforms, models  # \u2705 Device configuration (GPU if available) device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") print(f\"Using device: {device}\")  # \u2705 Data transforms transform = transforms.Compose([     transforms.Resize((224, 224)),     transforms.Grayscale(num_output_channels=3),     transforms.ToTensor(),     transforms.Normalize((0.485, 0.456, 0.406),                          (0.229, 0.224, 0.225)) ])  # \u2705 Datasets &amp; DataLoader train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform) train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)  # \u2705 Model setup model = models.resnet18(weights=None) model.fc = nn.Linear(model.fc.in_features, 10) model = model.to(device)  # \u2705 Loss &amp; optimizer criterion = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)  # \u2705 Training loop with progress display num_epochs =  total_steps = len(train_loader) total_images = len(train_dataset)  for epoch in range(num_epochs):     running_loss = 0.0     for batch_idx, (imgs, labels) in enumerate(train_loader):         imgs, labels = imgs.to(device), labels.to(device)          optimizer.zero_grad()         outputs = model(imgs)         loss = criterion(outputs, labels)         loss.backward()         optimizer.step()          running_loss += loss.item()          # Print progress         processed = (batch_idx + 1) * imgs.size(0)         print(f\"Epoch [{epoch+1}/{num_epochs}] | \"               f\"Step [{batch_idx+1}/{total_steps}] | \"               f\"Images: {processed}/{total_images} | \"               f\"Loss: {loss.item():.4f}\", end=\"\\r\")      print(f\"\\n\u2705 Epoch [{epoch+1}/{num_epochs}] completed. \"           f\"Average Loss: {running_loss / total_steps:.4f}\\n\")  print(\"\ud83c\udf89 Training complete!\") In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport torch\n\n# Assuming 'imgs' and 'outputs' are from your last training batch\n# Get predicted class\n_, predicted = torch.max(outputs, 1)\n\n# \u2705 Denormalize images (since you used ImageNet normalization)\nmean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\nstd = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n\n# Move tensors to CPU for plotting\nimgs_cpu = imgs.detach().cpu()\npredicted_cpu = predicted.detach().cpu()\n\n# \u2705 Denormalize\nimages_denorm = imgs_cpu * std + mean\nimages_denorm = torch.clamp(images_denorm, 0, 1)  # keep pixel range valid\n\n# \u2705 Plot 15 images with predictions\nfig, axes = plt.subplots(1, 15, figsize=(15, 3))\nfor i in range(15):\n    ax = axes[i]\n    ax.imshow(images_denorm[i].permute(1, 2, 0))  # C\u00d7H\u00d7W \u2192 H\u00d7W\u00d7C\n    ax.set_title(f\"Pred: {predicted_cpu[i].item()}\", fontsize=8)\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()\n</pre> import matplotlib.pyplot as plt import torch  # Assuming 'imgs' and 'outputs' are from your last training batch # Get predicted class _, predicted = torch.max(outputs, 1)  # \u2705 Denormalize images (since you used ImageNet normalization) mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1) std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)  # Move tensors to CPU for plotting imgs_cpu = imgs.detach().cpu() predicted_cpu = predicted.detach().cpu()  # \u2705 Denormalize images_denorm = imgs_cpu * std + mean images_denorm = torch.clamp(images_denorm, 0, 1)  # keep pixel range valid  # \u2705 Plot 15 images with predictions fig, axes = plt.subplots(1, 15, figsize=(15, 3)) for i in range(15):     ax = axes[i]     ax.imshow(images_denorm[i].permute(1, 2, 0))  # C\u00d7H\u00d7W \u2192 H\u00d7W\u00d7C     ax.set_title(f\"Pred: {predicted_cpu[i].item()}\", fontsize=8)     ax.axis('off')  plt.tight_layout() plt.show()"},{"location":"files/day4/simple_cnn/","title":"Simple cnn","text":"<p>First, we start off with importing all the models required, for this particular workshop, we will be using pytorch</p> In\u00a0[\u00a0]: Copied! <pre>import torch\nimport torch.nn as nn\nfrom torchvision import datasets, transforms, models\nimport matplotlib.pyplot as plt\n</pre> import torch import torch.nn as nn from torchvision import datasets, transforms, models import matplotlib.pyplot as plt <p>The above cell is used to detect if there's CUDA functionality, basically if you have an Nvidia GPU, you can use that for model training, instead of using CPU power completely.</p> In\u00a0[7]: Copied! <pre># \u2705 Device configuration (GPU if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n</pre> # \u2705 Device configuration (GPU if available) device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") print(f\"Using device: {device}\") <pre>Using device: cuda\n</pre> <p>Now, the model we will be focusing on is called Resnet-50. For resnet-50, the input required is 224 x 224 x 3, but then the image size of the dataset is 28 x 28 x 1. So, for resnet-50, we will need to transform to 224 x 224, and now, its only 1 channel present in the image, but then the input taken by the model is 3 channels, so we use the greyscale function to convert to 3 channels, and then we convert images to tensor values, and we normalize it.</p> In\u00a0[10]: Copied! <pre>transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.Grayscale(num_output_channels=3),\n    transforms.ToTensor(),\n    transforms.Normalize((0.485, 0.456, 0.406),\n                         (0.229, 0.224, 0.225))\n])\n</pre> transform = transforms.Compose([     transforms.Resize((224, 224)),     transforms.Grayscale(num_output_channels=3),     transforms.ToTensor(),     transforms.Normalize((0.485, 0.456, 0.406),                          (0.229, 0.224, 0.225)) ])  <p>This is to load the MNIST dataset (which is a digit classification dataset, has multiple hand written images from 0-9).</p> In\u00a0[\u00a0]: Copied! <pre>train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n</pre> train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform) train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True) <p>This is to load the resnet-18 model, and the resnet-18 model initially outputs 1000 classes, but then we only need 10 (0-9), so we mention 10 as well to output only 10 classes.</p> In\u00a0[\u00a0]: Copied! <pre>model = models.resnet18(weights=None)\nmodel.fc = nn.Linear(model.fc.in_features, 10)\nmodel = model.to(device)\n</pre> model = models.resnet18(weights=None) model.fc = nn.Linear(model.fc.in_features, 10) model = model.to(device) <p>This initializes the loss and optimizer required.</p> In\u00a0[\u00a0]: Copied! <pre># \u2705 Loss &amp; optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n</pre> # \u2705 Loss &amp; optimizer criterion = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=1e-4) <p>Initializes the number of epochs ( the number of times the model trains ), and total images</p> In\u00a0[\u00a0]: Copied! <pre>num_epochs = 2\ntotal_steps = len(train_loader)\ntotal_images = len(train_dataset)\n</pre> num_epochs = 2 total_steps = len(train_loader) total_images = len(train_dataset)  <p>Training with log information as well</p> In\u00a0[\u00a0]: Copied! <pre>for epoch in range(num_epochs):\n    running_loss = 0.0\n    for batch_idx, (imgs, labels) in enumerate(train_loader):\n        imgs, labels = imgs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(imgs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n        # Print progress\n        processed = (batch_idx + 1) * imgs.size(0)\n        print(f\"Epoch [{epoch+1}/{num_epochs}] | \"\n              f\"Step [{batch_idx+1}/{total_steps}] | \"\n              f\"Images: {processed}/{total_images} | \"\n              f\"Loss: {loss.item():.4f}\", end=\"\\r\")\n\n    print(f\"\\n\u2705 Epoch [{epoch+1}/{num_epochs}] completed. \"\n          f\"Average Loss: {running_loss / total_steps:.4f}\\n\")\n\nprint(\"\ud83c\udf89 Training complete!\")\n</pre> for epoch in range(num_epochs):     running_loss = 0.0     for batch_idx, (imgs, labels) in enumerate(train_loader):         imgs, labels = imgs.to(device), labels.to(device)          optimizer.zero_grad()         outputs = model(imgs)         loss = criterion(outputs, labels)         loss.backward()         optimizer.step()          running_loss += loss.item()          # Print progress         processed = (batch_idx + 1) * imgs.size(0)         print(f\"Epoch [{epoch+1}/{num_epochs}] | \"               f\"Step [{batch_idx+1}/{total_steps}] | \"               f\"Images: {processed}/{total_images} | \"               f\"Loss: {loss.item():.4f}\", end=\"\\r\")      print(f\"\\n\u2705 Epoch [{epoch+1}/{num_epochs}] completed. \"           f\"Average Loss: {running_loss / total_steps:.4f}\\n\")  print(\"\ud83c\udf89 Training complete!\") <p>To visualize 15 predictions, using matplotlib</p> In\u00a0[\u00a0]: Copied! <pre># Assuming 'imgs' and 'outputs' are from your last training batch\n# Get predicted class\n_, predicted = torch.max(outputs, 1)\n\n# \u2705 Denormalize images (since you used ImageNet normalization)\nmean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\nstd = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n\n# Move tensors to CPU for plotting\nimgs_cpu = imgs.detach().cpu()\npredicted_cpu = predicted.detach().cpu()\n\n# \u2705 Denormalize\nimages_denorm = imgs_cpu * std + mean\nimages_denorm = torch.clamp(images_denorm, 0, 1)  # keep pixel range valid\n\n# \u2705 Plot 15 images with predictions\nfig, axes = plt.subplots(1, 15, figsize=(15, 3))\nfor i in range(15):\n    ax = axes[i]\n    ax.imshow(images_denorm[i].permute(1, 2, 0))  # C\u00d7H\u00d7W \u2192 H\u00d7W\u00d7C\n    ax.set_title(f\"Pred: {predicted_cpu[i].item()}\", fontsize=8)\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()\n</pre> # Assuming 'imgs' and 'outputs' are from your last training batch # Get predicted class _, predicted = torch.max(outputs, 1)  # \u2705 Denormalize images (since you used ImageNet normalization) mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1) std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)  # Move tensors to CPU for plotting imgs_cpu = imgs.detach().cpu() predicted_cpu = predicted.detach().cpu()  # \u2705 Denormalize images_denorm = imgs_cpu * std + mean images_denorm = torch.clamp(images_denorm, 0, 1)  # keep pixel range valid  # \u2705 Plot 15 images with predictions fig, axes = plt.subplots(1, 15, figsize=(15, 3)) for i in range(15):     ax = axes[i]     ax.imshow(images_denorm[i].permute(1, 2, 0))  # C\u00d7H\u00d7W \u2192 H\u00d7W\u00d7C     ax.set_title(f\"Pred: {predicted_cpu[i].item()}\", fontsize=8)     ax.axis('off')  plt.tight_layout() plt.show()"}]}